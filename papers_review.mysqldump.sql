-- MySQL dump 10.13  Distrib 5.6.19, for osx10.7 (i386)
--
-- Host: localhost    Database: papers_review
-- ------------------------------------------------------
-- Server version	5.6.14

/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8 */;
/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;
/*!40103 SET TIME_ZONE='+00:00' */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;

--
-- Table structure for table `EndUser`
--

DROP TABLE IF EXISTS `EndUser`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `EndUser` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `active` bit(1) NOT NULL,
  `email` varchar(50) NOT NULL,
  `password` varchar(100) NOT NULL,
  `name` varchar(50) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `ix_EnduserEmail` (`email`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `EndUser`
--

LOCK TABLES `EndUser` WRITE;
/*!40000 ALTER TABLE `EndUser` DISABLE KEYS */;
INSERT INTO `EndUser` VALUES (1,'','manoelcampos@gmail.com','123','Manoel Campos');
/*!40000 ALTER TABLE `EndUser` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `Field`
--

DROP TABLE IF EXISTS `Field`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `Field` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `abbreviation` varchar(10) DEFAULT NULL,
  `description` varchar(120) NOT NULL,
  `fieldType_id` bigint(20) NOT NULL,
  `project_id` bigint(20) NOT NULL,
  `notes` text,
  `showInReports` bit(1) NOT NULL,
  `fieldGroup_id` bigint(20) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `ix_FieldDescription` (`project_id`,`description`),
  UNIQUE KEY `ix_FieldAbbrev` (`project_id`,`abbreviation`),
  KEY `FK_eqa0btcqnvbtclctkjf0tvsw` (`fieldType_id`),
  KEY `FK_4vctf7lf707qy6pu93908nita` (`fieldGroup_id`),
  CONSTRAINT `FK_4vctf7lf707qy6pu93908nita` FOREIGN KEY (`fieldGroup_id`) REFERENCES `FieldGroup` (`id`),
  CONSTRAINT `FK_eqa0btcqnvbtclctkjf0tvsw` FOREIGN KEY (`fieldType_id`) REFERENCES `FieldType` (`id`),
  CONSTRAINT `FK_q22a785cb1yeqrwdoftnxnwbj` FOREIGN KEY (`project_id`) REFERENCES `Project` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=30 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `Field`
--

LOCK TABLES `Field` WRITE;
/*!40000 ALTER TABLE `Field` DISABLE KEYS */;
INSERT INTO `Field` VALUES (10,'Exp','Experiments',2,1,NULL,'',3),(11,'Tech','Technologies used for proposal assessment',2,1,NULL,'',1),(12,'OSS','Open source',1,1,NULL,'',1),(13,'Conc','Concerns',2,1,NULL,'',2),(14,'Proact','Pro-activeness',1,1,NULL,'',1),(15,'Prob','Problem formulation',2,1,NULL,'',1),(16,'VMSO','VM selection order',1,1,NULL,'\0',NULL),(18,'DHS','Destionation host selection',2,1,NULL,'\0',NULL),(19,'Imp','Implementation approach',2,1,NULL,'',3),(20,'Params','Parameters',2,1,NULL,'',4),(21,'VMAD','VM allocation dynamics',1,1,NULL,'\0',NULL),(22,'Arch','Architecture',1,1,NULL,'',1),(23,'Sol','Solution',1,1,NULL,'\0',NULL),(24,'AC','Algorithm complexity',1,1,NULL,'\0',NULL),(25,'Cat','Paper category',2,1,NULL,'',4),(26,'F1','Field1.1',1,8,NULL,'',NULL),(28,'f2','Field2',1,8,NULL,'',NULL),(29,'f3','Field3.1',2,8,NULL,'',NULL);
/*!40000 ALTER TABLE `Field` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `FieldGroup`
--

DROP TABLE IF EXISTS `FieldGroup`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `FieldGroup` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `description` varchar(120) NOT NULL,
  `project_id` bigint(20) NOT NULL,
  `notes` varchar(255) NOT NULL,
  `tableId` varchar(80) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `ix_description` (`description`),
  KEY `FK_kbql3ro8cbm69mngqprm5w5h6` (`project_id`),
  CONSTRAINT `FK_kbql3ro8cbm69mngqprm5w5h6` FOREIGN KEY (`project_id`) REFERENCES `Project` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `FieldGroup`
--

LOCK TABLES `FieldGroup` WRITE;
/*!40000 ALTER TABLE `FieldGroup` DISABLE KEYS */;
INSERT INTO `FieldGroup` VALUES (1,'Grupo 1',1,'Comparison of proposals for VM placement and migration according to architecture, openness of source code, pro-activeness, problem formulation and technologies.','tab:papers-summary1'),(2,'Grupo 2',1,'Comparison of proposals for VM placement and migration according to concerns taken into account.','tab:papers-summary2'),(3,'Grupo 3',1,'Comparison of proposals for VM placement and migration according to performed experiments and implementation approach.','tab:papers-summary3'),(4,'Grupo 4',1,'Comparison of proposals for VM placement and migration according to classified categories and concerned parameters.','tab:papers-summary4'),(5,'Group1',8,'Group1','Group1');
/*!40000 ALTER TABLE `FieldGroup` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `FieldOption`
--

DROP TABLE IF EXISTS `FieldOption`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `FieldOption` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `abbreviation` varchar(20) DEFAULT NULL,
  `description` varchar(120) NOT NULL,
  `field_id` int(11) NOT NULL,
  `parentFieldOption_id` bigint(20) DEFAULT NULL,
  `notes` text,
  `showInReports` bit(1) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `ix_FieldOptionDesc` (`field_id`,`description`),
  UNIQUE KEY `ix_FieldOptionAbbrev` (`field_id`,`abbreviation`),
  KEY `fk_FieldOption_ParentFieldOption` (`parentFieldOption_id`),
  CONSTRAINT `FK_cfcpp5ek6yt9i9qja3muymgwm` FOREIGN KEY (`field_id`) REFERENCES `Field` (`id`),
  CONSTRAINT `fk_FieldOption_ParentFieldOption` FOREIGN KEY (`parentFieldOption_id`) REFERENCES `FieldOption` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=221 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `FieldOption`
--

LOCK TABLES `FieldOption` WRITE;
/*!40000 ALTER TABLE `FieldOption` DISABLE KEYS */;
INSERT INTO `FieldOption` VALUES (17,'Sim','Simulation',10,NULL,NULL,''),(18,'TB','Testbed',10,NULL,NULL,''),(19,'Trace','Real trace analysis',10,NULL,NULL,''),(20,'Math','Mathematical analysis',10,NULL,NULL,''),(21,'Bench','Benchmark testbeds',10,NULL,NULL,''),(22,'OSK','OpenStack',11,NULL,NULL,''),(23,'CS','CloudSim',11,NULL,NULL,''),(24,'CSC','CloudSched',11,NULL,NULL,''),(25,'CIS','Custom implemented simulator',11,NULL,NULL,''),(26,'NS2','Network Simulator 2',11,NULL,NULL,''),(27,'NS3','Network Simulator 3',11,NULL,NULL,''),(28,'PS','PeerSim',11,NULL,NULL,''),(29,'TPC-H','TPC-H',11,NULL,'Transaction Processing Performance Council Benchmark (Query per Hour)',''),(30,'XCP','Xen cloud platform',11,NULL,NULL,''),(31,'NPB','NAS parallel benchmarks',11,NULL,NULL,''),(33,'Y','Open source availability',12,NULL,NULL,''),(36,'Alg','Algorithms description',12,NULL,NULL,''),(37,'SLAM','SLA managment',13,NULL,NULL,''),(38,'VMS','VM selection',13,NULL,'(what VMs migrate)',''),(39,'VMP','VM placement',13,NULL,NULL,''),(40,'VMMig','VM migration',13,NULL,NULL,''),(41,'VMSh','VM shuffle',13,NULL,NULL,''),(42,'VMRP','VM resource provisioning',13,NULL,NULL,''),(43,'SLAV','SLA violations',13,NULL,NULL,''),(44,'QoS','Quality of service',13,NULL,NULL,''),(45,'AV','Availability',13,NULL,NULL,''),(46,'IaaS','IaaS',13,NULL,NULL,''),(47,'PaaS','PaaS',13,NULL,NULL,''),(48,'SaaS','SaaS',13,NULL,NULL,''),(49,'$','Pricing',13,NULL,NULL,'\0'),(50,'SC','Server consolidation',13,NULL,NULL,''),(51,'-EC','Reduce energy consumption',13,NULL,NULL,''),(52,'-M','Reduce number of VM migrations',13,NULL,NULL,''),(54,'-NET','Reduce network traffic/congestion',13,NULL,NULL,''),(55,'-VMT','Reduce VM migration time',13,NULL,NULL,'\0'),(56,'-DOWN','Reduce service downtime',13,NULL,NULL,''),(57,'LB','Load balancing',13,NULL,NULL,''),(58,'+RU','Maximize resource usage',13,NULL,NULL,''),(59,'-CO2','Carbon reduction',13,NULL,NULL,''),(60,'-$','Costs reduction',13,NULL,NULL,''),(61,'BP','Burstiness prediction',13,NULL,NULL,''),(62,'VMR','VM reconfiguration',13,NULL,NULL,''),(63,'-VMMC','Reduce VM migration cost/overhead',13,NULL,NULL,''),(64,'-MS','Minimize makespan',13,NULL,NULL,''),(65,'-T','Minimize temperature',13,NULL,NULL,''),(66,'-FAT','Minimize file access time',13,NULL,NULL,''),(67,'FT','Fault tolerance',13,NULL,NULL,''),(69,'N/I','Not identified',14,NULL,NULL,'\0'),(70,'R','Reactive',14,NULL,NULL,''),(71,'P','Predictive (proactive)',14,NULL,NULL,''),(72,'M','Multiple',14,NULL,NULL,''),(78,'OP','Optimization problem',15,NULL,NULL,''),(79,'TSP','Traveling salesman problem',15,NULL,NULL,''),(80,'CA','Combinatorial analysis',15,NULL,NULL,''),(81,'BPP','Bin-packing problem',15,NULL,NULL,''),(82,'MKBP','Multidimensional knapsack/bin-packing problem',15,NULL,'Solvers implemented by Google\r\nhttps://developers.google.com/optimization/bin/knapsack\r\n',''),(83,'VMSh','VM sheduling',15,NULL,NULL,''),(84,'VMS','VM selection',15,NULL,'(what VMs migrate)',''),(85,'QAP','Quadratic assignment problem',15,NULL,NULL,''),(86,'GPP','Graph partitioning problem',15,NULL,NULL,''),(87,'CSP','Constraint satisfaction problem',15,NULL,NULL,''),(88,'DAG-SPP','Directed acyclic graph shortest path problem',15,NULL,NULL,''),(89,'N/I','Not identified',16,NULL,'Not Identified','\0'),(90,'N/S','Not specified',16,NULL,'Not Specified','\0'),(91,NULL,'CurrentVmAvailabiltity desc',16,NULL,NULL,''),(92,NULL,'VmBandwidthUsage desc',16,NULL,NULL,''),(93,'M','Multiple',16,NULL,NULL,''),(94,NULL,'Arrival order',16,NULL,NULL,''),(95,NULL,'SumOfResourceUsage desc',16,NULL,NULL,''),(96,NULL,'VM group with the higher sum of some resource usage',16,NULL,NULL,''),(97,NULL,'Resources requirements desc',16,NULL,NULL,''),(98,NULL,'VM inter-communication traffic desc',16,NULL,NULL,''),(99,NULL,'Less loaded VM',16,NULL,NULL,''),(100,NULL,'More loaded VM',16,NULL,NULL,''),(101,NULL,'Must used resource desc',16,NULL,NULL,''),(102,'N/I','Not specified',18,NULL,'Not Identified','\0'),(103,'N/A','Not applicable',18,NULL,'Not Applicable','\0'),(104,NULL,'minor abs(CurrentCpuUsage - PreDefinedCpuUsageThreshold)',18,NULL,NULL,''),(105,'M','Multiple',18,NULL,NULL,''),(106,NULL,'PmPowerEfficiency desc',18,NULL,NULL,''),(107,NULL,'TurnOffTime closer to JobCompletionTime',18,NULL,NULL,''),(108,NULL,'Minor remaining resources',18,NULL,NULL,''),(109,'max #vm','Max number of VMs',18,NULL,NULL,''),(110,NULL,'Max cpu usage',18,NULL,NULL,''),(111,NULL,'Green data centers',18,NULL,NULL,''),(112,NULL,'Faster transmission rate (VM RAM/BW)',18,NULL,NULL,''),(113,NULL,'First PM that supports the VM capacity and extra resources reservation',18,NULL,NULL,''),(114,NULL,'Same previous hosted PM',18,NULL,NULL,''),(115,NULL,'Same host of parallelized app instances',18,NULL,NULL,''),(116,NULL,'Makespan desc',18,NULL,NULL,''),(117,NULL,'Inter-communicating VM host',18,NULL,NULL,''),(118,NULL,'More balanced resource usage',18,NULL,NULL,''),(119,NULL,'Worst-fit',18,NULL,'Max resource remaining','\0'),(120,NULL,'Resource usage closer to the max usage thresould',18,NULL,NULL,''),(121,NULL,'Minimize file access time',18,NULL,NULL,''),(122,NULL,'Host ID',18,NULL,NULL,''),(123,NULL,'Clustering algorithms',19,NULL,NULL,''),(124,NULL,'Graph partition',19,NULL,NULL,''),(125,NULL,'Annealing algorithm',19,NULL,NULL,''),(126,'BFD','Best-fit decreasing',19,NULL,NULL,''),(127,'M3SBP','Max-min multidimensional stochastic bin-packing',19,NULL,NULL,''),(128,NULL,'Greedy algorithm',19,NULL,NULL,''),(129,'LIP','Linear integer program',19,NULL,NULL,''),(130,'GA','Genetic algorithm',19,206,NULL,''),(131,NULL,'Max-matching problem of bipartite graph',19,NULL,NULL,''),(132,NULL,'Markov chains',19,NULL,NULL,''),(133,NULL,'Queuing theory',19,NULL,NULL,''),(134,'ISP','Interval scheduling problem',19,NULL,NULL,''),(135,NULL,'Fuzzy logic',19,NULL,NULL,''),(136,NULL,'Pareto efficiency',19,NULL,NULL,''),(137,NULL,'Ant colony system',19,206,NULL,''),(138,NULL,'Bernoulli trial',19,NULL,NULL,''),(139,NULL,'TOPSIS',19,NULL,NULL,''),(140,NULL,'MATLAB',19,NULL,NULL,''),(141,NULL,'Solver',19,NULL,NULL,''),(142,'SIP','Stochastic integer program',19,NULL,NULL,''),(143,NULL,'Dijkstra algorithm',19,NULL,NULL,''),(144,'LA','Linear algebra',19,NULL,NULL,''),(145,'E','Energy',20,NULL,NULL,''),(146,'D','Delay',20,NULL,NULL,''),(147,'T','Traffic',20,NULL,NULL,''),(148,'TP','Throughput',20,NULL,NULL,''),(149,NULL,'RAM',20,NULL,NULL,''),(150,NULL,'CPU',20,NULL,NULL,''),(151,'I/O','I/O',20,NULL,NULL,''),(152,'BW','Bandwidth',20,NULL,NULL,''),(153,NULL,'Cost',20,NULL,NULL,''),(154,'RUS','Resource usage',20,NULL,NULL,''),(155,'S','Storage',20,NULL,NULL,''),(156,'ASO','Any single one',20,NULL,NULL,''),(157,'ST','Server temperature',20,NULL,NULL,''),(158,'SLAR','SLA requirements',20,NULL,NULL,''),(159,NULL,'Generic',20,NULL,NULL,'\0'),(160,'N/S','Not specified',20,NULL,'Not Specified','\0'),(161,'N/I','Not identified',21,NULL,'Not Identified','\0'),(162,'D','Dynamic',21,NULL,NULL,''),(163,'S','Static',21,NULL,NULL,''),(164,'S&D','Static and dynamic',21,NULL,NULL,''),(165,'SS','Semi-static',21,NULL,NULL,''),(166,'N/I','Not identified',22,NULL,'Not Identified','\0'),(167,'CS','Client/Server',22,NULL,NULL,''),(168,'DCM','Distributed & centralized management',22,NULL,NULL,''),(169,'DDM','Distributed & decentralized management',22,NULL,NULL,''),(170,'C','Centralized',22,NULL,NULL,''),(171,'D','Distributed',22,NULL,NULL,''),(172,'N/I','Not identified',23,NULL,'Not Identified','\0'),(173,'AO','Almost optimal',23,NULL,NULL,''),(174,'N/I','Not identified',24,NULL,'Not Identified','\0'),(175,NULL,'NP-hard',24,NULL,NULL,''),(176,NULL,'Multiple',24,NULL,NULL,''),(177,'NP-C','NP-complete',24,NULL,NULL,''),(178,'P','Polynomial',24,NULL,NULL,''),(179,'ES','Exponential smoothing (time series analysis)',19,NULL,NULL,''),(180,'MRU','Minor resource usage',16,NULL,NULL,''),(182,'DHS','Destination host selection',13,NULL,NULL,''),(183,'TMVM','Time to migrate VMs',13,NULL,NULL,''),(184,'QBAP','Quadratic bottleneck assignment problem',15,NULL,NULL,''),(185,'Net','Network based',25,NULL,NULL,''),(186,'BA','Business-aware',25,NULL,NULL,''),(187,'CA','Cost-aware',25,186,NULL,''),(188,'QoS','QoS-aware / SLA-driven',25,186,NULL,''),(189,'Interf','Interference-aware',25,NULL,NULL,''),(190,'LB','Load balancing',25,NULL,NULL,''),(191,'VMSched','VM scheduling',25,NULL,NULL,''),(192,'MFT','Maintenance and fault tolerant',25,NULL,NULL,''),(193,'Elastic','Elasticity-aware',25,NULL,NULL,''),(194,'EP','Energy efficiency and power-aware',25,NULL,NULL,''),(195,'SC','Server consolidation',25,NULL,NULL,''),(196,'DD','Distributed/Decentralized',25,NULL,NULL,''),(197,'TB','Topology based',25,185,NULL,''),(198,'TA','Traffic-aware',25,185,NULL,''),(199,'TO','Topology optimization',25,185,NULL,''),(200,'RNCEC','Reduce network components energy consumption',13,51,NULL,''),(201,'2LS','2-opt local search',19,NULL,NULL,''),(202,'HC','Host capacity',20,NULL,NULL,''),(203,'VMMC','VM memory compression',13,NULL,NULL,''),(204,'MRG','Minimize resource gaps',13,NULL,NULL,''),(205,'M','Multiple',20,NULL,NULL,''),(206,'EC','Evolutionary computation/biology-based optimization',19,NULL,'http://www.wikiwand.com/en/Evolutionary_computation',''),(207,'BBO','Biogeography-based optimization',19,206,'Não confundir com a grande área Biology-based Optimisation. Biogeography está dentro da área de Biology',''),(210,'oa','option A',26,NULL,NULL,''),(211,'ob','option B',26,NULL,NULL,''),(212,NULL,'a',28,NULL,NULL,''),(213,NULL,'b',28,NULL,NULL,''),(215,NULL,'2',29,NULL,NULL,''),(216,NULL,'3',29,NULL,NULL,''),(217,NULL,'4',29,NULL,NULL,''),(218,'GIT','Green IT',25,NULL,NULL,''),(219,'MEE','Maximize energy efficiency',13,NULL,'Maximize the ratio between the amount of computing power provided, network throughput or other metric and the amount of energy consumption. ','\0'),(220,'CP','Constraint programming',19,NULL,NULL,'');
/*!40000 ALTER TABLE `FieldOption` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `FieldType`
--

DROP TABLE IF EXISTS `FieldType`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `FieldType` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `abbreviation` varchar(2) DEFAULT NULL,
  `description` varchar(50) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `ix_FieldTypeDesc` (`description`),
  UNIQUE KEY `ix_FieldTypeAbbrev` (`abbreviation`)
) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `FieldType`
--

LOCK TABLES `FieldType` WRITE;
/*!40000 ALTER TABLE `FieldType` DISABLE KEYS */;
INSERT INTO `FieldType` VALUES (1,'O','Objective'),(2,'M','Multiple Choice'),(3,'S','Subjective');
/*!40000 ALTER TABLE `FieldType` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `Paper`
--

DROP TABLE IF EXISTS `Paper`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `Paper` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `acceptedOnExtractionPhase` int(11) DEFAULT NULL,
  `acceptedOnSelectionPhase` int(11) DEFAULT NULL,
  `authors` varchar(400) NOT NULL,
  `citationKey` varchar(50) NOT NULL,
  `doi` varchar(120) DEFAULT NULL,
  `publicationYear` int(11) NOT NULL,
  `survey` int(11) DEFAULT NULL,
  `title` varchar(240) NOT NULL,
  `url` varchar(300) DEFAULT NULL,
  `paperType_id` bigint(20) DEFAULT NULL,
  `searchSession_id` bigint(20) NOT NULL,
  `paperAbstract` text,
  `status_id` bigint(20) DEFAULT NULL,
  `notes` varchar(255) DEFAULT NULL,
  `proposalName` varchar(100) DEFAULT NULL,
  `bibliographicAuthorNames` varchar(100) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `ix_PaperTitle` (`searchSession_id`,`title`),
  UNIQUE KEY `ix_PaperCitationKey` (`searchSession_id`,`citationKey`),
  KEY `FK_7u2qcmsxhgco373offjfe7njc` (`paperType_id`),
  KEY `FK_okfbrsmswx0067gdqhkmb9wlx` (`status_id`),
  CONSTRAINT `FK_7u2qcmsxhgco373offjfe7njc` FOREIGN KEY (`paperType_id`) REFERENCES `PaperType` (`id`),
  CONSTRAINT `FK_nqs4kycnvfs948k2x0alhs130` FOREIGN KEY (`searchSession_id`) REFERENCES `SearchSession` (`id`),
  CONSTRAINT `FK_okfbrsmswx0067gdqhkmb9wlx` FOREIGN KEY (`status_id`) REFERENCES `PaperStatus` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=347 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `Paper`
--

LOCK TABLES `Paper` WRITE;
/*!40000 ALTER TABLE `Paper` DISABLE KEYS */;
INSERT INTO `Paper` VALUES (106,1,1,'Mohamed Amine Kaaouache and Sadok Bouamama','Kaaouache20151061','http://dx.doi.org/10.1016/j.procs.2015.08.151',2015,0,'Solving bin Packing Problem with a Hybrid Genetic Algorithm for VM Placement in Cloud','http://www.sciencedirect.com/science/article/pii/S1877050915022784',1,3,NULL,4,NULL,'GA-BF packing (HGBF-BP)','Kaaouache and Bouamama.'),(107,1,1,'Raja Wasim Ahmad and Abdullah Gani and Siti Hafizah Ab. Hamid and Muhammad Shiraz and Abdullah Yousafzai and Feng Xia','Ahmad2015','http://dx.doi.org/10.1016/j.jnca.2015.02.002',2015,1,'A survey on virtual machine migration and server consolidation frameworks for cloud data centers ','http://www.sciencedirect.com/science/article/pii/S1084804515000284',2,3,NULL,4,NULL,NULL,NULL),(108,1,1,'Subhadra Bose Shaw and Anil Kumar Singh','Shaw2015','http://dx.doi.org/10.1016/j.compeleceng.2015.07.020',2015,0,'Use of proactive and reactive hotspot detection technique to reduce the number of virtual machine migration and energy consumption in cloud data center ','http://www.sciencedirect.com/science/article/pii/S0045790615002748',2,3,NULL,4,NULL,'Re/pro-active hotspot detection','Shaw and Singh.'),(109,1,1,'Yongqiang Gao and Haibing Guan and Zhengwei Qi and Yang Hou and Liang Liu','Gao2013','http://dx.doi.org/10.1016/j.jcss.2013.02.004',2013,0,'A multi-objective ant colony system algorithm for virtual machine placement in cloud computing ','http://www.sciencedirect.com/science/article/pii/S0022000013000627',1,3,NULL,4,NULL,'Multi-objective ant colony for VM placement','Gao et al.'),(110,0,1,'Vincenzo De Maio and Radu Prodan and Shajulin Benedict and Gabor Kecskemeti','DeMaio2015','http://dx.doi.org/10.1016/j.future.2015.07.007',2015,0,'Modelling energy consumption of network transfers and virtual machine migration ','http://www.sciencedirect.com/science/article/pii/S0167739X15002307',1,3,NULL,3,NULL,NULL,NULL),(111,1,1,'Shihong Zou and Xitao Wen and Kai Chen and Shan Huang and Yan Chen and Yongqiang Liu and Yong Xia and Chengchen Hu','Zou2014141','http://dx.doi.org/10.1016/j.comnet.2014.03.025',2014,0,'VirtualKnotter: Online virtual machine shuffling for congestion resolving in virtualized datacenter ','http://www.sciencedirect.com/science/article/pii/S138912861400139X',2,3,NULL,4,NULL,'VirtualKnotter','Zou et al'),(112,1,1,'Yangming Zhao and Yifan Huang and Kai Chen and Minlan Yu and Sheng Wang and DongSheng Li','Zhao2015109','http://dx.doi.org/10.1016/j.comnet.2014.12.014',2015,0,'Joint VM placement and topology optimization for traffic scalability in dynamic datacenter networks ','http://www.sciencedirect.com/science/article/pii/S138912861400468X',2,3,NULL,4,NULL,'Network and VM placement optimization','Zhao et al.'),(113,1,1,'Weiwei Fang and Xiangmin Liang and Shengxin Li and Luca Chiaraviglio and Naixue Xiong','Fang2013179','http://dx.doi.org/10.1016/j.comnet.2012.09.008',2013,0,'VMPlanner: Optimizing virtual machine placement and traffic flow routing to reduce network power costs in cloud data centers ','http://www.sciencedirect.com/science/article/pii/S1389128612003301',2,3,NULL,4,NULL,'VMPlanner','Fang et al.'),(114,1,1,'Jian-kang DONG and Hong-bo WANG and Yang-yang LI and Shi-duan CHENG','Dong2014','http://dx.doi.org/10.1016/S1005-8885(14)60302-2',2014,0,'Virtual machine placement optimizing to improve network performance in cloud data centers ','http://www.sciencedirect.com/science/article/pii/S1005888514603022',2,3,NULL,4,NULL,'Reduced MLU for traffic balance','Dong et al.'),(115,1,1,'Renuga Kanagavelu and Bu-Sung Lee and Nguyen The Dat Le and Luke Ng Mingjie and Khin Mi Mi Aung','Kanagavelu2014','http://dx.doi.org/10.1016/j.comcom.2014.07.009',2014,0,'Virtual machine placement with two-path traffic routing for reduced congestion in data center networks ','http://www.sciencedirect.com/science/article/pii/S0140366414002746',2,3,NULL,4,NULL,'Greedy VM placement (GVMTPR)','Kanagavelu et al.'),(116,NULL,0,'Hai Jin and Li Deng and Song Wu and Xuanhua Shi and Hanhua Chen and Xiaodong Pan','Jin201423','http://dx.doi.org/10.1016/j.future.2013.09.031',2014,NULL,'MECOM: Live migration of virtual machines by adaptively compressing memory pages ','http://www.sciencedirect.com/science/article/pii/S0167739X13002100',NULL,3,NULL,1,NULL,NULL,NULL),(117,1,1,'Xin Li and Zhuzhong Qian and Sanglu Lu and Jie Wu','Li20131222','http://dx.doi.org/10.1016/j.mcm.2013.02.003',2013,0,'Energy efficient virtual machine placement algorithm with balanced and improved resource utilization in a data center ','http://www.sciencedirect.com/science/article/pii/S0895717713000319',2,3,'Powerful data centers are the essential supporting infrastructure for mobile, ubiquitous, and cognitive computing, which are the most popular computing paradigms to utilize all kinds of physical resources and provide various services. To ensure the high quality of services, the performance and cost of a data center is a critical factor. In this paper, we investigate the issue of increasing the resource utilization of data centers to improve their performance and lower the cost. It is an efficient way to increase resource utilization via resource sharing. Technically, server virtualization provides the opportunity to share resources in data centers. However, it also introduces other problems, the primary problem being virtual machine placement (VMP), which is to choose a proper physical machine (PM) to deploy virtual machines (VMs) in runtime. We study the virtual machine placement problem with the target of minimizing the total energy consumption by the running of PMs, which is also an indication of resource utilization and the cost of a data center. Due to the multiple dimensionality of physical resources, there always exists a waste of resources, which results from the imbalanced use of multi-dimensional resources. To characterize the multi-dimensional resource usage states of PMs, we present a multi-dimensional space partition model. Based on this model, we then propose a virtual machine placement algorithm EAGLE, which can balance the utilization of multi-dimensional resources, reduce the number of running PMs, and thus lower the energy consumption. We also evaluate our proposed balanced algorithm EAGLE via extensive simulations and experiments on real traces. Experimental results show, over the long run, that EAGLE can save as much as 15% more energy than the first fit algorithm.',4,NULL,'EAGLE','Li et al.'),(118,NULL,0,'Zia ur Rehman and Omar Khadeer Hussain and Elizabeth Chang and Tharam Dillon','Rehman2015','http://dx.doi.org/10.1016/j.elerap.2015.08.002',2015,NULL,'Decision-making framework for user-based inter-cloud service migration ','http://www.sciencedirect.com/science/article/pii/S1567422315000575',NULL,3,NULL,1,NULL,NULL,NULL),(119,1,1,'Qinghua Zheng and Rui Li and Xiuqi Li and Nazaraf Shah and Jianke Zhang and Feng Tian and Kuo-Ming Chao and Jia Li','Zheng2015','http://dx.doi.org/10.1016/j.future.2015.02.010',2015,0,'Virtual machine consolidated placement based on multi-objective biogeography-based optimization ','http://www.sciencedirect.com/science/article/pii/S0167739X15000564',2,3,NULL,4,NULL,'Multi-objective BBO server consolidation','Zheng et al.'),(120,NULL,NULL,'Deshi Ye and Jianhai Chen','Ye20131345','http://dx.doi.org/10.1016/j.future.2013.02.004',2013,NULL,'Non-cooperative games on multidimensional resource allocation ','http://www.sciencedirect.com/science/article/pii/S0167739X13000320',NULL,3,NULL,NULL,NULL,NULL,NULL),(121,NULL,NULL,'S. Sohrabi and I. Moser','Sohrabi20152794','http://dx.doi.org/10.1016/j.procs.2015.05.436',2015,NULL,'The Effects of Hotspot Detection and Virtual Machine Migration Policies on Energy Consumption and Service Levels in the Cloud ','http://www.sciencedirect.com/science/article/pii/S1877050915012442',NULL,3,NULL,NULL,NULL,NULL,NULL),(122,NULL,NULL,'Jenn-Wei Lin and Chien-Hung Chen and Chi-Yi Lin','Lin2014478','http://dx.doi.org/10.1016/j.future.2013.12.034',2014,NULL,'Integrating QoS awareness with virtualization in cloud computing systems for delay-sensitive applications ','http://www.sciencedirect.com/science/article/pii/S0167739X13002987',NULL,3,NULL,NULL,NULL,NULL,NULL),(123,1,1,'Amir Rahimzadeh Ilkhechi and Ibrahim Korpeoglu and Özgür Ulusoy','Ilkhechi2015508','http://dx.doi.org/10.1016/j.comnet.2015.08.042',2015,0,'Network-aware virtual machine placement in cloud data centers with multiple traffic-intensive components ','http://www.sciencedirect.com/science/article/pii/S1389128615003023',2,3,NULL,4,NULL,'VM satisfaction-based placement','Ilkhechi et al.'),(124,NULL,NULL,'Saad Mustafa and Babar Nazir and Amir Hayat and Atta ur Rehman Khan and Sajjad A. Madani','Mustafa2015','http://dx.doi.org/10.1016/j.compeleceng.2015.07.021',2015,NULL,'Resource management in cloud computing: Taxonomy, prospects, and challenges ','http://www.sciencedirect.com/science/article/pii/S004579061500275X',NULL,3,NULL,NULL,NULL,NULL,NULL),(125,NULL,1,'Bane Raman Raghunath and B. Annappa','Raghunath2015167','http://dx.doi.org/10.1016/j.procs.2015.06.019',2015,0,'Virtual Machine Migration Triggering using Application Workload Prediction ','http://www.sciencedirect.com/science/article/pii/S1877050915013435',NULL,3,NULL,NULL,NULL,NULL,NULL),(126,0,1,'Xiaoying Wang and Zhihui Du and Yinong Chen and Mengqin Yang','Wang2015','http://dx.doi.org/10.1016/j.simpat.2015.01.005',2015,0,'A green-aware virtual machine migration strategy for sustainable datacenter powered by renewable energy ','http://www.sciencedirect.com/science/article/pii/S1569190X15000155',2,3,NULL,3,NULL,NULL,NULL),(127,NULL,0,'Sujesha Sudevalayam and Purushottam Kulkarni','Sudevalayam20132627','http://dx.doi.org/10.1016/j.jss.2013.04.085',2013,0,'Affinity-aware modeling of CPU usage with communicating virtual machines ','http://www.sciencedirect.com/science/article/pii/S0164121213001246',2,3,NULL,1,NULL,NULL,NULL),(128,NULL,NULL,'Fumio Machida and Dong Seong Kim and Kishor S. Trivedi','Machida2013212','http://dx.doi.org/10.1016/j.peva.2012.09.003',2013,NULL,'Modeling and analysis of software rejuvenation in a server virtualized system with live VM migration ','http://www.sciencedirect.com/science/article/pii/S0166531612000934',NULL,3,NULL,NULL,NULL,NULL,NULL),(129,NULL,NULL,'Juliano Araujo Wickboldt and Rafael Pereira Esteves and Márcio Barbosa de Carvalho and Lisandro Zambenedetti Granville','Wickboldt201454','http://dx.doi.org/10.1016/j.comnet.2014.02.018',2014,NULL,'Resource management in IaaS cloud platforms made flexible through programmability ','http://www.sciencedirect.com/science/article/pii/S138912861400084X',NULL,3,NULL,NULL,NULL,NULL,NULL),(130,NULL,NULL,'Mohan Raj Velayudhan Kumar and Shriram Raghunathan','VelayudhanKumar2015','http://dx.doi.org/10.1016/j.jcss.2015.07.005',2015,NULL,'Heterogeneity and thermal aware adaptive heuristics for energy efficient consolidation of virtual machines in infrastructure clouds ','http://www.sciencedirect.com/science/article/pii/S002200001500080X',NULL,3,NULL,NULL,NULL,NULL,NULL),(131,NULL,NULL,'Aissan Dalvandi and Mohan Gurusamy and Kee Chaing Chua','Dalvandi2015249','http://dx.doi.org/10.1016/j.comnet.2015.06.017',2015,NULL,'Power-efficient resource-guaranteed VM placement and routing for time-aware data center applications ','http://www.sciencedirect.com/science/article/pii/S1389128615002182',NULL,3,NULL,NULL,NULL,NULL,NULL),(132,NULL,NULL,'Narander Kumar and Swati Saxena','Kumar2015823','http://dx.doi.org/10.1016/j.procs.2015.03.163',2015,NULL,'Migration Performance of Cloud Applications- A Quantitative Analysis ','http://www.sciencedirect.com/science/article/pii/S1877050915003993',NULL,3,NULL,NULL,NULL,NULL,NULL),(133,1,1,'Gang Sun and Dan Liao and Vishal Anand and Dongcheng Zhao and Hongfang Yu','Sun201674','http://dx.doi.org/10.1016/j.future.2015.09.005',2016,0,'A new technique for efficient live migration of multiple virtual machines ','http://www.sciencedirect.com/science/article/pii/S0167739X15002848',2,3,NULL,4,NULL,'Serial and parallel migration','Sun et al.'),(134,NULL,NULL,'Jun-jie TONG and Hai-hong E and Mei-na SONG and Jun-de SONG','TONG201440','http://dx.doi.org/10.1016/S1005-8885(14)60314-9',2014,NULL,'Host load prediction in cloud based on classification methods ','http://www.sciencedirect.com/science/article/pii/S1005888514603149',NULL,3,NULL,NULL,NULL,NULL,NULL),(135,NULL,NULL,'Hai Jin and Wei Gao and Song Wu and Xuanhua Shi and Xiaoxin Wu and Fan Zhou','Jin20111088','http://dx.doi.org/10.1016/j.jnca.2010.06.013',2011,NULL,'Optimizing the live migration of virtual machine by CPU scheduling ','http://www.sciencedirect.com/science/article/pii/S1084804510001116',NULL,3,NULL,NULL,NULL,NULL,NULL),(136,NULL,NULL,'Saurabh Kumar Garg and Adel Nadjaran Toosi and Srinivasa K. Gopalaiyengar and Rajkumar Buyya','Garg2014108','http://dx.doi.org/10.1016/j.jnca.2014.07.030',2014,NULL,'SLA-based virtual machine management for heterogeneous workloads in a cloud datacenter ','http://www.sciencedirect.com/science/article/pii/S1084804514001787',NULL,3,NULL,NULL,NULL,NULL,NULL),(137,NULL,NULL,'L. Velasco and A. Asensio and J.Ll. Berral and E. Bonetto and F. Musumeci and V. López','Velasco2014142','http://dx.doi.org/10.1016/j.comcom.2014.03.004',2014,NULL,'Elastic operations in federated datacenters for performance and cost optimization ','http://www.sciencedirect.com/science/article/pii/S0140366414000905',NULL,3,NULL,NULL,NULL,NULL,NULL),(138,NULL,NULL,'Francesco Palmieri and Ugo Fiore and Sergio Ricciardi and Aniello Castiglione','Palmieri2015','http://dx.doi.org/10.1016/j.future.2015.01.017',2015,NULL,'GRASP-based resource re-optimization for effective big data access in federated clouds ','http://www.sciencedirect.com/science/article/pii/S0167739X15000345',NULL,3,NULL,NULL,NULL,NULL,NULL),(139,NULL,NULL,'Lu Lu and Xuanhua Shi and Hai Jin and Qiuyue Wang and Daxing Yuan and Song Wu','Lu201480','http://dx.doi.org/10.1016/j.future.2013.12.026',2014,NULL,'Morpho: A decoupled MapReduce framework for elastic cloud computing ','http://www.sciencedirect.com/science/article/pii/S0167739X13002902',NULL,3,NULL,NULL,NULL,NULL,NULL),(140,1,1,'Gregory Katsaros and Josep Subirats and J. Oriol Fitó and Jordi Guitart and Pierre Gilet and Daniel Espling','Katsaros20132077','http://dx.doi.org/10.1016/j.future.2012.12.006',2013,0,'A service framework for energy-aware monitoring and VM management in Clouds ','http://www.sciencedirect.com/science/article/pii/S0167739X12002269',2,3,NULL,4,NULL,'VM management framework','Katsaros et al.'),(141,NULL,NULL,'Nikos Tziritas and Samee Ullah Khan and Cheng-Zhong Xu and Thanasis Loukopoulos and Spyros Lalis','Tziritas20131690','http://dx.doi.org/10.1016/j.jpdc.2013.07.020',2013,NULL,'On minimizing the resource consumption of cloud applications using process migrations ','http://www.sciencedirect.com/science/article/pii/S0743731513001585',NULL,3,NULL,NULL,NULL,NULL,NULL),(142,NULL,NULL,'Tiago C. Ferreto and Marco A.S. Netto and Rodrigo N. Calheiros and César A.F. De Rose','Ferreto20111027','http://dx.doi.org/10.1016/j.future.2011.04.016',2011,NULL,'Server consolidation with migration control for virtualized data centers ','http://www.sciencedirect.com/science/article/pii/S0167739X11000677',NULL,3,NULL,NULL,NULL,NULL,NULL),(143,NULL,NULL,'Xiaolin Xu and Hai Jin and Song Wu and Yihong Wang','Xu201575','http://dx.doi.org/10.1016/j.future.2014.10.004',2015,NULL,'Rethink the storage of virtual machine images in clouds ','http://www.sciencedirect.com/science/article/pii/S0167739X14001885',NULL,3,NULL,NULL,NULL,NULL,NULL),(144,NULL,0,'Josep Subirats and Jordi Guitart','Subirats201570','http://dx.doi.org/10.1016/j.future.2014.11.008',2015,0,'Assessing and forecasting energy efficiency on Cloud computing platforms ','http://www.sciencedirect.com/science/article/pii/S0167739X14002428',2,3,NULL,1,NULL,NULL,NULL),(145,NULL,NULL,'Gregory Katsaros and Pascal Stichler and Josep Subirats and Jordi Guitart','Katsaros2015','http://dx.doi.org/10.1016/j.future.2015.01.002',2015,NULL,'Estimation and forecasting of ecological efficiency of virtual machines ','http://www.sciencedirect.com/science/article/pii/S0167739X15000035',NULL,3,NULL,NULL,NULL,NULL,NULL),(146,NULL,NULL,'Corentin Dupont and Fabien Hermenier and Thomas Schulze and Robert Basmadjian and Andrey Somov and Giovanni Giuliani','Dupont2015505','http://dx.doi.org/10.1016/j.adhoc.2014.11.003',2015,NULL,'Plug4Green: A flexible energy-aware VM manager to fit data centre particularities ','http://www.sciencedirect.com/science/article/pii/S1570870514002376',NULL,3,NULL,NULL,NULL,NULL,NULL),(147,NULL,NULL,'Andreas Wolke and Boldbaatar Tsend-Ayush and Carl Pfeiffer and Martin Bichler','Wolke201583','http://dx.doi.org/10.1016/j.is.2015.03.003',2015,NULL,'More than bin packing: Dynamic resource allocation strategies in cloud data centers ','http://www.sciencedirect.com/science/article/pii/S0306437915000472',NULL,3,NULL,NULL,NULL,NULL,NULL),(148,NULL,NULL,'Mario Macías and Jordi Guitart','Macías201419','http://dx.doi.org/10.1016/j.future.2014.03.004',2014,NULL,'SLA negotiation and enforcement policies for revenue maximization and client classification in cloud providers ','http://www.sciencedirect.com/science/article/pii/S0167739X14000491',NULL,3,NULL,NULL,NULL,NULL,NULL),(149,NULL,NULL,'Boris Teabe and Alain Tchana and Daniel Hagimont','Teabe20151','http://dx.doi.org/10.1016/j.future.2015.05.013',2015,NULL,'Enforcing CPU allocation in a heterogeneous IaaS ','http://www.sciencedirect.com/science/article/pii/S0167739X15001880',NULL,3,NULL,NULL,NULL,NULL,NULL),(150,NULL,NULL,'Wei Lin Guay and Sven-Arne Reinemo and Bjørn Dag Johnsen and Chien-Hua Yen and Tor Skeie and Olav Lysne and Ola Tørudbakken','Guay201539','http://dx.doi.org/10.1016/j.jpdc.2015.01.004',2015,NULL,'Early experiences with live migration of SR-IOV enabled InfiniBand ','http://www.sciencedirect.com/science/article/pii/S0743731515000052',NULL,3,NULL,NULL,NULL,NULL,NULL),(151,NULL,NULL,'Yaozu Dong and Xiaowei Yang and Jianhui Li and Guangdeng Liao and Kun Tian and Haibing Guan','Dong20121471','http://dx.doi.org/10.1016/j.jpdc.2012.01.020',2012,NULL,'High performance network virtualization with SR-IOV ','http://www.sciencedirect.com/science/article/pii/S0743731512000329',NULL,3,NULL,NULL,NULL,NULL,NULL),(152,NULL,NULL,'W. Lloyd and S. Pallickara and O. David and J. Lyon and M. Arabi and K. Rojas','Lloyd20131254','http://dx.doi.org/10.1016/j.future.2012.12.007',2013,NULL,'Performance implications of multi-tier application deployments on Infrastructure-as-a-Service clouds: Towards performance modeling ','http://www.sciencedirect.com/science/article/pii/S0167739X12002270',NULL,3,NULL,NULL,NULL,NULL,NULL),(153,NULL,0,'Gursharan Singh and Sunny Behal and Monal Taneja','Singh201591','http://dx.doi.org/10.1016/j.procs.2015.07.373',2015,0,'Advanced Memory Reusing Mechanism for Virtual Machines in Cloud Computing ','http://www.sciencedirect.com/science/article/pii/S187705091501902X',1,3,NULL,1,NULL,NULL,NULL),(154,NULL,NULL,'R. Jeyarani and N. Nagaveni and R. Vasanth Ram','Jeyarani2012811','http://dx.doi.org/10.1016/j.future.2011.06.002',2012,NULL,'Design and implementation of adaptive power-aware virtual machine provisioner (APA-VMP) using swarm intelligence ','http://www.sciencedirect.com/science/article/pii/S0167739X11001130',NULL,3,NULL,NULL,NULL,NULL,NULL),(155,NULL,NULL,'Mohamed Amine Kaaouache and Sadok Bouamama','Kaaouache20151061','http://dx.doi.org/10.1016/j.procs.2015.08.151',2015,NULL,'Solving bin Packing Problem with a Hybrid Genetic Algorithm for VM Placement in Cloud','http://www.sciencedirect.com/science/article/pii/S1877050915022784',NULL,8,'Abstract The Bin Packing Problem\'s purpose (BPP) is to find the minimum number of bins needed to pack a given set of objects of known sizes so that they do not exceed the capacity of each bin. This problem is known to be NP- Hard. In this paper, we propose an hybrid genetic algorithm using BFD (Best Fit Decreasing) to deal with infeasible solution due to the bin-used representation. Experimental results showed the effectiveness of our approach for infeasible chromosomes thereby improving the quality of the obtained solution. This will give a good result for the virtual machine placement in Cloud to minimize energy since it looks like a BPP.',NULL,NULL,NULL,NULL),(156,NULL,NULL,'Raja Wasim Ahmad and Abdullah Gani and Siti Hafizah Ab. Hamid and Muhammad Shiraz and Abdullah Yousafzai and Feng Xia','Ahmad2015','http://dx.doi.org/10.1016/j.jnca.2015.02.002',2015,NULL,'A survey on virtual machine migration and server consolidation frameworks for cloud data centers ','http://www.sciencedirect.com/science/article/pii/S1084804515000284',NULL,8,'Abstract Modern Cloud Data Centers exploit virtualization for efficient resource management to reduce cloud computational cost and energy budget. Virtualization empowered by virtual machine (VM) migration meets the ever increasing demands of dynamic workload by relocating VMs within Cloud Data Centers. VM migration helps successfully achieve various resource management objectives such as load balancing, power management, fault tolerance, and system maintenance. However, being resource-intensive, the VM migration process rigorously affects application performance unless attended by smart optimization methods. Furthermore, a Cloud Data Centre exploits server consolidation and DVFS methods to optimize energy consumption. This paper reviews state-of-the-art bandwidth optimization schemes, server consolidation frameworks, DVFS-enabled power optimization, and storage optimization methods over WAN links. Through a meticulous literature review of state-of-the-art live VM migration schemes, thematic taxonomies are proposed to categorize the reported literature. The critical aspects of virtual machine migration schemes are investigated through a comprehensive analysis of the existing schemes. The commonalties and differences among existing VM migration schemes are highlighted through a set of parameters derived from the literature. Finally, open research issues and trends in the VM migration domain that necessitate further consideration to develop optimal VM migration schemes are highlighted. ',NULL,NULL,NULL,NULL),(157,NULL,NULL,'Subhadra Bose Shaw and Anil Kumar Singh','Shaw2015','http://dx.doi.org/10.1016/j.compeleceng.2015.07.020',2015,NULL,'Use of proactive and reactive hotspot detection technique to reduce the number of virtual machine migration and energy consumption in cloud data center ','http://www.sciencedirect.com/science/article/pii/S0045790615002748',NULL,8,'Abstract The increasing demand of cloud computing motivates the researchers to make cloud environment more efficient for its users and more profitable for the providers. Though virtualization technology helps to increase the resource utilization, still the operational cost of cloud gradually increases mainly due to the consumption of large amount of electrical energy. So to reduce the energy consumption virtual machines (VM) are dynamically consolidated to lesser number of physical machines (PMs) by live VM migration technique. But this may cause SLA violation and the provider is penalized. So to maintain an energy-performance trade-off, the number of VM migration should be minimized. VM migration primarily takes place in two cases: for hotspot mitigation and to switch off the underutilized nodes by migrating all its VMs. If a host is found to be overloaded then instead of immediately migrating some of its VMs we can check whether the migration is really required or not. For this we have proposed a load prediction algorithm to decide whether the migration will be performed or not. After the decision has been taken the algorithm finds a suitable destination host where the VM will be shifted. For this we have proposed a novel approach to decide whether a particular host is suitable as destination depending on its probable future load. We have simulated our algorithms in CloudSim using real world workload traces and compared them with the existing benchmark algorithms. Results show that the proposed methods significantly reduce the number of VM migration and subsequent energy consumption while maintaining the SLA. ',NULL,NULL,NULL,NULL),(158,0,1,'Yongqiang Gao and Haibing Guan and Zhengwei Qi and Yang Hou and Liang Liu','Gao2013','http://dx.doi.org/10.1016/j.jcss.2013.02.004',2013,0,'A multi-objective ant colony system algorithm for virtual machine placement in cloud computing ','http://www.sciencedirect.com/science/article/pii/S0022000013000627',NULL,8,'Abstract Virtual machine placement is a process of mapping virtual machines to physical machines. The optimal placement is important for improving power efficiency and resource utilization in a cloud computing environment. In this paper, we propose a multi-objective ant colony system algorithm for the virtual machine placement problem. The goal is to efficiently obtain a set of non-dominated solutions (the Pareto set) that simultaneously minimize total resource wastage and power consumption. The proposed algorithm is tested with some instances from the literature. Its solution performance is compared to that of an existing multi-objective genetic algorithm and two single-objective algorithms, a well-known bin-packing algorithm and a maxmin ant system (MMAS) algorithm. The results show that the proposed algorithm is more efficient and effective than the methods we compared it to. ',3,NULL,NULL,NULL),(159,NULL,NULL,'Vincenzo De Maio and Radu Prodan and Shajulin Benedict and Gabor Kecskemeti','DeMaio2015','http://dx.doi.org/10.1016/j.future.2015.07.007',2015,NULL,'Modelling energy consumption of network transfers and virtual machine migration ','http://www.sciencedirect.com/science/article/pii/S0167739X15002307',NULL,8,'Abstract Reducing energy consumption has become a key issue for data centres, not only because of economical benefits but also for environmental and marketing reasons. Therefore, assessing their energy consumption requires precise models. In the past years, many models targeting different hardware components, such as CPU, storage and network interface cards (NIC) have been proposed. However, most of them neglect energy consumption related to VM migration. Since VM migration is a network-intensive process, to accurately model its energy consumption we also need energy models for network transfers, comprising their complete software stacks with different energy characteristics. In this work, we present a comparative analysis of the energy consumption of the software stack of two of todays most used NICs in data centres, Ethernet and Infiniband. We carefully design for this purpose a set of benchmark experiments to assess the impact of different traffic patterns and interface settings on energy consumption. Using our benchmark results, we derive an energy consumption model for network transfers. Based on this model, we propose an energy consumption model for VM migration providing accurate predictions for paravirtualised VMs running on homogeneous hosts. We present a comprehensive analysis of our model on different machine sets and compare it with other models for energy consumption of VM migration, showing an improvement of up to 24% in accuracy, according to the NRMSE error metric. ',NULL,NULL,NULL,NULL),(160,NULL,NULL,'Shihong Zou and Xitao Wen and Kai Chen and Shan Huang and Yan Chen and Yongqiang Liu and Yong Xia and Chengchen Hu','Zou2014141','http://dx.doi.org/10.1016/j.comnet.2014.03.025',2014,NULL,'VirtualKnotter: Online virtual machine shuffling for congestion resolving in virtualized datacenter ','http://www.sciencedirect.com/science/article/pii/S138912861400139X',NULL,8,'Abstract Our measurements on production datacenter traffic together with recently-reported results (Kandula et al.) [1] suggest that datacenter networks suffer from long-lived congestion caused by core network oversubscription and unbalanced workload placement. In contrast to traditional traffic engineering approaches that optimize flow routing, in this paper, we explore the opportunity to address the continuous congestion via optimizing VM placement in virtualized datacenters. To this end, we present VirtualKnotter to reduce congestion with controllable VM migration traffic as well as low migration time, which includes an online VM placement algorithm and an efficient VM migration scheduling algorithm. Our evaluation with both real and synthetic traffic patterns shows that VirtualKnotter performs close to the baseline algorithm in terms of link unitization, with only 510% migration traffic of the baseline algorithm. Furthermore, VirtualKnotter decreases link congestion time by 53% for the production datacenter traffic. ',NULL,NULL,NULL,NULL),(161,NULL,NULL,'Yangming Zhao and Yifan Huang and Kai Chen and Minlan Yu and Sheng Wang and DongSheng Li','Zhao2015109','http://dx.doi.org/10.1016/j.comnet.2014.12.014',2015,NULL,'Joint VM placement and topology optimization for traffic scalability in dynamic datacenter networks ','http://www.sciencedirect.com/science/article/pii/S138912861400468X',NULL,8,'Abstract In dynamic datacenter networks (DDNs), there are two ways to handle growing traffic: adjusting the network topology according to the traffic and placing virtual machines (VMs) to change the workload according to the topology. While previous work only focused on one of these two approaches, in this paper, we jointly optimize both virtual machine placement and topology design to achieve higher traffic scalability. We formulate this joint optimization problem to be a mixed integer linear programming (MILP) model and design an efficient heuristic based on Lagranges relaxation decomposition. To handle traffic dynamics, we introduce an online algorithm that can balance algorithm performance and overhead. Our extensive simulation with various network settings and traffic patterns shows that compared with randomly placing VMs in fixed datacenter networks, our algorithm can reduce up to 58.78% of the traffic in the network, and completely avoid traffic overflow in most cases. Furthermore, our online algorithm greatly reduces network cost without sacrificing too much network stability. ',NULL,NULL,NULL,NULL),(162,NULL,NULL,'Weiwei Fang and Xiangmin Liang and Shengxin Li and Luca Chiaraviglio and Naixue Xiong','Fang2013179','http://dx.doi.org/10.1016/j.comnet.2012.09.008',2013,NULL,'VMPlanner: Optimizing virtual machine placement and traffic flow routing to reduce network power costs in cloud data centers ','http://www.sciencedirect.com/science/article/pii/S1389128612003301',NULL,8,'In recent years, the power costs of cloud data centers have become a practical concern and have attracted significant attention from both industry and academia. Most of the early works on data center energy efficiency have focused on the biggest power consumers (i.e., computer servers and cooling systems), yet without taking the networking part into consideration. However, recent studies have revealed that the network elements consume 1020% of the total power in the data center, which poses a great challenge to effectively reducing network power cost without adversely affecting overall network performance. Based on the analysis on topology characteristics and traffic patterns of data centers, this paper presents a novel approach, called VMPlanner, for network power reduction in the virtualization-based data centers. The basic idea of VMPlanner is to optimize both virtual machine placement and traffic flow routing so as to turn off as many unneeded network elements as possible for power saving. We formulate the optimization problem, analyze its hardness, and solve it by designing VMPlanner as a stepwise optimization approach with three approximation algorithms. VMPlanner is implemented and evaluated in a simulated environment with traffic traces collected from a data center test-bed, and the experiment results illustrate the efficacy and efficiency of this approach. ',NULL,NULL,NULL,NULL),(163,NULL,NULL,'Jian-kang DONG and Hong-bo WANG and Yang-yang LI and Shi-duan CHENG','Dong2014','http://dx.doi.org/10.1016/S1005-8885(14)60302-2',2014,NULL,'Virtual machine placement optimizing to improve network performance in cloud data centers ','http://www.sciencedirect.com/science/article/pii/S1005888514603022',NULL,8,'Abstract With the wide application of virtualization technology in cloud data centers, how to effectively place virtual machine (VM) is becoming a major issue for cloud providers. The existing virtual machine placement (VMP) solutions are mainly to optimize server resources. However, they pay little consideration on network resources optimization, and they do not concern the impact of the network topology and the current network traffic. A multi-resource constraints VMP scheme is proposed. Firstly, the authors attempt to reduce the total communication traffic in the data center network, which is abstracted as a quadratic assignment problem; and then aim at optimizing network maximum link utilization (MLU). On the condition of slight variation of the total traffic, minimizing MLU can balance network traffic distribution and reduce network congestion hotspots, a classic combinatorial optimization problem as well as NP-hard problem. Ant colony optimization and 2-opt local search are combined to solve the problem. Simulation shows that MLU is decreased by 20%, and the number of hot links is decreased by 37%. ',NULL,NULL,NULL,NULL),(164,NULL,NULL,'Renuga Kanagavelu and Bu-Sung Lee and Nguyen The Dat Le and Luke Ng Mingjie and Khin Mi Mi Aung','Kanagavelu2014','http://dx.doi.org/10.1016/j.comcom.2014.07.009',2014,NULL,'Virtual machine placement with two-path traffic routing for reduced congestion in data center networks ','http://www.sciencedirect.com/science/article/pii/S0140366414002746',NULL,8,'Abstract Virtualization-based Data Centers are increasingly becoming the hosting platform for a wide range of applications. The communication patterns in Data Center networks show the trend towards increasing bandwidth usage between virtual machines (VMs) within the Data Center resulting in higher chance of occurrence of network congestion. Thus, VM placement and routing algorithms are increasingly important to maximize application performance, provide fault tolerance, and reduce network loads. A less-than optimal placement of communicating VMs can cause inter-VM traffic to traverse bottlenecked network paths leading to large cross network traffic. The core network oversubscription and unbalanced workload placement could lead to long-lived congestion in Data Center networks. Multipath routing with traffic distributed in an appropriate proportion helps balance the load and decrease the possibility of congestion. Furthermore, by routing traffic on multiple link-disjoint paths, traffic can be protected against failures. The use of link-disjoint paths ensures the availability of at least one path for the traffic upon a link failure, thus guaranteeing a certain bandwidth (associated with the surviving paths). In this paper, we study the problem of VM placement with traffic routing on multiple paths for reduced occurrence of congestion while satisfying a certain protection grade which is defined as the fraction of rate (or bandwidth) guaranteed to be available in the event of single link failures. We develop an efficient algorithm based on a greedy technique for placing VMs onto servers satisfying the computing and memory resource requirements, taking into account the amount of inter-VM traffic and network load. In addition, we develop a two-path routing algorithm to satisfy the bandwidth and protection grade requirements so as to reduce the network congestion. Our simulation results show the effectiveness of the proposed algorithms in balancing the load and resilient when compared to other first-fit and random algorithms. ',NULL,NULL,NULL,NULL),(165,NULL,NULL,'Hai Jin and Li Deng and Song Wu and Xuanhua Shi and Hanhua Chen and Xiaodong Pan','Jin201423','http://dx.doi.org/10.1016/j.future.2013.09.031',2014,NULL,'MECOM: Live migration of virtual machines by adaptively compressing memory pages ','http://www.sciencedirect.com/science/article/pii/S0167739X13002100',NULL,8,'Abstract Live migration of virtual machines has been a powerful tool to facilitate system maintenance, load balancing, fault tolerance, and power-saving, especially in clusters or data centers. Although pre-copy is extensively used to migrate memory data of virtual machines, it cannot provide quick migration with low network overhead but leads to large performance degradation of virtual machine services due to the great amount of transferred data during migration. To solve the problem, this paper presents the design and implementation of a novel memory-compression-based VM migration approach (MECOM for short) that uses memory compression to provide fast, stable virtual machine migration, while guaranteeing the virtual machine services to be slightly affected. Based on memory page characteristics, we design an adaptive zero-aware compression algorithm for balancing the performance and the cost of virtual machine migration. Using the proposed scheme pages are rapidly compressed in batches on the source and exactly recovered on the target. Experimental results demonstrate that compared with Xen, our system can significantly reduce downtime, total migration time, and total transferred data by 27.1%, 32%, and 68.8% respectively. ',NULL,NULL,NULL,NULL),(166,NULL,NULL,'Xin Li and Zhuzhong Qian and Sanglu Lu and Jie Wu','Li20131222','http://dx.doi.org/10.1016/j.mcm.2013.02.003',2013,NULL,'Energy efficient virtual machine placement algorithm with balanced and improved resource utilization in a data center ','http://www.sciencedirect.com/science/article/pii/S0895717713000319',NULL,8,'Powerful data centers are the essential supporting infrastructure for mobile, ubiquitous, and cognitive computing, which are the most popular computing paradigms to utilize all kinds of physical resources and provide various services. To ensure the high quality of services, the performance and cost of a data center is a critical factor. In this paper, we investigate the issue of increasing the resource utilization of data centers to improve their performance and lower the cost. It is an efficient way to increase resource utilization via resource sharing. Technically, server virtualization provides the opportunity to share resources in data centers. However, it also introduces other problems, the primary problem being virtual machine placement (VMP), which is to choose a proper physical machine (PM) to deploy virtual machines (VMs) in runtime. We study the virtual machine placement problem with the target of minimizing the total energy consumption by the running of PMs, which is also an indication of resource utilization and the cost of a data center. Due to the multiple dimensionality of physical resources, there always exists a waste of resources, which results from the imbalanced use of multi-dimensional resources. To characterize the multi-dimensional resource usage states of PMs, we present a multi-dimensional space partition model. Based on this model, we then propose a virtual machine placement algorithm EAGLE, which can balance the utilization of multi-dimensional resources, reduce the number of running PMs, and thus lower the energy consumption. We also evaluate our proposed balanced algorithm EAGLE via extensive simulations and experiments on real traces. Experimental results show, over the long run, that EAGLE can save as much as 15% more energy than the first fit algorithm. ',NULL,NULL,NULL,NULL),(167,NULL,NULL,'Zia ur Rehman and Omar Khadeer Hussain and Elizabeth Chang and Tharam Dillon','Rehman2015','http://dx.doi.org/10.1016/j.elerap.2015.08.002',2015,NULL,'Decision-making framework for user-based inter-cloud service migration ','http://www.sciencedirect.com/science/article/pii/S1567422315000575',NULL,8,'Abstract Cloud computing has rapidly become the most effective computing paradigm for todays increasingly technology-dependent society. The emerging concepts of federated clouds with support for interoperability between different cloud providers and open standards in cloud middleware have opened up new challenges in cloud service management. One of the emerging research areas in cloud computing is the possibility of live virtual machine migration between different clouds. This is of importance when the quality of a cloud service currently used by a user degrades or a new cloud service is developed which is better in terms of quality, performance and cost than the current service being used. In such scenarios, the user needs to make a decision as to whether to continue with the currently used service or migrate to the newly available service. In our previous work, we presented a decision-making approach that assists a cloud service user in selecting a cloud service provider based on the QoS of its services. In this paper, we extend our previous work in the pre-interaction time phase and discuss the decision-making process involved in the migration from one cloud service to another cloud service through inter-cloud virtual machine migration. ',NULL,NULL,NULL,NULL),(168,NULL,NULL,'Qinghua Zheng and Rui Li and Xiuqi Li and Nazaraf Shah and Jianke Zhang and Feng Tian and Kuo-Ming Chao and Jia Li','Zheng2015','http://dx.doi.org/10.1016/j.future.2015.02.010',2015,NULL,'Virtual machine consolidated placement based on multi-objective biogeography-based optimization ','http://www.sciencedirect.com/science/article/pii/S0167739X15000564',NULL,8,'Abstract Virtual machine placement (VMP) is an important issue in selecting most suitable set of physical machines (PMs) for a set of virtual machines (VMs) in cloud computing environment. VMP problem consists of two sub problems: incremental placement (VMiP) problem and consolidated placement (VMcP) problem. The goal of VMcP is to consolidate the VMs to more suitable PMs. The challenge in VMcP problem is how to find optimal solution effectively and efficiently especially when VMcP is a kind of NP-hard problem. In this paper, we present a novel solution to the VMcP problem called VMPMBBO. The proposed VMPMBBO treats VMcP problem as a complex system and utilizes the biogeography-based optimization (BBO) technique to optimize the virtual machine placement that minimizes both the resource wastage and the power consumption at the same time. Extensive experiments have been conducted using synthetic data from related literature and data from two real datasets. First of all, the necessity of VMcP has been proved by experimental results obtained by applying VMPMBBO. Then, the proposed method is compared with two existing multi-objective VMcP optimization algorithms and it is shown that VMPMBBO has better convergence characteristics and is more computationally efficient as well as robust. And then, the issue of parameter setting of the proposed method has been discussed. Finally, adaptability and extensibility of VMPMBBO have also been proved through experimental results. To the best of our knowledge, this work is the first approach that applies biogeography-based optimization (BBO) to virtual machine placement. ',NULL,NULL,NULL,NULL),(169,NULL,NULL,'Deshi Ye and Jianhai Chen','Ye20131345','http://dx.doi.org/10.1016/j.future.2013.02.004',2013,NULL,'Non-cooperative games on multidimensional resource allocation ','http://www.sciencedirect.com/science/article/pii/S0167739X13000320',NULL,8,'Virtualization technology is widely used in clusters, data centers and cloud computing environments and enables easy resource allocation to realize the server load balance and high consolidation by allocating virtual machines (VMs) to physical machines (PMs). In this work, motivated by the operation mechanism of the market setting, we study non-cooperative games amongst VMs on multidimensional resource allocation problems which arise in cloud computing environments. In our model, there are a set of VMs controlled by selfish agents, and each VM requires a d -dimensional vector of resource for running a job. The agent can decide to lessen its payoff by relocating the VM to another PM according to a payoff function. We consider two variants of resource allocation games: 1) server load balancing game, in which there are a number of machines, and the payoff function of an agent is the maximum load on any dimension on a machine and each agent would like to minimize its own payoff function by switching machines; 2) virtual machine placement game, in which each bin has a d -dimensional capacity, the payoff function of an agent is proportional to the portion of the bin it occupies in a given packing and the agent attempts to minimize its own payoff by choosing different bins if there is enough space available. We investigate the existence of a pure Nash equilibrium and measure the inefficiency of equilibria by the price of anarchy and the price of stability. The social cost for the server load balancing game is defined to be the maximum load on any dimension on a machine and the social cost for the virtual machine placement game is defined to be the number of bins that are not empty. We show that the price of stability is 1 for both games. The price of anarchy for the server load balancing game is at least d and at most d + 1  d / m , where m is the number of machines. The price of anarchy for the virtual machine placement game is at least d and at most d + 16 / 5 . Finally, we set up experiments to illustrate the price of anarchy, the price of stability and the average ratio of an equilibria to the optimal solution. The experiment shows that the ratios under the Google trace workload, and the uniform distribution are much smaller than the worst case ratios. ',NULL,NULL,NULL,NULL),(170,NULL,NULL,'S. Sohrabi and I. Moser','Sohrabi20152794','http://dx.doi.org/10.1016/j.procs.2015.05.436',2015,NULL,'The Effects of Hotspot Detection and Virtual Machine Migration Policies on Energy Consumption and Service Levels in the Cloud ','http://www.sciencedirect.com/science/article/pii/S1877050915012442',NULL,8,'Abstract Managing Cloud resources efficiently necessitates effective policies that assign applications to hardware in a way that they require the least resources possible. Applications are first assigned to virtual machines which are subsequently placed on the most appropriate server host. If a server becomes overloaded, some of its virtual machines are reassigned. This process requires a hotspot detection mechanism in combination with techniques that select the virtual machine(s) to migrate. In this work we introduce two new virtual machine selection policies, Median Migration Time and Maximum Utilisation, and show that they outperform existing approaches on the criteria of minimising energy consumption, service level agreement violations and the number of migrations when combined with different hotspot detection mechanisms. We show that parametrising the the hotspot detection policies correctly has a significant influence on the workload balance of the system. ',NULL,NULL,NULL,NULL),(171,NULL,NULL,'Jenn-Wei Lin and Chien-Hung Chen and Chi-Yi Lin','Lin2014478','http://dx.doi.org/10.1016/j.future.2013.12.034',2014,NULL,'Integrating QoS awareness with virtualization in cloud computing systems for delay-sensitive applications ','http://www.sciencedirect.com/science/article/pii/S0167739X13002987',NULL,8,'Abstract Cloud computing provides scalable computing and storage resources over the Internet. These scalable resources can be dynamically organized as many virtual machines (VMs) to run user applications based on a pay-per-use basis. The required resources of a VM are sliced from a physical machine (PM) in the cloud computing system. A PM may hold one or more VMs. When a cloud provider would like to create a number of VMs, the main concerned issue is the VM placement problem, such that how to place these VMs at appropriate PMs to provision their required resources of VMs. However, if two or more VMs are placed at the same PM, there exists certain degree of interference between these VMs due to sharing non-sliceable resources, e.g.I/O resources. This phenomenon is called as the VM interference. The VM interference will affect the performance of applications running in VMs, especially the delay-sensitive applications. The delay-sensitive applications have quality of service (QoS) requirements in their data access delays. This paper investigates how to integrate QoS awareness with virtualization in cloud computing systems, such as the QoS-aware VM placement (QAVMP) problem. In addition to fully exploiting the resources of PMs, the QAVMP problem considers the QoS requirements of user applications and the VM interference reduction. Therefore, in the QAVMP problem, there are following three factors: resource utilization, application QoS, and VM interference. We first formulate the QAVMP problem as an Integer Linear Programming (ILP) model by integrating the three factors as the profit of cloud provider. Due to the computation complexity of the ILP model, we propose a polynomial-time heuristic algorithm to efficiently solve the QAVMP problem. In the heuristic algorithm, a bipartite graph is modeled to represent all the possible placement relationships between VMs and PMs. Then, the VMs are gradually placed at their preferable PMs to maximize the profit of cloud provider as much as possible. Finally, simulation experiments are performed to demonstrate the effectiveness of the proposed heuristic algorithm by comparing with other VM placement algorithms. ',NULL,NULL,NULL,NULL),(172,NULL,NULL,'Amir Rahimzadeh Ilkhechi and Ibrahim Korpeoglu and Özgür Ulusoy','Ilkhechi2015508','http://dx.doi.org/10.1016/j.comnet.2015.08.042',2015,NULL,'Network-aware virtual machine placement in cloud data centers with multiple traffic-intensive components ','http://www.sciencedirect.com/science/article/pii/S1389128615003023',NULL,8,'Abstract Following a shift from computing as a purchasable product to computing as a deliverable service to consumers over the Internet, cloud computing has emerged as a novel paradigm with an unprecedented success in turning utility computing into a reality. Like any emerging technology, with its advent, it also brought new challenges to be addressed. This work studies network and traffic aware virtual machine (VM) placement in a special cloud computing scenario from a providers perspective, where certain infrastructure components have a predisposition to be the endpoints of a large number of intensive flows whose other endpoints are VMs located in physical machines (PMs). In the scenarios of interest, the performance of any VM is strictly dependent on the infrastructures ability to meet their intensive traffic demands. We first introduce and attempt to maximize the total value of a metric named satisfaction that reflects the performance of a VM when placed on a particular PM. The problem of finding a perfect assignment for a set of given VMs is NP-hard and there is no polynomial time algorithm that can yield optimal solutions for large problems. Therefore, we introduce several off-line heuristic-based algorithms that yield nearly optimal solutions given the communication pattern and flow demand profiles of subject VMs. With extensive simulation experiments we evaluate and compare the effectiveness of our proposed algorithms against each other and also against naïve approaches. ',NULL,NULL,NULL,NULL),(173,NULL,NULL,'Saad Mustafa and Babar Nazir and Amir Hayat and Atta ur Rehman Khan and Sajjad A. Madani','Mustafa2015','http://dx.doi.org/10.1016/j.compeleceng.2015.07.021',2015,NULL,'Resource management in cloud computing: Taxonomy, prospects, and challenges ','http://www.sciencedirect.com/science/article/pii/S004579061500275X',NULL,8,'Abstract Cloud computing has emerged as a popular computing paradigm for hosting large computing systems and services. Recently, significant research is carried out on Resource Management (RM) techniques that focus on the efficient sharing of cloud resources among multiple users. RM techniques in cloud are designed for computing and workload intensive applications that have different optimization parameters. This study presents a comprehensive review of RM techniques and elaborates their extensive taxonomy based on the distinct features. It highlights evaluation parameters and platforms that are used to evaluate RM techniques. Moreover, it presents design goals and research challenges that should be considered while proposing novel RM techniques. ',NULL,NULL,NULL,NULL),(174,NULL,NULL,'Bane Raman Raghunath and B. Annappa','Raghunath2015167','http://dx.doi.org/10.1016/j.procs.2015.06.019',2015,NULL,'Virtual Machine Migration Triggering using Application Workload Prediction ','http://www.sciencedirect.com/science/article/pii/S1877050915013435',NULL,8,'Abstract Dynamic provisioning of physical resources to Virtual Machines (VMs) in virtualized environments can be achieved by (i) vertical scaling-adding/removing attached resources from existing virtual machine and (ii) horizontal scaling-adding a new virtual machine with additional resources. The live migration of virtual machines across different Physical Machines (PMs) is a vertical scaling technique which facilitates resource hot-spot mitigation, server consolidation, load balancing and system level maintenance. It takes significant amount of resources to iteratively copy memory pages. Hence during the migration there may be too much overload which can affect the performance of applications running on the VMs on the physical server. It is better to predict the future workload of applications running on physical server for early detection of overloads and trigger the migration at an appropriate point where sufficient number of resources are available for all the applications so that there will not be performance degradation. This paper presents an intelligent decision maker to trigger the migration by predicting the future workload and combining it with predicted performance parameters of migration process. Experimental results shows that migration is triggered at an appropriate point such that there are sufficient amount of resources available (1520% more resources than high valued threshold method) and no application performance degradation exists as compared to properly chosen threshold method for triggering the migration. Prediction with support vector regression has got decent accuracy with MSE of 0.026. Also this system helps to improve resource utilization as compared to safer threshold value for triggering migration by removing unnecessary migrations. ',NULL,NULL,NULL,NULL),(175,1,1,'Xiaoying Wang and Zhihui Du and Yinong Chen and Mengqin Yang','Wang2015','http://dx.doi.org/10.1016/j.simpat.2015.01.005',2015,NULL,'AAAAAAAAAAAAAAA','http://www.sciencedirect.com/science/article/pii/S1569190X15000155',NULL,8,'Abstract With the rapid growth of cloud services, huge energy consumption of the underlying large-scale datacenters becomes a major concern of both the resource providers and the society. Datacenter owners are beginning to use renewable energy as extra supply for the devices. In this paper, we design a green-aware power management strategy for such datacenters powered by sustainable energy sources, considering the power consumption of both IT functional devices and cooling devices. Specifically, we make use of energy-aware methods to formulate an overall optimization problem, and try to solve it by combining heuristic and statistical searching approaches. The ultimate objective is to utilize green energy sufficiently while keeping the demand of applications deployed inside the datacenter at an acceptable level. Performance evaluation and simulation experiments are designed upon a simulated testbed, with realistic workload traces and solar energy generation considered, in order to validate the feasibility of our approach. Results show that it can significantly improve the green energy utilization, and achieve the highest overall revenues for the resource provider. ',4,NULL,NULL,NULL),(176,NULL,NULL,'Sujesha Sudevalayam and Purushottam Kulkarni','Sudevalayam20132627','http://dx.doi.org/10.1016/j.jss.2013.04.085',2013,NULL,'Affinity-aware modeling of CPU usage with communicating virtual machines ','http://www.sciencedirect.com/science/article/pii/S0164121213001246',NULL,8,'Abstract Use of virtualization in Infrastructure as a Service (IaaS) environments provides benefits to both users and providers: users can make use of resources following a pay-per-use model and negotiate performance guarantees, whereas providers can provide quick, scalable and hardware-fault tolerant service and also utilize resources efficiently and economically. With increased acceptance of virtualization-based systems, an important issue is that of virtual machine migration-enabled consolidation and dynamic resource provisioning. Effective resource provisioning can result in higher gains for users and providers alike. Most hosted applications (for example, web services) are multi-tiered and can benefit from their various tiers being hosted on different virtual machines. These mutually communicating virtual machines may get colocated on the same physical machine or placed on different machines, as part of consolidation and flexible provisioning strategies. In this work, we argue the need for network affinity-awareness in resource provisioning for virtual machines. First, we empirically quantify the change in CPU resource usage due to colocation or dispersion of communicating virtual machines for both Xen and KVM virtualization technologies. Next, we build models based on these empirical measurements to predict the change in CPU utilization when transitioning between colocated and dispersed placements. Due to the modeling process being independent of virtualization technology and specific applications, the resultant model is generic and application-agnostic. Via extensive experimentation, we evaluate the applicability of our models for synthetic and benchmark application workloads. We find that the models have high prediction accuracy  maximum prediction error within 2% absolute CPU usage. ',NULL,NULL,NULL,NULL),(177,NULL,NULL,'Fumio Machida and Dong Seong Kim and Kishor S. Trivedi','Machida2013212','http://dx.doi.org/10.1016/j.peva.2012.09.003',2013,NULL,'Modeling and analysis of software rejuvenation in a server virtualized system with live VM migration ','http://www.sciencedirect.com/science/article/pii/S0166531612000934',NULL,8,'As server virtualization is used in a number of IT systems, the unavailability of virtual machines (VM) on server virtualized systems becomes a significant concern. Software rejuvenation is a promising technique for improving the availability of server virtualized systems as it can postpone or prevent failures caused by software aging in both the VM and the underlying virtual machine monitor (VMM). In this paper, we study the effectiveness of a combination of VMM rejuvenation and live VM migration. When a VMM needs to be rejuvenated, the hosted VMs running on the VMM can be moved to another host using live VM migration and continue the execution even during the VMM rejuvenation. We call this technique Migrate-VM rejuvenation and construct an availability model in the stochastic reward net for evaluating it in comparison with the conventional approaches; Cold-VM rejuvenation and Warm-VM rejuvenation. The designed model enables us to find the optimum combinations of rejuvenation trigger intervals that maximize the availability of VM. In terms of the maximum VM availability, Migrate-VM rejuvenation is potentially the best approach. However, the advantage of Migrate-VM rejuvenation depends on the type of live VM migration (stop-and-copy or pre-copy) and the policy for migration back to the original host after VMM rejuvenation (return-back or stay-on). Through numerical examples, we show that pre-copy live VM migration is encouraged rather than pure stop-and-copy migration and it is better to return back VM to the original host soon after the VMM rejuvenation (i.e., return-back rather than stay-on policy) for high-availability. The effect of the VMM rejuvenation technique on the expected number of transactions lost is also studied by combining the availability model with an M / M / 1 / n queueing model. ',NULL,NULL,NULL,NULL),(178,NULL,NULL,'Juliano Araujo Wickboldt and Rafael Pereira Esteves and Márcio Barbosa de Carvalho and Lisandro Zambenedetti Granville','Wickboldt201454','http://dx.doi.org/10.1016/j.comnet.2014.02.018',2014,NULL,'Resource management in IaaS cloud platforms made flexible through programmability ','http://www.sciencedirect.com/science/article/pii/S138912861400084X',NULL,8,'Abstract Infrastructure as a Service (IaaS) clouds are becoming a customary way to deploy modern Internet applications. Many cloud management platforms are available for one who wants to build a private or public IaaS cloud (e.g., OpenStack, Eucalyptus, OpenNebula). A common design aspect of current platforms regards their black-box-like controlling nature, where cloud administrators have few opportunities to influence how resources are actually managed (e.g., virtual machine placement or virtual link path selection). We envision that administrators could benefit from customizations in resource management strategies to achieve environment specific objectives or to enable application oriented resource allocation. In this article, we introduce a new concept of cloud management platform where resource management is made flexible by the addition of programmability to the core of the platform, with a simplified object-oriented API. We present a proof of concept prototype and an evaluation of three resource management programs on an emulated network using Linux virtualization containers and Open vSwitch running the OpenFlow protocol. Results show the feasibility of our approach and how optimization programs were able to achieve different objectives defined by the administrator. ',NULL,NULL,NULL,NULL),(179,NULL,NULL,'Mohan Raj Velayudhan Kumar and Shriram Raghunathan','VelayudhanKumar2015','http://dx.doi.org/10.1016/j.jcss.2015.07.005',2015,NULL,'Heterogeneity and thermal aware adaptive heuristics for energy efficient consolidation of virtual machines in infrastructure clouds ','http://www.sciencedirect.com/science/article/pii/S002200001500080X',NULL,8,'Abstract Holistic datacenter energy minimization operation should consider interactions between computing and cooling source specific usage patterns. Decisions like workload type, server configuration, load, utilization etc., contributes to power consumption and influences datacenter\'s thermal profile and impacts the energy required to control temperature within operational thresholds. In this paper, we present an adaptive virtual machine placement and consolidation approach to improve energy efficiency of a cloud datacenter; accounting for server heterogeneity, server processor low-power SLEEP state, state transition latency and integrated thermal controls to maintain datacenter within operational temperature. Our proposed heuristic approach reduces energy consumption with acceptable level of performance. ',NULL,NULL,NULL,NULL),(180,NULL,NULL,'Aissan Dalvandi and Mohan Gurusamy and Kee Chaing Chua','Dalvandi2015249','http://dx.doi.org/10.1016/j.comnet.2015.06.017',2015,NULL,'Power-efficient resource-guaranteed VM placement and routing for time-aware data center applications ','http://www.sciencedirect.com/science/article/pii/S1389128615002182',NULL,8,'Abstract Power efficiency and performance guarantees have become major concerns of data center cloud providers as they significantly affect providers economic benefits. Providing guaranteed resources necessitates developing a user-friendly and concise request model which accurately abstracts the required server and network resources for a tenant application. We propose a time-aware tenant application (TTA) request model which enables a tenant to express an application request by specifying its resource requirement graph (server resources for VMs and bandwidth) associated with its estimated required time duration. We investigate the power-efficient resource-guaranteed virtual machine (VM)-placement and routing problem for dynamically arriving TTA requests. The problem requires provisioning of the specified resources in a data center for the required time duration of requests by selecting an appropriate set of servers for VM placement and routes for their communication, so as to maximize the number of accepted requests while consuming as low power as possible. We develop a mixed integer linear programming optimization problem formulation based on the multi-component utilization-based power model. Since this problem which is a combination of routing and VM-placement problem, is computationally prohibitive, we develop two algorithms which select servers and routes based on: (1) our proposed goodness function and pre-computed candidate paths, and (2) minimum power cost paths, respectively. We demonstrate the effectiveness of the proposed algorithms in terms of power saving and acceptance ratio through simulation results. ',NULL,NULL,NULL,NULL),(181,NULL,NULL,'Narander Kumar and Swati Saxena','Kumar2015823','http://dx.doi.org/10.1016/j.procs.2015.03.163',2015,NULL,'Migration Performance of Cloud Applications- A Quantitative Analysis ','http://www.sciencedirect.com/science/article/pii/S1877050915003993',NULL,8,'Abstract Performance of a cloud data centre must abide the Service Level Agreement parameters and must provide negotiated Quality of Service values. One of the key areas in cloud computing, where the possibility of performance tuning is the maximum, is live virtual machine migration. Migration entails time and traffic which are crucial factors as far as the QoS is concerned. This paper focuses on quantitative analysis of live migration within a cloud data centre with the aim of understanding the factors which are responsible for cloud\'s efficiency. Various key parameters, such as, virtual machine size, network bandwidth available and dirty rate of a cloud application are discussed in detail and given the comparisons also, to give a clear view of their role in live migration\'s performance. The analysis presented in this paper gives a proper platform for considering future enhancements and/or modifications in the existing migration technology. ',NULL,NULL,NULL,NULL),(182,NULL,NULL,'Gang Sun and Dan Liao and Vishal Anand and Dongcheng Zhao and Hongfang Yu','Sun201674','http://dx.doi.org/10.1016/j.future.2015.09.005',2016,NULL,'A new technique for efficient live migration of multiple virtual machines ','http://www.sciencedirect.com/science/article/pii/S0167739X15002848',NULL,8,'Abstract Datacenter virtualization technologies have attracted a lot of attention to enable various cloud computing services and to facilitate virtual machine (VM) migration. VM migration can help service providers to achieve the goals of saving energy, enhancing resource efficiency and quality of service (QoS). In order to ensure the QoS, the migration time and the downtime of VM should be considered while implementing the VM migration. Most researches focus on the issue of single VM migration by using the post-copy migration strategy or pre-copy migration strategy. However, there is few research that focuses on the problem of live migration for multiple VMs. Therefore, in this paper, we first propose an improved serial migration strategy and introduce the post-copy migration scheme into it. We then propose the m mixed migration strategy that is based on the improved serial migration strategy and the parallel migration strategy. Furthermore, we develop queuing models (i.e., the M/M/C/C and the M/M/C queuing models) to quantify performance metrics, such as the blocking ratio and average waiting time of each migration request. We evaluate the performance of the proposed migration strategy by conducting mathematical analysis, the numerical results show that our proposed strategy outperforms the existing approach. ',NULL,NULL,NULL,NULL),(183,NULL,NULL,'Jun-jie TONG and Hai-hong E and Mei-na SONG and Jun-de SONG','TONG201440','http://dx.doi.org/10.1016/S1005-8885(14)60314-9',2014,NULL,'Host load prediction in cloud based on classification methods ','http://www.sciencedirect.com/science/article/pii/S1005888514603149',NULL,8,'Abstract The host load prediction problem in cloud computing has also been received much attention. To solve this problem, we have to use the historical load data to predict the future load level. Accurate prediction methods are useful for host load balance and virtual machine migration. Although cloud is likely to grids at some extent, the length of tasks are much shorter and host loads change more frequently with higher noise. The above characteristics introduce challenges for host load prediction. In this paper, based on the proposed exponentially segmented pattern and the Corresponding transformation, prediction problem is transformed into the traditional classification problem. This classification problem can be solved based on the traditional methods, and features are given for training the classification model. For achieving accurate prediction, a new feature periodical coefficient is introduced and some existed classification methods are implemented. Experiments on the real world dataset invalidate the efficiency of the new proposed feature, which is in the most effective combinations of features, it increases successful rate (SR) 1.33%~2.82% and decreases the mean square error (MSE) 1.37%~2.91%. And the results also show that support vector machine (SVM) method can achieve nearly the same performance as the Bayes methods and their performance is about 50% higher in successful rate and 17% better in the mean square error compared to the existed methods. ',NULL,NULL,NULL,NULL),(184,NULL,NULL,'Hai Jin and Wei Gao and Song Wu and Xuanhua Shi and Xiaoxin Wu and Fan Zhou','Jin20111088','http://dx.doi.org/10.1016/j.jnca.2010.06.013',2011,NULL,'Optimizing the live migration of virtual machine by CPU scheduling ','http://www.sciencedirect.com/science/article/pii/S1084804510001116',NULL,8,'Live migration has been proposed to reduce the downtime for migrated VMs by pre-copying the generated run-time memory state files from the original host to the migration destination host. However, if the rate for such a dirty memory generation is high, it may take a long time to accomplish live migration because a large amount of data needs to be transferred. In extreme cases when dirty memory generation rate is faster than pre-copy speed, live migration will fail. In this work we address the problem by designing an optimization scheme for live migration, under which according to pre-copy speed, the VCPU working frequency may be reduced so that at a certain phase of the pre-copy the remaining dirty memory can reach a desired small amount. The VM downtime during the migration can be limited. The scheme works for the scenario where the migrated application has a high memory writing speed, or the pre-copy speed is slow, e.g., due to low network bandwidth between the migration parties. The method improves migration liveness at the cost of application performance, and works for those applications for which interruption causes much more serious problems than quality deterioration. Compared to the original live migration, our experiments show that the optimized scheme can reduce up to 88% of application downtime with an acceptable overhead. ',NULL,NULL,NULL,NULL),(185,NULL,NULL,'Saurabh Kumar Garg and Adel Nadjaran Toosi and Srinivasa K. Gopalaiyengar and Rajkumar Buyya','Garg2014108','http://dx.doi.org/10.1016/j.jnca.2014.07.030',2014,NULL,'SLA-based virtual machine management for heterogeneous workloads in a cloud datacenter ','http://www.sciencedirect.com/science/article/pii/S1084804514001787',NULL,8,'Abstract Efficient provisioning of resources is a challenging problem in cloud computing environments due to its dynamic nature and the need for supporting heterogeneous applications. Even though VM (Virtual Machine) technology allows several workloads to run concurrently and to use a shared infrastructure, still it does not guarantee application performance. Thus, currently cloud datacenter providers either do not offer any performance guarantee or prefer static VM allocation over dynamic, which leads to inefficient utilization of resources. Moreover, the workload may have different QoS (Quality Of Service) requirements due to the execution of different types of applications such as HPC and web, which makes resource provisioning much harder. Earlier work either concentrate on single type of SLAs (Service Level Agreements) or resource usage patterns of applications, such as web applications, leading to inefficient utilization of datacenter resources. In this paper, we tackle the resource allocation problem within a datacenter that runs different types of application workloads, particularly non-interactive and transactional applications. We propose an admission control and scheduling mechanism which not only maximizes the resource utilization and profit, but also ensures that the QoS requirements of users are met as specified in SLAs. In our experimental study, we found that it is important to be aware of different types of SLAs along with applicable penalties and the mix of workloads for better resource provisioning and utilization of datacenters. The proposed mechanism provides substantial improvement over static server consolidation and reduces SLA violations. ',NULL,NULL,NULL,NULL),(186,NULL,NULL,'L. Velasco and A. Asensio and J.Ll. Berral and E. Bonetto and F. Musumeci and V. López','Velasco2014142','http://dx.doi.org/10.1016/j.comcom.2014.03.004',2014,NULL,'Elastic operations in federated datacenters for performance and cost optimization ','http://www.sciencedirect.com/science/article/pii/S0140366414000905',NULL,8,'Abstract The huge energy consumption of datacenters providing cloud services over the Internet has motivated different studies regarding cost savings in datacenters. Since energy expenditure is a predominant part of the total operational expenditures for datacenter operators, energy aware policies for minimizing datacenters energy consumption try to minimize energy costs while guaranteeing a certain quality of experience (QoE). Federated datacenters can take advantage of its geographically distributed infrastructure by managing appropriately the green energy resources available in each datacenter at a given time, in combination with workload consolidation and virtual machine migration policies. In this scenario, inter-datacenter networks play an important role and communications cost must be considered when minimizing operational expenditures. In this work we tackle the Elastic Operations in Federated Datacenter for Performance and Cost Optimization (ELFADO) problem for scheduling workload orchestrating federated datacenters. Two approaches, distributed and centralized, are studied and integer linear programming (ILP) formulations and heuristics are provided. Using those heuristics, we analyze cost savings with respect to a fixed workload placement. For the sake of a compelling analysis, exhaustive simulation experiments are carried out considering realistic scenarios. Results show that the centralized ELFADO approach can save up to 52% of energy cost and more than 44% when communication costs are also considered. ',NULL,NULL,NULL,NULL),(187,NULL,NULL,'Francesco Palmieri and Ugo Fiore and Sergio Ricciardi and Aniello Castiglione','Palmieri2015','http://dx.doi.org/10.1016/j.future.2015.01.017',2015,NULL,'GRASP-based resource re-optimization for effective big data access in federated clouds ','http://www.sciencedirect.com/science/article/pii/S0167739X15000345',NULL,8,'Abstract Federated cloud organizations, spanning across multiple networked sites that provide both computing and storage resources, can be considered the state-of-the-art solutions for providing multi-tenant runtime services in modern distributed processing environments. In these scenarios, by re-optimizing the communication paths between virtual machines and big data sources, at evenly spaced interval or when required by circumstances, the overall communication and runtime resource utilization on the cloud infrastructure is re-balanced, so that more virtual machines can be allowed to access the needed big data sources with adequate bandwidth, thereby significantly improving the perceived performance and quality of service. The problem of re-optimization is tackled with a powerful meta-heuristic, the greedy randomized adaptive search procedure (GRASP), augmented by path re-linking. In order to evaluate the proposed approach, extensive simulations have been performed, leading to very interesting results, demonstrating the effectiveness and validity of the underlying ideas and their applicability to real large-scale federated cloud scenarios. ',NULL,NULL,NULL,NULL),(188,NULL,NULL,'Lu Lu and Xuanhua Shi and Hai Jin and Qiuyue Wang and Daxing Yuan and Song Wu','Lu201480','http://dx.doi.org/10.1016/j.future.2013.12.026',2014,NULL,'Morpho: A decoupled MapReduce framework for elastic cloud computing ','http://www.sciencedirect.com/science/article/pii/S0167739X13002902',NULL,8,'Abstract MapReduce as a service enjoys wide adoption in commercial clouds today [3,23]. But most cloud providers just deploy native Hadoop [24] systems on their cloud platforms to provide MapReduce services without any adaptation to these virtualized environments [6,25]. In cloud environments, the basic executing units of data processing are virtual machines. Each users virtual cluster needs to deploy HDFS [26] every time when it is initialized, while the users input and output data should be transferred between the HDFS and external persistent data storage to ensure that the native Hadoop works properly. These costly data movements can lead to significant performance degradation of MapReduce jobs in the cloud. We present Morphoa modified version of the Hadoop MapReduce framework, which decouples storage and computation into physical clusters and virtual clusters respectively. In Morpho, the map/reduce tasks are still running in VMs without corresponding ad-hoc HDFS deployments; instead, HDFS is deployed on the underlying physical machines. When MapReduce computation is performing, the map tasks can get data directly from physical machines without any extra data transfers. We design data location perception module to improve the cooperativity of the computation and storage layers, which means that the map tasks can intelligently fetch information about the network topology of physical machines and the VM placements. Additionally, Morpho also achieves high performance by two complementary strategies for data placement and VM placement, which can provide better map and reduce input locality. Furthermore, our data placement strategy can mitigate the resource contentions between jobs. The evaluation of our Morpho system prototype shows it achieves a nearly 62% speedup of job execution time and a significant reduction in network traffic of the entire system compared with the traditional cloud computing scheme of Amazon and other cloud providers. ',NULL,NULL,NULL,NULL),(189,NULL,NULL,'Gregory Katsaros and Josep Subirats and J. Oriol Fitó and Jordi Guitart and Pierre Gilet and Daniel Espling','Katsaros20132077','http://dx.doi.org/10.1016/j.future.2012.12.006',2013,NULL,'A service framework for energy-aware monitoring and VM management in Clouds ','http://www.sciencedirect.com/science/article/pii/S0167739X12002269',NULL,8,'The monitoring of QoS parameters in Services Computing as well as in Clouds has been a functionality provided by all contemporary systems. As the optimization of energy consumption becomes a major concern for system designers and administrators, it can be considered as another QoS metric to be monitored. In this paper, we present a service framework that allows us to monitor the energy consumption of a Cloud infrastructure, calculate its energy efficiency, and evaluate the gathered data in order to put in place an effective virtual machine (VM) management. In that context, a simulation scenario of an eco-driven VM placement policy resulted in a 14% improvement of the infrastructures energy efficiency. In total, the proposed approaches and implementations have been validated against a testbed, producing very promising results regarding the prospect of energy efficiency as an important quality factor in Clouds. ',NULL,NULL,NULL,NULL),(190,NULL,NULL,'Nikos Tziritas and Samee Ullah Khan and Cheng-Zhong Xu and Thanasis Loukopoulos and Spyros Lalis','Tziritas20131690','http://dx.doi.org/10.1016/j.jpdc.2013.07.020',2013,NULL,'On minimizing the resource consumption of cloud applications using process migrations ','http://www.sciencedirect.com/science/article/pii/S0743731513001585',NULL,8,'Abstract According to the pay-per-use model adopted in clouds, the more resources an application running in a cloud computing environment consumes, the greater the amount of money the owner of the corresponding application will be charged. Therefore, applying intelligent solutions to minimize the resource consumption is of great importance. In this paper, we study the problem of identifying an assignment scheme between the interacting components of an application, such as processes and virtual machines, and the computing nodes of a cloud system, such that the total amount of resources consumed by the respective application is minimized. Because centralized solutions are deemed unsuitable for large distributed systems or large-scale applications, we propose a fully distributed algorithm (called DRA) to overcome scalability issues. DRA takes decisions concerning the transition from one assignment scheme to another in a dynamic way, based solely on local information. We also propose and test two modifications of the basic DRA algorithm to deal better with the heterogeneity of cloud servers in terms of capacity constraints. We must note that we capture heterogeneity regarding the network model. Through theoretical analysis, we formally prove that DRA achieves convergence and always provides an optimal solution for tree-based networks in the uncapacitated case. Moreover, we prove through experimental evaluation that DRA achieves up to 55% network cost reduction when compared to the most recent algorithm in the literature. We also show that the proposed modifications of DRA improve the algorithms performance considerably in the case where servers have limited capacity. ',NULL,NULL,NULL,NULL),(191,NULL,NULL,'Tiago C. Ferreto and Marco A.S. Netto and Rodrigo N. Calheiros and César A.F. De Rose','Ferreto20111027','http://dx.doi.org/10.1016/j.future.2011.04.016',2011,NULL,'Server consolidation with migration control for virtualized data centers ','http://www.sciencedirect.com/science/article/pii/S0167739X11000677',NULL,8,'Virtualization has become a key technology for simplifying service management and reducing energy costs in data centers. One of the challenges faced by data centers is to decide when, how, and which virtual machines (VMs) have to be consolidated into a single physical server. Server consolidation involves VM migration, which has a direct impact on service response time. Most of the existing solutions for server consolidation rely on eager migrations, which try to minimize the number of physical servers running VMs. These solutions generate unnecessary migrations due to unpredictable workloads that require VM resizing. This paper proposes an LP formulation and heuristics to control VM migration, which prioritize virtual machines with steady capacity. We performed experiments using TU-Berlin and Google data center workloads to compare our migration control strategy against existing eager-migration-based solutions. We observed that avoiding migration of VMs with steady capacity reduces the number of migrations with minimal penalty in the number of physical servers. ',NULL,NULL,NULL,NULL),(192,NULL,NULL,'Xiaolin Xu and Hai Jin and Song Wu and Yihong Wang','Xu201575','http://dx.doi.org/10.1016/j.future.2014.10.004',2015,NULL,'Rethink the storage of virtual machine images in clouds ','http://www.sciencedirect.com/science/article/pii/S0167739X14001885',NULL,8,'Abstract As one of the most prevalent cloud services, Infrastructure-as-a-Service provides virtual machines (VMs) with high flexibility. How to effectively manage a huge amount of VM images becomes a big challenge. On one hand, images affect VMs disk IO performance significantly, which is essential to the quality of services, especially for those having intensive disk IO workloads. On the other hand, they consume many storage resources and cause much management cost, which is cared by cloud managers. Current ways to optimize images usually focus on either improving performance or decreasing image size, which unfortunately cannot satisfy the requirements of high IO performance, low storage consumption, and low management cost simultaneously. Typically, high IO performance requires images storing close to VMs, but this increases redundant data and consumes extra storage at the same time. Besides, a closer image means more data stored in local disks rather than a normal shared storage, which increases management cost as well. In this paper, we analyze these requirements and potential tradeoffs among them, and propose Zone-based model to well balance the requirements. We partition computing nodes into many zones, and construct a shared storage in each zone to cache hot data for high IO performance and low storage consumption. In addition, we improve the normal Copy-on-Write and cache mechanisms, providing new image types and cache functions to enhance the eventual effectiveness. The evaluations show that, our solution improves IO performance by more than 100% in general and even 10 times while adopting a friendly VM placement strategy, and gets close or less storage consumption and management cost than the other models at the same time. ',NULL,NULL,NULL,NULL),(193,NULL,NULL,'Josep Subirats and Jordi Guitart','Subirats201570','http://dx.doi.org/10.1016/j.future.2014.11.008',2015,NULL,'Assessing and forecasting energy efficiency on Cloud computing platforms ','http://www.sciencedirect.com/science/article/pii/S0167739X14002428',NULL,8,'Abstract IaaS providers have become interested in optimising their infrastructure energy efficiency. To do so, their VM placement algorithms need to know the current and future energy efficiency at different levels (Virtual Machine, node, infrastructure and service levels) and for potential actions such as service deployment or VM deployment, migration or cancellation. This publication provides a mathematical formulation for the previous aspects, as well as the design of a CPU utilisation estimator used to calculate the aforementioned forecasts. The correct adjustment of the estimators configuration parameters has been proved to lead to considerable precision improvements. When running Web workloads, estimators focused on noise filtering provide the best precision even if they react slowly to changes, whereas reactive predictors are desirable for batch workloads. Furthermore, the precision when running batch workloads partially depends on each execution. Finally, it has been observed that the forecasts precision degradation as such forecasts are performed for a longer time period in the future is smaller when running web workloads. ',NULL,NULL,NULL,NULL),(194,NULL,NULL,'Gregory Katsaros and Pascal Stichler and Josep Subirats and Jordi Guitart','Katsaros2015','http://dx.doi.org/10.1016/j.future.2015.01.002',2015,NULL,'Estimation and forecasting of ecological efficiency of virtual machines ','http://www.sciencedirect.com/science/article/pii/S0167739X15000035',NULL,8,'Abstract The massive development of the cloud marketplace is leading to an increase in the number of the Data Centers (DCs) globally and eventually to an increase of the CO 2 related footprint. The calculation of the impact of Virtual Machines (VMs) on the environment is a challenging task, not only due to the technical difficulties but also due to the lack of information from the energy providers. The ecological efficiency of a system captures the relationship between the performance of the system with its environmental footprint. In this paper we present a methodology for the estimation and prediction of the ecological efficiency of VMs in private cloud infrastructures. We specifically focus on the information management starting from the energy resources in a region, the energy consumption and the performance of the resources and finally the calculation of ecological efficiency of a VM. To this end, we have designed and implemented a framework through which the ecological efficiency of a running VM can be assessed and the ecological efficiency of a VM to be deployed can be forecasted. The presented framework is being evaluated through several private cloud scenarios with VM deployments in hosts located in Germany. ',NULL,NULL,NULL,NULL),(195,NULL,NULL,'Corentin Dupont and Fabien Hermenier and Thomas Schulze and Robert Basmadjian and Andrey Somov and Giovanni Giuliani','Dupont2015505','http://dx.doi.org/10.1016/j.adhoc.2014.11.003',2015,NULL,'Plug4Green: A flexible energy-aware VM manager to fit data centre particularities ','http://www.sciencedirect.com/science/article/pii/S1570870514002376',NULL,8,'Abstract To maintain an energy footprint as low as possible, data centres manage their VMs according to conventional and established rules. Each data centre is however made unique due to its hardware and workload specificities. This prevents the ad hoc design of current VM managers from taking these particularities into account to provide additional energy savings. In this paper, we present Plug4Green, an energy-aware VM placement algorithm that can be easily specialized and extended to fit the specificities of the data centres. Plug4Green computes the placement of the VMs and state of the servers depending on a large number of constraints, extracted automatically from SLAs. The flexibility of Plug4Green is achieved by allowing the constraints to be formulated independently from each other but also from the power models. This flexibility is validated through the implementation of 23 SLA constraints and 2 objectives aiming at reducing either the power consumption or the greenhouse gas emissions. On a heterogeneous test bed, Plug4Green specialization to fit the hardware and the workload specificities allowed to reduce the energy consumption and the gas emission by up to 33% and 34%, respectively. Finally, simulations showed that Plug4Green is capable of computing an improved placement for 7500 VMs running on 1500 servers within a minute. ',NULL,NULL,NULL,NULL),(196,NULL,NULL,'Andreas Wolke and Boldbaatar Tsend-Ayush and Carl Pfeiffer and Martin Bichler','Wolke201583','http://dx.doi.org/10.1016/j.is.2015.03.003',2015,NULL,'More than bin packing: Dynamic resource allocation strategies in cloud data centers ','http://www.sciencedirect.com/science/article/pii/S0306437915000472',NULL,8,'Abstract Resource allocation strategies in virtualized data centers have received considerable attention recently as they can have substantial impact on the energy efficiency of a data center. This led to new decision and control strategies with significant managerial impact for IT service providers. We focus on dynamic environments where virtual machines need to be allocated and deallocated to servers over time. Simple bin packing heuristics have been analyzed and used to place virtual machines upon arrival. However, these placement heuristics can lead to suboptimal server utilization, because they cannot consider virtual machines, which arrive in the future. We ran extensive lab experiments and simulations with different controllers and different workloads to understand which control strategies achieve high levels of energy efficiency in different workload environments. We found that combinations of placement controllers and periodic reallocations achieve the highest energy efficiency subject to predefined service levels. While the type of placement heuristic had little impact on the average server demand, the type of virtual machine resource demand estimator used for the placement decisions had a significant impact on the overall energy efficiency. ',NULL,NULL,NULL,NULL),(197,NULL,NULL,'Mario Macías and Jordi Guitart','Macías201419','http://dx.doi.org/10.1016/j.future.2014.03.004',2014,NULL,'SLA negotiation and enforcement policies for revenue maximization and client classification in cloud providers ','http://www.sciencedirect.com/science/article/pii/S0167739X14000491',NULL,8,'Abstract In Cloud Computing markets, owners of computing resources negotiate with their potential clients to sell computing power. The terms of the Quality of Service (QoS) to be provided as well as the economic conditions are established in a Service-Level Agreement (SLA). The common objective of a Cloud provider is to maximize its economic profit. However, there are situations in which providers must differentiate the SLAs with respect to the type of client that is willing to access the resources or the agreed QoS, e.g. when the hardware resources are shared between users of the company that own the resources and external users. This article proposes two sets of policies to manage SLAs with respect to the business objectives of a Cloud provider: Revenue Maximization or classification of clients. The criterion to classify clients is established according to the relationship between client and provider (external user, internal or another privileged relationship) and the QoS that the client purchases (cheap contracts or extra QoS by paying an extra fee). The validity of the policies is demonstrated through exhaustive experiments. ',NULL,NULL,NULL,NULL),(198,NULL,NULL,'Boris Teabe and Alain Tchana and Daniel Hagimont','Teabe20151','http://dx.doi.org/10.1016/j.future.2015.05.013',2015,NULL,'Enforcing CPU allocation in a heterogeneous IaaS ','http://www.sciencedirect.com/science/article/pii/S0167739X15001880',NULL,8,'Abstract In an Infrastructure as a Service (IaaS), the amount of resources allocated to a virtual machine (VM) at creation time may be expressed with relative values (relative to the hardware, i.e., a fraction of the capacity of a device) or absolute values (i.e., a performance metric which is independent from the capacity of the hardware). Surprisingly, disk or network resource allocations are expressed with absolute values (bandwidth), but CPU resource allocations are expressed with relative values (a percentage of a processor). The major problem with CPU relative value allocations is that it depends on the capacity of the CPU, which may vary due to different factors (server heterogeneity in a cluster, Dynamic Voltage Frequency Scaling (DVFS)). In this paper, we analyze the side effects and drawbacks of relative allocations. We claim that CPU allocation should be expressed with absolute values. We propose such a CPU resource management system and we demonstrate and evaluate its benefits. ',NULL,NULL,NULL,NULL),(199,NULL,NULL,'Wei Lin Guay and Sven-Arne Reinemo and Bjørn Dag Johnsen and Chien-Hua Yen and Tor Skeie and Olav Lysne and Ola Tørudbakken','Guay201539','http://dx.doi.org/10.1016/j.jpdc.2015.01.004',2015,NULL,'Early experiences with live migration of SR-IOV enabled InfiniBand ','http://www.sciencedirect.com/science/article/pii/S0743731515000052',NULL,8,'Abstract Virtualization is the key to efficient resource utilization and elastic resource allocation in cloud computing. It enables consolidation, the on-demand provisioning of resources, and elasticity through live migration. Live migration makes it possible to optimize resource usage by moving virtual machines (VMs) between physical servers in an application transparent manner. It does, however, require a flexible, high-performance, scalable virtualized I/O architecture to reach its full potential. This is challenging to achieve with high-speed networks such as InfiniBand and remote direct memory access enhanced Ethernet, because these devices usually maintain their connection state in the network device hardware. Fortunately, the single root IO virtualization (SR-IOV) specification addresses the performance and scalability issues. With SR-IOV, each VM has direct access to a hardware assisted virtual device without the overhead introduced by emulation or para-virtualization. However, SR-IOV does not address the migration of the network device state. In this paper we present and evaluate the first available prototype implementation of live migration over SR-IOV enabled InfiniBand devices. ',NULL,NULL,NULL,NULL),(200,NULL,NULL,'Yaozu Dong and Xiaowei Yang and Jianhui Li and Guangdeng Liao and Kun Tian and Haibing Guan','Dong20121471','http://dx.doi.org/10.1016/j.jpdc.2012.01.020',2012,NULL,'High performance network virtualization with SR-IOV ','http://www.sciencedirect.com/science/article/pii/S0743731512000329',NULL,8,'Virtualization poses new challenges to I/O performance. The single-root I/O virtualization (SR-IOV) standard allows an I/O device to be shared by multiple Virtual Machines (VMs), without losing performance. We propose a generic virtualization architecture for SR-IOV-capable devices, which can be implemented on multiple Virtual Machine Monitors (VMMs). With the support of our architecture, the SR-IOV-capable device driver is highly portable and agnostic of the underlying VMM. Because the Virtual Function (VF) driver with SR-IOV architecture sticks to hardware and poses a challenge to VM migration, we also propose a dynamic network interface switching (DNIS) scheme to address the migration challenge. Based on our first implementation of the network device driver, we deployed several optimizations to reduce virtualization overhead. Then, we conducted comprehensive experiments to evaluate SR-IOV performance. The results show that SR-IOV can achieve a line rate throughput (9.48Gbps) and scale network up to 60 VMs, at the cost of only 1.76% additional CPU overhead per VM, without sacrificing throughput and migration. ',NULL,NULL,NULL,NULL),(201,NULL,NULL,'W. Lloyd and S. Pallickara and O. David and J. Lyon and M. Arabi and K. Rojas','Lloyd20131254','http://dx.doi.org/10.1016/j.future.2012.12.007',2013,NULL,'Performance implications of multi-tier application deployments on Infrastructure-as-a-Service clouds: Towards performance modeling ','http://www.sciencedirect.com/science/article/pii/S0167739X12002270',NULL,8,'Hosting a multi-tier application using an Infrastructure-as-a-Service (IaaS) cloud requires deploying components of the application stack across virtual machines (VMs) to provide the applications infrastructure while considering factors such as scalability, fault tolerance, performance and deployment costs (# of VMs). This paper presents results from an empirical study which investigates implications for application performance and resource requirements (CPU, disk and network) resulting from how multi-tier applications are deployed to IaaS clouds. We investigate the implications of: (1) component placement across VMs, (2) VM memory size, (3) VM hypervisor type (KVM vs. Xen), and (4) VM placement across physical hosts (provisioning variation). All possible deployment configurations for two multi-tier application variants are tested. One application variant was computationally bound by the application middleware, the other bound by geospatial queries. The best performing deployments required as few as 2 VMs, half the number required for VM-level service isolation, demonstrating potential cost savings when components can be consolidated. Resource utilization (CPU time, disk I/O, and network I/O) varied with component deployment location, VM memory allocation, and the hypervisor used (Xen or KVM) demonstrating how application deployment decisions impact required resources. Isolating application components using separate VMs produced performance overhead of 1%2%. Provisioning variation of VMs across physical hosts produced overhead up to 3%. Relationships between resource utilization and performance were assessed using multiple linear regression to develop a model to predict application deployment performance. Our model explained over 84% of the variance and predicted application performance with mean absolute error of only 0.3s with CPU time, disk sector reads, and disk sector writes serving as the most powerful predictors of application performance. ',NULL,NULL,NULL,NULL),(202,NULL,NULL,'Gursharan Singh and Sunny Behal and Monal Taneja','Singh201591','http://dx.doi.org/10.1016/j.procs.2015.07.373',2015,NULL,'Advanced Memory Reusing Mechanism for Virtual Machines in Cloud Computing ','http://www.sciencedirect.com/science/article/pii/S187705091501902X',NULL,8,'Abstract Cloud computing is an emerging computing technology for large data centers that maintains computational resources through the internet, rather than on local computers. With the increasing popularity of cloud computing, also increase the demand of cloud resources. In Infrastructure-as-a-Service datacenters, the placements of Virtual Machines (VMs) on physical hosts are dynamically optimized in response to resource utilization of the hosts. VM migration provides the capability to balance the load, system maintenance and fault tolerance etc. However, existing migration techniques, used to migrate virtual machines keeping data images of VMs in host and skipping transfer of unchanged data fields to reduce the amount of transfer data during migration, if number of migrations increases, number of data images stored on host are also increased, this causes memory starvation. In this paper, we propose a technique that reduces the size of data image stored on source host before migration. When a Virtual Machine migrates to another host, the data image for that VM is kept in the source host after removing unwanted data according to the probability factor. When the VM migrates back to the original host later, the kept memory image will be reused, i.e. data which are identical to the kept data will not be transferred and comparative to existing system the size of memory image is small. To validate this approach, results evaluated using different threshold levels and probability factor of change in data. Proposed system required less memory to store the memory image and allow more VMs to be hosted. Specifically, our proposed work is used to improve resource efficiency throughout by reducing the size of memory image that is stored on source host. Evaluations show that size of memory image reduced 33% (approx) of unnecessary memory consumption. ',NULL,NULL,NULL,NULL),(203,NULL,NULL,'R. Jeyarani and N. Nagaveni and R. Vasanth Ram','Jeyarani2012811','http://dx.doi.org/10.1016/j.future.2011.06.002',2012,NULL,'Design and implementation of adaptive power-aware virtual machine provisioner (APA-VMP) using swarm intelligence ','http://www.sciencedirect.com/science/article/pii/S0167739X11001130',NULL,8,'Cloud computing aims at providing dynamic leasing of server capabilities as scalable, virtualized services to end users. Our work focuses on the Infrastructure as a Service (IaaS) model where custom Virtual Machines (VM) are launched in appropriate servers available in a data center. The cloud data center taken into consideration is heterogeneous and large scale in nature. Such a resource pool is basically characterized by high resource dynamics caused by non-linear variation in the availability of processing elements, memory size, storage capacity, bandwidth and power drawn resulting from the sporadic nature of workload. Apart from the said resource dynamics, our proposed work also considers the processor transitions to various sleep states and their corresponding wake up latencies that are inherent in contemporary enterprise servers. The primary objective of the proposed metascheduler is to map efficiently a set of VM instances onto a set of servers from a highly dynamic resource pool by fulfilling resource requirements of maximum number of workloads. As the cloud data centers are overprovisioned to meet the unexpected workload surges, huge power consumption has become one of the major issues of concern. We have proposed a novel metascheduler called Adaptive Power-Aware Virtual Machine Provisioner (APA-VMP) that schedules the workload in such a way that the total incremental power drawn by the server pool is minimum without compromising the performance objectives. The APA-VMP makes use of swarm intelligence methodology to detect and track the changing optimal target servers for VM placement very efficiently. The scenario was experimented by novel Self-adaptive Particle Swarm Optimization (SAPSO) for VM provisioning, which makes best possible use of the power saving states of idle servers and instantaneous workload on the operational servers. It is evident from the results that there is a significant reduction in the power numbers against the existing strategies. ',NULL,NULL,NULL,NULL),(249,NULL,NULL,'Fung Po Tso and Hamilton, G. and Oikonomou, K. and Pezaros, D.P.','6676740','10.1109/CLOUD.2013.82',2013,0,'Implementing Scalable, Network-Aware Virtual Machine Migration for Cloud Data Centers',NULL,1,1,'Virtualization has been key to the success of Cloud Computing through the on-demand allocation of shared hardware resources to Virtual Machines (VM)s. However, the network-agnostic placement of VMs over the underlying network topology can itself be a factor of performance degradation by causing congestion at the core layers of the infrastructure where bandwidth is heavily oversubscribed. In this paper, we design and implement S-CORE, a scalable live VM migration scheme to dynamically reallocate VMs to servers while minimizing the overall communication footprint of active traffic flows. We evaluate S- CORE over diverse aggregate load and coordination policies. Our results show that it can achieve up to a 87% communication cost reduction with a limited number of migration rounds, and can be easily accommodated within commodity hardware and hypervisor architectures. The associated memory, CPU, and network overhead are also minimum under typical Cloud Data Center workloads.',4,NULL,'S-CORE','Tso et al.'),(250,NULL,NULL,'Xu, F. and Liu, F. and Liu, L. and Jin, H. and Li, B. and Li, B.','Xu2013','10.1109/TC.2013.185',2013,0,'iAware: Making Live Migration of Virtual Machines Interference-Aware in the Cloud',NULL,2,1,'While recent studies have primarily focused on harnessing live migration of virtual machines (VMs) to achieve load balancing and power saving among different servers, there has been a lack of attention devoted to the incurred performance interference and cost on both source and destination servers during and after the migration of VMs. To avoid potential violations of service-level-agreement (SLA) demanded by cloud applications, this paper proposes iAware, a lightweight interference-aware VM live migration strategy. It empirically captures and understands the relationships between VM performance interference and key influential factors that are practically accessible, through realistic experiments of benchmark workloads on a Xen virtualized cluster platform. Guided by such relationships, iAware jointly estimates and minimizes both migration and co-location interference among VMs, by designing a simple multi-resource demand-supply model. Extensive experiments and complementary large-scale simulations are conducted to validate the performance gain and running overhead of iAware in terms of I/O and network throughputs, CPU consumption and scalability, compared with traditional interference-unaware VM migration approaches. Moreover, iAware is flexible enough to cooperate with existing VM scheduling or consolidation policies in a complementary manner, in order to achieve load balancing or power saving without sacrificing performance in the cloud.',4,'Antigo citationKey: 6598665','iAware','Xu et al.'),(251,NULL,NULL,'Marzolla, M. and Babaoglu, O. and Panzieri, F.','5986483','10.1109/WoWMoM.2011.5986483',2011,0,'Server consolidation in Clouds through gossiping',NULL,1,1,'The success of Cloud computing, where computing power is treated as a utility, has resulted in the creation of many large data centers that are very expensive to build and operate. In particular, the energy bill accounts for a significant fraction of the total operation costs. For this reason a significant attention is being devoted to energy conservation techniques, for example by taking advantage of the built-in power saving features of modern hardware. Cloud computing offers novel opportunities for achieving energy savings: Cloud systems rely on virtualization techniques to allocate computing resources on demand, and modern Virtual Machine (VM) monitors allow live migration of running VMs. Thus, energy conservation can be achieved through server consolidation, moving VM instances away from lightly loaded computing nodes so that they become empty and can be switched to low-power mode. In this paper we present V-MAN, a fully decentralized algorithm for consolidating VMs in large Cloud datacenters. V-MAN can operate on any arbitrary initial allocation of VMs on the Cloud, iteratively producing new allocations that quickly converge towards the one maximizing the number of idle hosts. V-MAN uses a simple gossip protocol to achieve efficiency, scalability and robustness to failures. Simulation experiments indicate that, starting from a random allocation, V-MAN produces an almost-optimal VM placement in just a few rounds; the protocol is intrinsically robust and can cope with computing nodes being added to or removed from the Cloud.',4,NULL,'Gossiping','Marzolla et al.'),(252,NULL,NULL,'Ming-Han Tsai and Chou, J. and Jye Chen','6753851','10.1109/CloudCom.2013.85',2013,NULL,'Prevent VM Migration in Virtualized Clusters via Deadline Driven Placement Policy',NULL,NULL,1,'VM consolidation has been shown as a promising technique for saving energy costs of a data center. It relies on VM migration to move user applications or jobs onto fewer numbers of physical servers during off peak hour. However, VM migration is a costly operation that could cause several concerns, such as performance degradation and system instability. Most existing works were proposed to minimize the migration cost for dynamic consolidation which migrates VM at the runtime when SLA violation or resource under-utilization is detected. In contrast, this paper aims to proactively prevent VM migration for semi-static VM consolidation by proposing a deadline driven VM placement strategy based on the awareness of the server turn-off time and job execution time. We evaluate our approach using a real HPC cluster trace as well as a set of synthetic generated workloads. The results show our approach can significantly reduce the number of migrations by 70% on the real trace. We also demonstrate that our approach can be resilient to different workload patterns by achieving consistent improvement around 50% over all the synthetic workloads.',4,NULL,'Semi-static VM consolidation','Tsai et al.'),(253,NULL,NULL,'Kuan Lu and Yahyapour, R. and Wieder, P. and Kotsokalis, C. and Yaqub, E. and Jehangiri, AI','6676754','10.1109/CLOUD.2013.112',2013,0,'QoS-Aware VM Placement in Multi-domain Service Level Agreements Scenarios',NULL,1,1,'Virtualization technologies of Infrastructure-as-a- Service enable the live migration of running Virtual Machines (VMs) to achieve load balancing, fault-tolerance and hardware consolidation in data centers. However, the downtime/service unavailability due to live migration may be substantial with relevance to the customers\' expectations on responsiveness, as the latter are declared in established Service Level Agreements (SLAs). Moreover, it may cause significant (potentially exponential) SLA violation penalties to its associated higher- level domains (Platform-as-a-Service and Software-as-a-Service). Therefore, VM live migration should be managed carefully. In this paper, we present the OpenStack version of the Generic SLA Manager, alongside its strategies for VM selection and allocation during live migration of VMs. We simulate a use case where IaaS (OpenStack-SLAM) and PaaS (OpenShift) are combined, and assess performance and efficiency of the aforementioned VM placement strategies, when a multi-domain SLA pricing & penalty model is involved. We find that our proposal is efficient in managing trade-offs between the operational objectives of service providers (including financial considerations) and the customers\' expected QoS requirements.',4,NULL,'QoS-aware VM placement in multi-domain SLA','Lu et al.'),(254,NULL,NULL,'Tziritas, N. and Cheng-Zhong Xu and Loukopoulos, T. and Khan, S.U. and Zhibin Yu','6687378','10.1109/ICPP.2013.54',2013,0,'Application-Aware Workload Consolidation to Minimize Both Energy Consumption and Network Load in Cloud Environments',NULL,1,1,'In this paper we tackle the problem of virtual machine (VM) placement onto physical servers to jointly optimize two objective functions. The first objective is to minimize the total energy spent within a cloud due to the servers that are commissioned to satisfy the computational demands of VMs. The second objective is to minimize the total network overhead incurred due to: (a) communicational dependencies between VMs, and (b) the VM migrations performed for the transition from an old assignment scheme to a new one. We study different methodologies for solving the aforementioned problem. The first approach is based on VM packing algorithms that optimize the above objective functions separately, reaching a single solution. The other approach is to tackle simultaneously the two optimization targets and define a set of non-dominating solutions. Performance evaluation using simulation experiments reveals interesting trade-offs between energy consumption and network load.',4,NULL,'Power/computing-aware BFD (PCA-BFD)','Tziritas et al.'),(255,NULL,NULL,'Feller, E. and Rohr, C. and Margery, D. and Morin, C.','6253507','10.1109/CLOUD.2012.50',2012,0,'Energy Management in IaaS Clouds: A Holistic Approach',NULL,1,1,'Energy efficiency has now become one of the major design constraints for current and future cloud data center operators. One way to conserve energy is to transition idle servers into a lower power-state (e.g. suspend). Therefore, virtual machine (VM) placement and dynamic VM scheduling algorithms are proposed to facilitate the creation of idle times. However, these algorithms are rarely integrated in a holistic approach and experimentally evaluated in a realistic environment. In this paper we present the energy management algorithms and mechanisms of a novel holistic energy-aware VM management framework for private clouds called Snooze. We conduct an extensive evaluation of the energy and performance implications of our system on 34 power-metered machines of the Grid\'5000 experimentation testbed under dynamic web workloads. The results show that the energy saving mechanisms allow Snooze to dynamically scale data center energy consumption proportionally to the load, thus achieving substantial energy savings with only limited impact on application performance.',4,NULL,'Multi-dimensional predictive VM placement','Feller et al.'),(256,NULL,NULL,'Mastroianni, C. and Meo, M. and Papuzzo, G.','6681861','10.1109/TCC.2013.17',2013,NULL,'Probabilistic Consolidation of Virtual Machines in Self-Organizing Cloud Data Centers',NULL,NULL,1,'Power efficiency is one of the main issues that will drive the design of data centers, especially of those devoted to provide Cloud computing services. In virtualized data centers, consolidation of Virtual Machines (VMs) on the minimum number of physical servers has been recognized as a very efficient approach, as this allows unloaded servers to be switched off or used to accommodate more load, which is clearly a cheaper alternative to buy more resources. The consolidation problem must be solved on multiple dimensions, since in modern data centers CPU is not the only critical resource: depending on the characteristics of the workload other resources, for example, RAM and bandwidth, can become the bottleneck. The problem is so complex that centralized and deterministic solutions are practically useless in large data centers with hundreds or thousands of servers. This paper presents ecoCloud, a self-organizing and adaptive approach for the consolidation of VMs on two resources, namely CPU and RAM. Decisions on the assignment and migration of VMs are driven by probabilistic processes and are based exclusively on local information, which makes the approach very simple to implement. Both a fluid-like mathematical model and experiments on a real data center show that the approach rapidly consolidates the workload, and CPU-bound and RAM-bound VMs are balanced, so that both resources are exploited efficiently.',4,NULL,'EcoCloud','Mastroianni et al.'),(257,NULL,NULL,'Mishra, M. and Sahoo, A','6008720','10.1109/CLOUD.2011.38',2011,NULL,'On Theory of VM Placement: Anomalies in Existing Methodologies and Their Mitigation Using a Novel Vector Based Approach',NULL,NULL,1,'In this paper, we present the methodologies used in existing literature for Virtual Machine (VM) placement, load balancing and server consolidation in a data center environment. While the methodologies may seem fine on the surface, certain drawbacks and anomalies can be uncovered when they are analyzed deeper. We point out those anomalies and draw backs in the existing literature and explain what are the root causes of such anomalies. Then we propose a novel methodology based on vector arithmetic which not only addresses those anomalies but also leads to some interesting theories and algorithms to tackle the above mentioned three functionalities required in managing resources of data centers. We believe that with a strong mathematical base, our methodology has the potential to become the foundation of future models and algorithms in this research area.',4,NULL,NULL,NULL),(258,NULL,NULL,'Wei Chen and Xiaoqiang Qiao and Jun Wei and Tao Huang','6253484','10.1109/CLOUD.2012.60',2012,0,'A Profit-Aware Virtual Machine Deployment Optimization Framework for Cloud Platform Providers',NULL,1,1,'As a rising application paradigm, cloud computing enables the resources to be virtualized and shared among applications. In a typical cloud computing scenario, customers, Service Providers (SP), and Platform Providers (PP) are independent participants, and they have their own objectives with different revenues and costs. From PPs\' viewpoints, much research work reduced the costs by optimizing VM placement and deciding when and how to perform the VM migrations. However, some work ignored the fact that the balanced use of the multi-dimensional resources can affect overall resource utilization significantly. Furthermore, some work focuses on the selection of the VMs and the target servers without considering how to perform the reconfigurations. In this paper, with a comprehensive consideration of PPs\' interests, we propose a framework to improve their profits by maximizing the resource utilization and reducing the reconfiguration costs. Firstly, we use the vector arithmetic to model the objective of balancing the multi-dimensional resources use and propose a VM deployment optimization method to maximize the resource utilization. Then a two-level runtime reconfiguration strategy, including local adjustment and VM parallel migration, is presented to reduce the VM migration and shorten the total migration time. Finally, we conduct some preliminary experiments, and the results show that our framework is effective in maximizing the resource utilization and reducing the costs of the runtime reconfiguration.',4,NULL,'Profit-Aware VM Deployment Framework','Chen et al.'),(259,NULL,NULL,'Xinlong Liang and Rui Jiang and Huafeng Kong','6743327','10.1109/IMSNA.2013.6743327',2013,NULL,'Secure and reliable VM-vTPM migration in private cloud',NULL,NULL,1,'Cloud computing, which has emerged as one of the most influential paradigms in the IT industry in recent years, is powered by the concept of virtualization technology. For achieving energy efficiency, load balancing and high availability of physical server in Cloud Data Center, the virtual machines should be migrated from one physical server to another. During the migration process, some steps should be taken to protect user\'s data and privacy. The extension of trusted computing to virtual systems using vTPMs can make the virtual machine more secure and reliable. So vTPM should be migrated to destination platform together with its corresponding virtual machine. However, most of the present researches just focus on the migration of VM without considering the vTPM migration. Moreover, the current migration protocols are not secure enough. In this paper, we focus on the secure implementation of virtual machine migration from one platform to another platform in private cloud model. We propose a thorough and secure VM-vTPM migration scheme. In this scheme we first propose a vTPM key structure to make non-migratable vTPM keys to be migratable. Then we leverage on this structure to construct a secure VM-vTPM migration protocol which includes three phases. The first phase is a dual authentication between source platform and destination platform, the second phase is the migration of vTPM, and the third phase is the migration of VM. Finally, we analyze the security of our protocol to make sure our proposed protocol can realize all the security goals such as confidentiality and integrity, authentication of source and destination platform, preserving the association between VM and vTPM, and atomicity of the transfer.',1,NULL,NULL,NULL),(260,NULL,NULL,'YamunaDevi, L. and Aruna, P. and Sudha, D.D. and Priya, N.','5979008','10.1109/PACC.2011.5979008',2011,NULL,'Security in Virtual Machine Live Migration for KVM',NULL,NULL,1,'Virtualization is a technical innovation designed to increase the level of system abstraction and enable IT users to harness ever-increasing levels of computer performance. The most fundamental part of virtualization is the hypervisor. To perform maintenance on a host server, we need to be able to move all of its VMs onto other host servers with minimum disruption to end users. Even more importantly, if our virtual workloads peak and need more resources, our virtual infrastructure should be able to automatically locate a host server with adequate resources and move the VMs to this host. And the entire process should be completely transparent to end users. VM migration, especially live migration is a desirable feature in Grid and Cloud Computing systems for load balancing, elastic scaling, fault tolerance, and hardware maintenance. The security and reliability issues that reside in VM live migration is a critical factor for its acceptance by IT industry. Though the performance is optimized through live migration, there are two important issues to be noted, reliability and security. In this paper, we propose how to migrate a VM Guest to another VM Host Server. We discuss the steps regarding the KVM live migration requirements and the security and reliability issues in live migration.',1,NULL,NULL,NULL),(261,NULL,NULL,'Kanada, Y. and Tarui, T.','5723191','10.1109/ICOIN.2011.5723191',2011,NULL,'A \"network-paging\" based method for wide-area live-migration of VMs',NULL,NULL,1,'In cloud-computing environments, migration of virtual machines (VMs) between data centers can solve many problems such as load balancing and power saving. One of the difficulties in wide-area migration, however, is the address-warping problem, in which the address of the VM warps from the source server to the destination server. This confuses or complicates the status of the WAN, and the LANs connected to the WAN. We propose two solutions to this problem. One is to switch an address-translation rule, and the other is to switch multiple virtual networks. The former is analogous to paging in memory virtualization, and the latter is analogous to segmentation. The network-paging based method is described and our evaluation results are shown. It took less than 100 ms in average to switch from the source to the destination server using this method.',1,NULL,NULL,NULL),(262,NULL,NULL,'Manvar, D. and Mishra, M. and Sahoo, A','6151364','10.1109/COMSNETS.2012.6151364',2012,NULL,'Low cost computing using virtualization for Remote Desktop',NULL,NULL,1,'The goal of this project is to provide a low cost open source Remote Desktop based computing environment to users by using Virtualization Technology and existing open source software and tools. In this project, we have used LTSP (Linux Terminal Server Project) [1] to access Remote Desktop and Xen [2] hypervisor to provide virtual desktop environment at server. A user gets a personalized desktop in the form of a VM (Virtual Machine) running over remote server. It is different from the old Remote Desktop solutions in a way that instead of a login session on single OS remote server, user will get a full fledged desktop with desired OS. The most important benefit for the system will be the resource conservation as facilities like live VM migration [3] available with virtualization will help in Load Consolidation and Load Balancing. Because of virtualization, user\'s desktop is also isolated from other users in the same physical machine. This project makes use of open source tools like LTSP and XEN to make the solution affordable to everybody. There are certain similar solutions available in market like Citrix Xen Desktop [4], VM Ware View [5] but because of their proprietary nature they are both costly and closed systems.',1,NULL,NULL,NULL),(263,NULL,NULL,'Ranjana, R. and Raja, J.','6823533','10.1109/ICGCE.2013.6823533',2013,1,'A survey on power aware virtual machine placement strategies in a cloud data center',NULL,NULL,1,'Cloud computing is one of the fast spreading technologies for providing utility based IT services to users. These services are provided by large-scale virtualised data centers. As it is these data centers consume large amount of electrical energy for their operation and maintenance. Extensive research had been undertaken to reduce the power consumption of data center by bringing changes in the hardware technology of data center. Alternatively, virtualization can also be used to reduce the power consumption in a data center. Migration of virtual machines(VM), one of the key feature of virtualization can be used to consolidate workloads on fewer servers and thus switch off idle servers to conserve energy. The major concern in any server consolidation technique is the optimal choice of physical server for VM placement. This paper aims to summarise the available VM placement algorithm that takes into consideration the power consumption factor during VM migration. A survey of various solutions to VM placement based on bin packing strategy is discussed.',4,NULL,NULL,NULL),(264,NULL,NULL,'Challa, N.R.','6354610','10.1109/CCEM.2012.6354610',2012,NULL,'Hardware Based I/O Virtualization Technologies for Hypervisors, Configurations and Advantages - A Study',NULL,NULL,1,'IT organizations are taking advantage of virtualization to consolidate server infrastructure, reduce power consumption, cooling and management costs, and provide simpler and more affordable solutions for high availability, load balancing and disaster recovery [1]. One of the challenges with Virtualization Technology today is: delivering fast and scalable I/O bandwidth for virtualized servers. This paper will present various native I/O virtualization technologies (SRIOV, MRIOV and other proprietary ones) and describe how they will address the challenges presented by Server Virtualization. These emerging Hardware based I/O technologies can help IT organizations further increase consolidation ratios, virtualize a wider range of applications, and manage workloads more effectively by providing near to direct I/O access Performance as the I/O path length is comparable to a dedicated adapter assigned to a VM. . They also provide a necessary prerequisite for next-generation cloud computing models, which will ultimately deliver another major leap in data center efficiency through dynamic control of hardware. At the end, this paper will evaluate the advantages, limitations of native I/O Virtualization technologies.',1,NULL,NULL,NULL),(265,NULL,NULL,'Leelipushpam, P.G.J. and Sharmila, J.','6558130','10.1109/CICT.2013.6558130',2013,1,'Live VM migration techniques in cloud environment - A survey',NULL,1,1,'Cloud computing is a service where storage and computing resources can be accessed on subscription basis. Cloud computing is powered by the concept of virtualization technology. The virtual machines (VM) are hosted in servers so that user\'s requests are serviced in an optimal manner. The process of moving a running virtual machine or application between different physical machines without disconnecting the client or application is referred to as Live Migration. System resources memory, storage, process and Network resources like connectivity that are allocated to the virtual machine are transferred from the original host machine to the destination machine. Live Migration is performed for achieving Energy efficiency, Load Balancing and High availability of physical servers in Cloud Data center. This paper presents a detailed survey on Live Migration of Virtual machines in cloud environment.',4,NULL,NULL,NULL),(266,NULL,NULL,'Shao-Heng Wang and Huang, P.P.-W. and Wen, C.H.-P. and Li-Chun Wang','6799695','10.1109/ICOIN.2014.6799695',2014,0,'EQVMP: Energy-efficient and QoS-aware virtual machine placement for software defined datacenter networks',NULL,1,1,'To provide effective and reliable services, cloud datacenters need parallel computing and virtualization techniques. This work presents an improved virtual machine (VM) placement mechanism, called Energy efficiency and Quality of Service (QoS) aware VM Placement (EQVMP) to overcome the problem of unbalanced traffic load in switching on and off VMs for the purpose of energy saving. EQVMP combines three key techniques: (1) hop reduction, (2) energy saving and (3) load balancing. Hop reduction can regroup VMs to lower the traffic load among them. Energy saving techniques aim at choosing the appropriate servers. The proposed load balancing updates VM placement periodically. Our experimental results show that the proposed scheme can lower energy consumption and maintain QoS. We propose an evaluation score [1] to assess VM placement in terms of energy, delay and throughput. Comparing to other existing placement policies, our proposed mechanism can enhance system throughput by 25% and can have better evaluation score.',4,NULL,'QoS-aware SDN VM placement','Wang et al.'),(267,NULL,NULL,'Okamoto, Y. and Noguchi, S. and Matsuura, S. and Inomata, A and Fujikawa, K.','6305312','10.1109/SAINT.2012.65',2012,NULL,'Koshien-Cloud: Operations of Distributed Cloud as a Large Scale Web Contents Distribution Platform',NULL,NULL,1,'The cloud computing paradigm is now widely used thanks to its scalable on-demand resource management techniques. Since one of the most important challenges of cloud computing services is to stably provide their consumers with computing resources, it is necessary to investigate the performance of cloud computing services in actual operations. To study the realistic performance of cloud computing services, we propose in this paper deploying and operating a distributed cloud computing system as a web-based, nationwide image delivery service for the annual baseball championship game in Japan. Measurement results show that the proposed system has worked stably, though it raised some performance issues related to the highly loaded VM.',1,NULL,NULL,NULL),(268,NULL,NULL,'Jinhai Wang and Chuanhe Huang and Kai He and Xiaomao Wang and Xi Chen and Kuangyu Qin','Wang2014','10.1109/HPCC.and.EUC.2013.89',2013,0,'An Energy-Aware Resource Allocation Heuristics for VM Scheduling in Cloud',NULL,1,1,'Energy consumption has become a major concern to the widespread deployment of cloud data centers. Many techniques have been devised to help reduce energy consumption for cloud data centers that consist of a large number of identical servers, including dynamic allocation of active servers, consolidating diverse applications, and adjusting the CPU frequency of an active server. However, these techniques normally have a high migration and low resource utilization. CPU and memory is the dominant factors of the performance and energy consumption and whose allocation determines the energy efficiency of cloud system. Leveraging these techniques, we focus on the problem of VM placement, propose a heuristic greedy algorithm to implement VM deployment and live migration to maximize total resource utilization and minimize energy consumption, which is based on energy-aware and quadratic exponential smoothing method to predict the workloads. Our heuristic algorithm makes CPU-intensive services and memory-intensive services mapped to the same physical server more complementary. The experiment results show that there is significant improvement in the aspect of energy saving, workload balancing and scalability, compared with single-objective approaches based on CPU utilization.',4,'Antigo citationKey: 6831971','Predictive power-aware server consolidation','Wang et al.'),(269,NULL,NULL,'Bo Yin and Lin Lin','6356083','10.1109/APNOMS.2012.6356083',2012,NULL,'Energy reducing dynamic multi-dimensional resource allocation in cloud data center',NULL,NULL,1,'Consolidation virtual machine (VM) on fewer physical servers is an efficient way to reduce energy consumption in data center. However, VMs often have inherent dependencies which bring complex load interactions among the underlying physical servers. Unreasonable migration will lead to a decline in network performance. In this paper, aiming at energy saving, we propose an application-aware policy in VM consolidation process. Moreover, in order to guarantee that the migrated VMs can be efficiently placed on physical servers determined by the application-aware policy, we also propose a multi-dimensional resource allocation algorithm (MDRA). Experiment results show that the proposed application-aware policy can efficiently reduce energy consumption as well as improving network performance. And the proposed MDRA algorithm can ensure the effectiveness of our policy.',3,NULL,NULL,NULL),(270,NULL,NULL,'Silva, B. and Maciel, P. and Zimmermann, A','6750283','10.1109/ICITST.2013.6750283',2013,NULL,'Performability models for designing disaster tolerant Infrastructure-as-a-Service cloud computing systems',NULL,NULL,1,'Providing high availability in a diverse client demand scenario is a key challenge of cloud computing providers. A possible approach to protect cloud systems from natural disasters corresponds to the utilization of redundant data centers located far enough apart. The time to migrate a virtual machine (VM) from a data center to another increases due to distance. As this time impacts the overall system behavior, performability evaluation considering VM migration time and different load balances is of utmost importance when considering the analysis of distributed cloud systems. This work presents performability models for evaluating distributed cloud computing systems deployed into multiple data centers considering disaster occurrence. Additionally, we present a case study which evaluates a set of scenarios with different client loads and distances between data centers.',3,NULL,NULL,NULL),(271,NULL,NULL,'Acharya, S. and D\'Mello, D.A','6823545','10.1109/ICGCE.2013.6823545',2013,1,'A taxonomy of Live Virtual Machine (VM) Migration mechanisms in cloud computing environment',NULL,1,1,'The resources in a cloud environment are efficiently governed by employing virtualization technology. Virtualization allows multiple operating system instances to run simultaneously on a single computer. The administration of virtual machines (VMs) in the datacenter of cloud computing environment is a challenging task which requires live migration techniques. Live migration transfers running virtual machines between different physical machines without yielding control over its resources and network connections for maintaining the load balancing. The main aim of VM migration is to minimize the number of physical machines serving the given task/job with least energy consumption by switching off the idle nodes. In this paper, we provide a detailed review of different live migration techniques. The authors also define the taxonomy of live migration mechanisms based on their utilization in specific cloud environments by bringing out their salient features in the respective domains of service.',4,NULL,NULL,NULL),(272,NULL,NULL,'Moghaddam, F.F. and Cheriet, M. and Kim Khoa Nguyen','6008718','10.1109/CLOUD.2011.36',2011,NULL,'Low Carbon Virtual Private Clouds',NULL,NULL,1,'Data center energy efficiency and carbon footprint reduction have attracted a great deal of attention across the world for some years now, and recently more than ever. Live Virtual Machine (VM) migration is a prominent solution for achieving server consolidation in Local Area Network (LAN) environments. With the introduction of live Wide Area Network (WAN) VM migration, however, the challenge of energy efficiency extends from a single data center to a network of data centers. In this paper, intelligent live migration of VMs within a WAN is used as a reallocation tool to minimize the overall carbon footprint of the network. We provide a formulation to calculate carbon footprint and energy consumption for the whole network and its components, which will be helpful for customers of a provider of cleaner energy cloud services. Simulation results show that using the proposed Genetic Algorithm (GA)-based method for live VM migration can significantly reduce the carbon footprint of a cloud network compared to the consolidation of individual data center servers. In addition, the WAN data center consolidation results show that an optimum solution for carbon reduction is not necessarily optimal for energy consumption, and vice versa. Also, the simulation platform was tested under heavy and light VM loads, the results showing the levels of improvement in carbon reduction under different loads.',4,NULL,'Low Carbon Virtual Private Clouds','Moghaddam et al.'),(273,NULL,NULL,'Xiulei Qin and Wenbo Zhang and Wei Wang and Jun Wei and Xin Zhao and Tao Huang','6337821','10.1109/CLUSTER.2012.14',2012,NULL,'Towards a Cost-Aware Data Migration Approach for Key-Value Stores',NULL,NULL,1,'Live data migration is an important technique for key-value stores. However, due to the stateful feature, new virtualization technology, stringent low latency requirements and unexpected workload changes, key-value stores deployed in cloud environment have to face new challenges for data migration: effects of VM interference, and the need to trade off between the two ingredients of migration cost, say migration time and performance impact. To address these challenges, we focus on the data migration problem in a load rebalancing scenario and build a new framework that aims to rebalance load while minimizing migration costs. We build two interference-aware prediction models to predict the migration time and performance impact for each action using statistical machine learning and then create a cost model to strike a right balance between the two ingredients of cost. A cost-aware migration algorithm is designed to utilize the cost model and balance rate to guide the choice of possible migration actions. We demonstrate the effectiveness of the data migration approach as well as the cost model and two prediction models using YCSB.',1,NULL,NULL,NULL),(274,NULL,NULL,'Zhaoyi Luo and Zhuzhong Qian','6569823','10.1109/IPDPS.2013.62',2013,0,'Burstiness-aware Server Consolidation via Queuing Theory Approach in a Computing Cloud',NULL,1,1,'Burstiness is a common pattern of virtual machines (VMs)\'s workload in production data centers, where spikes usually occur aperiodically with low frequency and last shortly. Since virtualization technology enables elastic resource provisioning in a computing cloud, the bursty workloads could be handled effectively through dynamically scaling up/down. However, to cut back energy consumption, VMs are usually highly consolidated with the minimum number of physical machines (PMs) used. In this case, to meet the runtime expanding demands of the resources (spikes), some VMs have to be migrated to other idle PMs, which is costly and causes performance degradation potentially. In this paper, we investigate the elastic resource provisioning problem and propose a novel VM consolidation mechanism with resource reservation which takes burstiness into consideration as well as energy consumption. We model the resource requirement pattern as the popular ON-OFF Markov chain to represent burstiness, based on which a reservation strategy via queuing theory approach is given for each PM. Next we present a complete VM consolidation scheme with resource reservation within reasonable time complexity. The experiment result show that our algorithms improve the consolidation ratio by up to 45% with large spike size and around 30% with normal spike size compared to those provisioning for peak workload, and a better balance of performance and energy consumption is achieved in comparison with other commonly used consolidation algorithms.',4,NULL,'Burstiness-aware Server Consolidation','Luo and Qian'),(275,NULL,NULL,'Kumar, N. and Agarwal, S.','6779414','10.1109/IAdCC.2014.6779414',2014,NULL,'Self regulatory graph based model for managing VM migration in cloud data centers',NULL,NULL,1,'Cloud Computing involves the concepts of parallel processing and distributed computing in order to provide the shared resources by means of Virtual Machines(VMs) hosted by physical servers. Efficient management of VMs directly influences resource utilization and QoS delivered by the system. As the cloud setting is dynamic in nature, the number of VMs distributed among the physical servers tends to become uneven over a period of time. Under this circumstance, VMs must be migrated from overloaded server to underloaded server to balance the load. In this paper, we present a random graph model of the network of servers in a data center. By initiating random walks and using the heuristics Maximum Correlation Coefficient and Migration Opportunity, we select the migrating set of VMs as well as the target server respectively. Simulation results show that the model always finds a target server in minimum time. Also the graph maintains uniform average degree which shows that the network of physical servers remains load balanced even when the load and the migration opportunity vary with time.',3,NULL,NULL,NULL),(276,NULL,NULL,'Li Hongyou and Wang Jiangyong and Peng Jian and Wang Junfeng and Liu Tang','6723884','10.1109/CC.2013.6723884',2013,0,'Energy-aware scheduling scheme using workload-aware consolidation technique in cloud data centres',NULL,2,1,'To reduce energy consumption in cloud data centres, in this paper, we propose two algorithms called the Energy-aware Scheduling algorithm using Workload-aware Consolidation Technique (ESWCT) and the Energy-aware Live Migration algorithm using Workload-aware Consolidation Technique (ELMWCT). As opposed to traditional energy-aware scheduling algorithms, which often focus on only one-dimensional resource, the two algorithms are based on the fact that multiple resources (such as CPU, memory and network bandwidth) are shared by users concurrently in cloud data centres and heterogeneous workloads have different resource consumption characteristics. Both algorithms investigate the problem of consolidating heterogeneous workloads. They try to execute all Virtual Machines (VMs) with the minimum amount of Physical Machines (PMs), and then power off unused physical servers to reduce power consumption. Simulation results show that both algorithms efficiently utilise the resources in cloud data centres, and the multidimensional resources have good balanced utilizations, which demonstrate their promising energy saving capability.',4,NULL,'Integrated resource utilization','Hongyou et al.'),(277,NULL,NULL,'Ashraf, A and Porres, I','6787319','10.1109/PDP.2014.101',2014,NULL,'Using Ant Colony System to Consolidate Multiple Web Applications in a Cloud Environment',NULL,NULL,1,'Infrastructure as a Service (IaaS) clouds provide virtual machines (VMs) under a pay-per-use business model, which can be used to create a dynamically scalable cluster of servers to deploy one or more web applications. In contrast to the traditional dedicated hosting of web applications where each VM is used exclusively for one particular web application, the shared hosting of web applications allows improved VM utilization by sharing VM resources among multiple concurrent web applications. However, in a shared hosting environment, dynamic scaling alone does not minimize over-provisioning of VMs. In this paper, we present a novel approach to consolidate multiple web applications in a cloud-based shared hosting environment. The proposed approach uses Ant Colony Optimization (ACO) to build a web application migration plan, which is then used to minimize over-provisioning of VMs by consolidating web applications on under-utilized VMs. The proposed approach is demonstrated in discrete-event simulations and is evaluated in a series of experiments involving synthetic as well as realistic load patterns.',3,NULL,NULL,NULL),(278,NULL,NULL,'Sarker, T.K. and Maolin Tang','6740417','10.1109/GrC.2013.6740417',2013,NULL,'Performance-driven live migration of multiple virtual machines in datacenters',NULL,NULL,1,'Live migration of multiple Virtual Machines (VMs) has become an indispensible management activity in datacenters for application performance, load balancing, server consolidation. While state-of-the-art live VM migration strategies focus on the improvement of the migration performance of a single VM, little attention has been given to the case of multiple VMs migration. Moreover, existing works on live VM migration ignore the inter-VM dependencies, and underlying network topology and its bandwidth. Different sequences of migration and different allocations of bandwidth result in different total migration times and total migration downtimes. This paper concentrates on developing a multiple VMs migration scheduling algorithm such that the performance of migration is maximized. We evaluate our proposed algorithm through simulation. The simulation results show that our proposed algorithm can migrate multiple VMs on any datacenter with minimum total migration time and total migration downtime.',1,NULL,NULL,NULL),(279,NULL,NULL,'Loreti, Daniela and Ciampolini, Anna','6903712','10.1109/HPCSim.2014.6903712',2014,0,'A distributed self-balancing policy for virtual machine management in cloud datacenters',NULL,1,1,'Cloud Computing is a crucial computational paradigm for modern companies because it can discharge them from managing their ever growing IT infrastructure. Dynamically offering a plenty of computational resources, the cloud can also simplify the execution of CPU-intensive applications. Modern data centers for cloud computing are facing the challenge of a growing complexity due to the increasing number of users and their augmenting resource requests. A lot of efforts are now concentrated on providing the cloud infrastructure with autonomic behavior, so that it can take decisions about virtual machine (VM) management across the datacenter\'s nodes without human intervention. While the major part of these solutions is intrinsically centralized and suffers of scalability and reliability problems, we investigate the possibility to provide the cloud with a decentralized self-organizing behavior. To this purpose we present a novel VM migration policy suitable for a distributed environment, where hosts can exchange status information with each other according to a predefined protocol. The main goal of the policy is to balance the computational load on datacenter\'s physical hosts by conveniently moving virtual machines (VMs). We tested the policy performance by means of an ad hoc built simulator.',4,NULL,'Distributed autonomic migration','Loreti and Ciampolini'),(280,NULL,NULL,'Wenhong Tian and Minxian Xu and Yu Chen and Yong Zhao','6883949','10.1109/ICC.2014.6883949',2014,NULL,'Prepartition: A new paradigm for the load balance of virtual machine reservations in data centers',NULL,NULL,1,'It is significant to apply load-balancing strategy to improve the performance and reliability of resource in data centers. One of the challenging scheduling problems in Cloud data centers is to take the allocation and migration of reconfigurable virtual machines (VMs) as well as the integrated features of hosting physical machines (PMs) into consideration. In the reservation model, workload of data centers has fixed process interval characteristics. In general, load-balance scheduling is NP-hard problem as proved in many open literatures. Traditionally, for offline load balance without migration, one of the best approaches is LPT (Longest Process Time first), which is well known to have approximation ratio 4/3. With virtualization, reactive (post) migration of VMs after allocation is one popular way for load balance and traffic consolidation. However, reactive migration has difficulty to reach predefined load balance objectives, and may cause interruption and instability of service and other associated costs. In view of this, we propose a new paradigm-Prepartition: it proactively sets process-time bound for each request on each PM and prepares in advance to migrate VMs to achieve the predefined balance goal. Prepartition can reduce process time by preparing VM migration in advance and therefore reduce instability and achieve better load balance as desired. Trace-driven and synthetic simulation results show that Prepartition has 10%-20% better performance than the well known load balancing algorithms with regard to average CPU utilization, makespan as well as capacity makespan.',4,NULL,'Prepartition VM placement','Tian et al.'),(281,NULL,NULL,'Zhiyong Yang and Chunlin Li and Yun, A-G. and Chang Liu','Yang2012','10.1109/ICCECT.2012.129',2012,NULL,'A New Trigger Strategy Based on Live Migration of the Virtual Machine',NULL,NULL,1,'A load balancing algorithm based on live migration of the virtual machine is proposed in the paper. The algorithm proposed the trigger strategy based on the load prediction. The algorithm calculates the next load values according to the historical load records to determine the trigger time for migration of VM\", \"it avoids that Instantaneous load value triggers migration of VM. After the migration the system resource is used more balanced. Finally\", \"The experiment results show that the algorithm is very good for load balancing and it can improve system performance.',3,'CitationKey: 6414017','Trigger strategy for live VM migration','Yang et al.'),(282,NULL,NULL,'Xing Jin and Hongbo Wang and Jianjian Wang and Shiduan Cheng and Jihan Li','6832122','10.1109/HPCC.and.EUC.2013.240',2013,NULL,'A Partners Assisted Virtual Machine Live Storage Migration for Intensive Disk I/O Workloads',NULL,NULL,1,'Live migration of virtual machine (VM) enables mobility of VM and contributes to advantages of virtualization like energy saving, high availability, fault tolerance and work load balancing. However solutions of VMs\' migration in both theoretical and industrial areas concentrate more on memory migration other than storage migration. Lots of applications with intensive disk I/O operations rely on local storage, especially when it comes to high performance computing. Migration of shared storage is also of necessity for consolidation and workload balance. Current approaches on storage migration can hardly work effectively in disk I/O intensive environment. They cannot reduce migration time and guarantee the disk I/O performance of VMs at the same time. This paper proposes an approach called Partners Assisted Storage Migration (PASM). We are the first to utilize disk I/O ability of pre-allocated storage nodes to relieve the competition between VMs\' intensive disk I/O and storage migration. It can migrate VMs\' storage effectively comparing to current methods: post-copy and write-mirror. Experiments including single VM\'s migration and multiple VMs\' migration show that PASM can save 78.9% migration time and achieve additional 27.1% in disk I/O performance over existing methods.',3,NULL,NULL,NULL),(283,NULL,NULL,'Takouna, I and Rojas-Cessa, R. and Sachs, K. and Meinel, C.','6809407','10.1109/UCC.2013.50',2013,0,'Communication-Aware and Energy-Efficient Scheduling for Parallel Applications in Virtualized Data Centers',NULL,1,1,'In this paper, we propose Peer VMs Aggregation (PVA) to enable dynamic discovery of communication patterns and reschedule VMs based on the determined communication patterns using VM migration. In the implementation, we consider that communication delays occur at the server (i.e., memory-bus) and at the data center network. To evaluate our approach, we modeled a network and a memory subsystem on CloudSim simulator. We then used NAS Parallel Benchmarks, which consists of six different applications as parallel applications. We thoroughly evaluated our proposed approach measuring several assessment metrics including VMs placement, performance degradation, and network utilization of each link. The results of the simulation show that our proposed approach significantly reduces the total amount of traffic in the network where it reduces the average of the network\'s utilization by 25%.',4,NULL,'Peer VMs migration (PVA)','Takouna et al.'),(284,NULL,NULL,'Kamga, C.M.','6424974','10.1109/UCC.2012.34',2012,NULL,'CPU Frequency Emulation Based on DVFS',NULL,NULL,1,'Nowadays, virtualization is present in almost all computing infrastructures. Thanks to VM migration and server consolidation, virtualization helps in reducing power consumption in distributed environments. On another side, Dynamic Voltage and Frequency Scaling (DVFS) allows servers to dynamically modify the processor frequency (according to the CPU load) in order to achieve less energy consumption. We observe that DVFS is mainly used, but still generates a waste of energy. In fact, the DVFS frequency scaling policies are based on advertised processor frequency. By default and thanks to the on demand governor, it scales up or down the processor frequency according to the current load and the different predefined threshold (up and down). However, the set of frequencies constitutes a discrete range of frequencies. In this case, the frequency required for a specific load will almost be scaled to a frequency more higher than expected, which leads to a non-efficient use of energy. In this paper, we analyze and address a way of emulating a precise CPU frequency thanks to the DVFS management in virtualized environments. We implemented and evaluated our prototype in the Xen hyper visor.',1,NULL,NULL,NULL),(285,NULL,NULL,'Masoumzadeh, S.S. and Hlavacs, H.','6727854','10.1109/CNSM.2013.6727854',2013,0,'Integrating VM selection criteria in distributed dynamic VM consolidation using Fuzzy Q-Learning',NULL,1,1,'Distributed dynamic VM consolidation can be an effective strategy to improve energy efficiency in cloud environments. In general, this strategy can be decomposed into four decision-making tasks: (1) Host overloading detection, (2) VM selection, (3) Host underloading detection, and (4) VM placement. The goal is to consolidate virtual machines dynamically in a way that optimizes the energy-performance tradeoff online. In fact, this goal is achieved when each of the aforementioned decisions are made in an optimized fashion. In this paper we concentrate on the VM selection task and propose a Fuzzy Q-Learning (FQL) technique so as to make optimal decisions to select virtual machines for migration. We validate our approach with the CloudSim toolkit using real world PlanetLab workload. Experimental results show that using FQL yields far better results w.r.t. the energy-performance trade-off in cloud data centers in comparison to state of the art algorithms.',4,NULL,'Fuzzy VM consolidation','Masoumzadeh and Hlavacs.'),(286,NULL,NULL,'Wuhib, F. and Stadler, R. and Lindgren, H.','6380035',NULL,2012,NULL,'Dynamic resource allocation with management objectives #x2014;Implementation for an OpenStack cloud',NULL,NULL,1,'We report on design, implementation and evaluation of a resource management system that builds upon OpenStack, an open-source cloud platform for private and public clouds. Our implementation supports an Infrastructure-as-a-Service (IaaS) cloud and currently provides allocation for computational resources in support of both interactive and computationally intensive applications. The design supports an extensible set of management objectives between which the system can switch at runtime. We demonstrate through examples how management objectives related to load-balancing and energy efficiency can be mapped onto the controllers of the resource allocation subsystem, which attempts to achieve an activated management objective at all times. The design is extensible in the sense that additional objectives can be introduced by providing instantiations for generic functions in the controllers. Our implementation monitors the fulfillment of the relevant management metrics in real time. Testbed evaluation demonstrates the effectiveness of our approach in a dynamic environment. It further illustrates the trade-off between closely meeting a specific management objective and the associated cost of VM live-migration.',3,NULL,NULL,NULL),(287,NULL,NULL,'Lloyd, W. and Pallickara, S. and David, O. and Lyon, J. and Arabi, M. and Rojas, K.','6424931','10.1109/UCC.2012.20',2012,NULL,'Performance Modeling to Support Multi-tier Application Deployment to Infrastructure-as-a-Service Clouds',NULL,NULL,1,'Infrastructure-as-a-service (IaaS) clouds support migration of multi-tier applications through virtualization of diverse application stack(s) of components which may require various operating systems and environments. To maximize performance of applications deployed to IaaS clouds while minimizing deployment costs, it is necessary to create virtual machine images to host application components with consideration for component dependencies that may affect load balancing of physical resources of VM hosts including CPU time, disk and network bandwidth. This paper presents results of an investigation utilizing physical machine (PM) and virtual machine (VM) resource utilization statistics to build performance models to predict application performance and rank performance of application component deployment configurations deployed across VMs. Our objective was to predict which component compositions provide best performance while requiring the fewest number of VMs. Eighteen individual resource utilization statistics were investigated for use as independent variables to predict service execution time using four different modeling approaches. Overall CPU time was the strongest predictor of execution time. The strength of individual predictors varied with respect to the resource utilization profiles of the applications. CPU statistics including idle time and number of context switches were good predictors when the test application was more disk I/O bound, while disk I/O statistics were better predictors when the application was more CPU bound. All performance models built were effective at determining the best performing service composition deployments validating the utility of our approach.',1,NULL,NULL,NULL),(288,NULL,NULL,'Janpan, T. and Visoottiviseth, V. and Takano, R.','6799494','10.1109/ICOIN.2014.6799494',2014,NULL,'A virtual machine consolidation framework for CloudStack platforms',NULL,NULL,1,'Virtual machine (VM) consolidation is an emerging solution for energy saving in cloud data centers. An appropriate VM consolidation policy must change accordingly, depending on the situation. To easily deploy a VM consolidation solution, this paper proposes a VM consolidation framework for Apache CloudStack, which is a popular open source cloud platform software suite. We separate the VM consolidation mechanism from the policy used to decide which VMs should be moved to somewhere more appropriate to run. This framework is a modular architecture that combines various resource monitoring systems, power control mechanisms, and VM packing algorithms. Using the proposed framework, we have demonstrated that a Web application can consolidate and deconsolidate VMs so as to balance the load of CPU utilization among physical machines in response to the number of physical machines in operation. Experimental results reveal the proposed framework is preliminary, but provides the required functionality.',3,NULL,NULL,NULL),(289,NULL,NULL,'Peng Zhang and Hongbo Wang and Junbo Li and Jiankang Dong and Yangyang Li and Shiduan Cheng','6679873','10.1109/ICDCSW.2013.51',2013,0,': Managing Online Virtual Machine Shuffle in Virtualized Data Centers',NULL,1,1,'Virtual machine (VM) live migration provides spatial flexibility by rearranging VM placement (i.e., VM shuffle) in several scenarios, including server consolidation, power consumption saving, fault tolerance, QoS management and network congestion resolving. However, VM live migration would consume scarce bandwidth and even cause network congestion. Since the bandwidth used by VM migration is usually the same as the services running in the VM, migration traffic would dominate network path and affect other application traffic as the traffic of a VM migration is usually several GBs. It gets worse in VM shuffle where plenty of VMs are needed to be moved. In this paper, we explore the opportunity to manage online VM shuffle and minimize the impact to data center networks. An efficient online VM shuffle scheduling method named SmartShuffle is presented. SmartShuffle tries to minimize the VM shuffle duration by coordinating VM migration in a proper scheduling order. VMs benefiting others maximally are migrated preferentially. We employ the simulated annealing algorithm to search for a solution for SmartShuffle. Our evaluation shows that SmartShuffle decreases the shuffle duration dramatically.',4,NULL,'SmartShuffle','Zhang et al.'),(290,NULL,NULL,'Akiyama, S. and Hirofuchi, T. and Takano, R. and Honiden, S.','6253557','10.1109/CLOUD.2012.56',2012,NULL,'MiyakoDori: A Memory Reusing Mechanism for Dynamic VM Consolidation',NULL,NULL,1,'In Infrastructure-as-a-Service datacenters, the placement of Virtual Machines (VMs) on physical hosts are dynamically optimized in response to resource utilization of the hosts. However, existing live migration techniques, used to move VMs between hosts, need to involve large data transfer and prevents dynamic consolidation systems from optimizing VM placements efficiently. In this paper, we propose a technique called memory reusing that reduces the amount of transferred memory of live migration. When a VM migrates to another host, the memory image of the VM is kept in the source host. When the VM migrates back to the original host later, the kept memory image will be reused, i.e. memory pages which are identical to the kept pages will not be transferred. We implemented a system named MiyakoDori that uses memory reusing in live migrations. Evaluations show that MiyakoDori significantly reduced the amount of transferred memory of live migrations and reduced 87% of unnecessary energy consumption when integrated with our dynamic VM consolidation system.',1,NULL,NULL,NULL),(291,NULL,NULL,'Linjiun Tsai and Wanjiun Liao','6483650','10.1109/CloudNet.2012.6483650',2012,0,'Cost-aware workload consolidation in green cloud datacenter',NULL,1,1,'Server consolidation is important for better resource utilization and efficient energy saving for cloud datacenters which host thousands of virtual machines (VMs) to support multitenant applications. The typical approach is to migrate VMs and reallocate workload among different servers in a way to minimize the total number of servers used. However, most existing works for server consolidation focus mainly on how to reduce the number of active servers and do not account for the migration overhead incurred to the applications on the migrating VMs (such as downtime). In this paper, we propose an adaptive mechanism to schedule VM allocation in cloud datacenters. Our solution takes into account the resource utilization and migration overheads, and adaptively allocates each VM to servers based on the estimated saturation level. As a result, the quality and overhead of consolidation is balanced and the total cost is minimized. The simulation results show that our mechanism could increase the average utilization on servers by up to 97% while reducing the total migration cost by about 60%, as compared with existing solutions.',4,NULL,'Adaptive Fit VM placement','Tsai and Liao'),(292,NULL,NULL,'Jiao Zhang and Fengyuan Ren and Chuang Lin','6847982','10.1109/INFOCOM.2014.6847982',2014,NULL,'Delay guaranteed live migration of Virtual Machines',NULL,NULL,1,'The proliferation of cloud services makes virtualization technology more important. One important feature of virtualization is live Virtual Machine (VM) migration, which can be employed to facilitate load balancing, fault management and server maintenance etc. Two main metrics of evaluating a live VM migration mechanism are total migration time and downtime. The existing literature on live VM migration mainly focus on designing migration mechanisms to shorten these two metrics or making a tradeoff between them. Few of them can be applied to the applications with delay requirements, such as, delay-sensitive web services or a VM backup process that needs to be done in a specific time. This will not only negatively impact the user experiences, but also reduce the profit of cloud service providers. Besides, the frequently varied bandwidth required by the widely used pre-copy mechanism is difficult to be provided by current network technologies. In this work, we theoretically analyze how much bandwidth is required to guarantee the total migration time and downtime of a live VM migration. We first propose a deterministic-based model as a simple example, then assume that the dirtying frequency of each page obeys the bernoulli distribution. At last, we analyze the statistic features of the typical workload running in a VM and build a reciprocal-based workload model, and theoretically give the required bandwidth value to satisfy the performance metrics of a live VM migration. The experimental results demonstrate that the bandwidth obtained from the reciprocal-based model can guarantee the expected total migration time and downtime.',1,NULL,NULL,NULL),(293,NULL,NULL,'Kord, N. and Haghighi, H.','6620036','10.1109/IKT.2013.6620036',2013,NULL,'An energy-efficient approach for virtual machine placement in cloud based data centers',NULL,NULL,1,'Cloud computing is a new technology which is proffering IT services based on pay-as-you-go model to consumers from everywhere in the world. The growing demand of Cloud infrastructure and modern computational requests like business, scientific and web applications result in large-scale data centers and lead to extra electrical energy consumption. High energy consumption causes high operational cost and also led to high carbon emission which is harmful for atmosphere. Hence, energy-efficient techniques are required to minimize the negative effects of Cloud computing on the environment. Virtual machines (VMs) migration, dynamic consolidation in the virtualized data centers in cloud environments and switching idle physical machines off could yield reduce energy consumption; hence, one of the important issues for energy efficiency in virtualized cloud environments is how to place new VMs or selected VMs for migration across the hosts. In this paper we propose an energy-efficient approach based on Minimum Correlation Coefficient (MCC) method for virtual machine placement in cloud based, virtualized data centers. The proposed approach regards both Service Level Agreement (SLA) and low energy consumption and tries to make a trade-off between these two concerns using fuzzy Analytic Hierarchy Process (AHP). We evaluate our approach using Cloudsim toolkit as a modern cloud computing environment simulator. The evaluation shows that our proposed method offers a suitable trade-off between power efficiency and SLA violation reduction in cloud data centers.',2,'Não disponível pela conta da UBI',NULL,NULL),(294,NULL,NULL,'Chaisiri, S. and Bu-Sung Lee and Niyato, D.','Chaisiri2009','10.1109/APSCC.2009.5394134',2009,1,'Optimal virtual machine placement across multiple cloud providers','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5394134',1,2,'Cloud computing provides users an efficient way to dynamically allocate computing resources to meet demands. Cloud providers can offer users two payment plans, i.e., reservation and on-demand plans for resource provisioning. Price of resources in reservation plan is generally cheaper than that in on-demand plan. However, since the reservation plan has to be acquired in advance, it may not fully meet future demands in which the on-demand plan can be used to guarantee the availability to the user. In this paper, we propose an optimal virtual machine placement (OVMP) algorithm. This algorithm can minimize the cost spending in each plan for hosting virtual machines in a multiple cloud provider environment under future demand and price uncertainty. OVMP algorithm makes a decision based on the optimal solution of stochastic integer programming (SIP) to rent resources from cloud providers. The performance of OVMP algorithm is evaluated by numerical studies and simulation. The results clearly show that the proposed OVMP algorithm can minimize users\' budgets. This algorithm can be applied to provision resources in emerging cloud computing environments.',4,'Antigo citationKey: 5394134','VM placement based on charging models','Chaisiri et al.'),(295,NULL,NULL,'Hien Nguyen Van and Tran, F.D. and Menaud, J.-M.','Van2010','10.1109/CLOUD.2010.25',2010,0,'Performance and Power Management for Cloud Infrastructures','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5557975',1,2,'A key issue for Cloud Computing data-centers is to maximize their profits by minimizing power consumption and SLA violations of hosted applications. In this paper, we propose a resource management framework combining a utility-based dynamic Virtual Machine provisioning manager and a dynamic VM placement manager. Both problems are modeled as constraint satisfaction problems. The VM provisioning process aims at maximizing a global utility capturing both the performance of the hosted applications with regard to their SLAs and the energy-related operational cost of the cloud computing infrastructure. We show several experiments how our system can be controlled through high level handles to make different trade-off between application performance and energy consumption or to arbitrate resource allocations in case of contention.',4,'Antigo citationKey: 5557975','SLA-driven server consolidation','Van et al.'),(296,NULL,NULL,'Jing Tai Piao and Jun Yan','Piao2010','10.1109/GCC.2010.29',2010,0,'A Network-aware Virtual Machine Placement and Migration Approach in Cloud Computing','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5662521',1,2,'Cloud computing represents a major step up in computing whereby shared computation resources are provided on demand. In such a scenario, applications and data thereof can be hosted by various networked virtual machines (VMs). As applications, especially data-intensive applications, often need to communicate with data frequently, the network I/O performance would affect the overall application performance significantly. Therefore, placement of virtual machines which host an application and migration of these virtual machines while the unexpected network latency or congestion occurs is critical to achieve and maintain the application performance. To address these issues, this paper proposes a virtual machine placement and migration approach to minimizing the data transfer time consumption. Our simulation studies suggest that the proposed approach is effective in optimizing the data transfer between the virtual machine and data, thus helping optimize the overall application performance.',4,'Antigo citationKey: 5662521','Network-aware data access time reduction',NULL),(297,NULL,NULL,'Li Li and Woo, T.','5634451','10.1109/NPSEC.2010.5634451',2010,NULL,'VSITE: A scalable and secure architecture for seamless L2 enterprise extension in the cloud','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5634451',NULL,2,'This paper presents an end-to-end architecture, called VSITE, for seamless integration of cloud resources into an enterprise\'s intranet at layer 2. VSITE allows a cloud provider to carve out its resources to serve multiple enterprises simultaneously while maintaining isolation and security. Resources (allocated to an enterprise) in the cloud provider appears \"internal\" to the enterprise. VSITE achieves this abstraction through the use of VPN technologies, the assignment of different VLANs to different enterprises, and the encoding of enterprise IDs in MAC addresses. Unlike traditional layer 2 VPN technology such as VPLS, VSITE suppresses layer 2 MAC learning related broadcast traffic from reaching the remote sites. VSITE makes use of location IP (represents location area) for scalable migration support. The MAC or IP address of a VM is not visible in data center core. VSITE hypervisor enforces security mechanisms to prevent enterprises from attacking one another. Thus, VSITE is scalable, secure and efficient, and it facilitates common data center operation such as VM migration. Because VSITE extends enterprise network at layer 2, this offers transparency to most existing applications and presents an easy migration path for an enterprise to leverage cloud computing resources.',1,NULL,NULL,NULL),(298,NULL,NULL,'Hirofuchi, T. and Ogawa, H. and Nakada, H. and Itoh, S. and Sekiguchi, S.','5071905','10.1109/CCGRID.2009.44',2009,NULL,'A Live Storage Migration Mechanism over WAN for Relocatable Virtual Machine Services on Clouds','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5071905',NULL,2,'IaaS (Infrastructure-as-a-Service) is an emerging concept of cloud computing, which allows users to obtain hardware resources from virtualized data centers. Although many commercial IaaS clouds have recently been launched, dynamic virtual machine (VM) migration is not possible among service providers; users are locked into a particular provider, and cannot transparently relocate their VMs to another one for the best cost-effectiveness. In this paper, we propose an advanced storage access mechanism that strongly supports live VM migration over WAN. It rapidly relocates VM disks between source and destination sites with the minimum impact on I/O performance. The proposed mechanism addresses I/O consistency of virtual disks before/after migration, which is the major issue regarding wide-area live migration. The proposed mechanism works as a storage server of a block-level storage I/O protocol (e.g.,iSCSI and NBD). Two key techniques (on-demand fetching and background copying) move on-line virtual disks among remote sites, transparently and efficiently. Our prototype system works perfectly for Xen and KVM without any modification to them. Experiments showed the prototype system also worked successfully for an emulated WAN environment.',1,NULL,NULL,NULL),(299,NULL,NULL,'Sonnek, J. and Greensky, J. and Reutiman, R. and Chandra, A.','5599167','10.1109/ICPP.2010.30',2010,0,'Starling: Minimizing Communication Overhead in Virtualized Computing Platforms Using Decentralized Affinity-Aware Migration','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5599167',1,2,'Virtualization is being widely used in large-scale computing environments, such as clouds, data centers, and grids, to provide application portability and facilitate resource multiplexing while retaining application isolation. In many existing virtualized platforms, it has been found that the network bandwidth often becomes the bottleneck resource, causing both high network contention and reduced performance for communication and data-intensive applications. In this paper, we present a decentralized affinity-aware migration technique that incorporates heterogeneity and dynamism in network topology and job communication patterns to allocate virtual machines on the available physical resources. Our technique monitors network affinity between pairs of VMs and uses a distributed bartering algorithm, coupled with migration, to dynamically adjust VM placement such that communication overhead is minimized. Our experimental results running the Intel MPI benchmark and a scientific application on a 7-node Xen cluster show that we can get up to 42% improvement in the runtime of the application over a no-migration technique, while achieving up to 85% reduction in network communication cost. In addition, our technique is able to adjust to dynamic variations in communication patterns and provides both good performance and low network contention with minimal overhead.',4,NULL,'Starlings','Sonnek et al.'),(300,NULL,NULL,'Rodero, I. and Eun Kyung Lee and Pompili, D. and Parashar, M. and Gamell, M. and Figueiredo, R.J.','Rodero2010','10.1109/GRID.2010.5698002',2010,0,'Towards energy-efficient reactive thermal management in instrumented datacenters','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5698002',1,2,'Virtual Machine (VM) migration is one of the most common techniques used to alleviate thermal anomalies (i.e., hotspots) in cloud datacenter\'s servers of by reducing the load and, therefore, decreasing the server utilization. However, there are other techniques such as voltage scaling that also can be applied to reduce the temperature of the servers in datacenters. Because no single technique is the most efficient to meet temperature/performance optimization goals in all situations, we work towards an autonomic approach that performs energy-efficient thermal management while ensuring the Quality of Service (QoS) delivered to the users. In this paper, we explore ways to take actions to reduce energy consumption at the server side before performing costly migrations of VMs. Specifically, we focus on exploiting VM Monitor (VMM) configurations, such as pinning techniques in Xen platforms, which are complementary to other techniques at the physical server layer such as using low power modes. To support the arguments of our approach, we present the results obtained from an experimental evaluation on real hardware using High Performance Computing (HPC) workloads on different scenarios.',4,'Antigo citationKey: 5698002','VM CPU pinning','Rodero et al.'),(301,NULL,NULL,'Machida, F. and Dong Seong Kim and Trivedi, K.S.','5722098','10.1109/WOSAR.2010.5722098',2010,NULL,'Modeling and analysis of software rejuvenation in a server virtualized system','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5722098',NULL,2,'As server virtualization is used as an essential software infrastructure of various software services such as cloud computing, availability management of server virtualized system is becoming more significant. Although time-based software rejuvenation is useful to postpone/prevent failures due to software aging in a server virtualized system, the rejuvenation schedules for virtual machine (VM) and virtual machine monitor (VMM) need to be determined in a proper way for the VM availability, since VMM rejuvenation affects VMs running on the VMM. This paper presents analytic models using stochastic reward nets for three time-based rejuvenation techniques of VMM; (i) Cold-VM rejuvenation in which all VMs are shut down before the VMM rejuvenation, (ii) Warm-VM rejuvenation in which all VMs are suspended before the VMM rejuvenation and (iii) Migrate-VM rejuvenation in which all VMs are moved to the other host server during the VMM rejuvenation. We compare the three techniques in terms of steady-state availability and the number of transactions lost in a year. We find the optimal combination of rejuvenation trigger intervals for each rejuvenation technique by a gradient search method. The numerical analysis shows the interesting result that Warm-VM rejuvenation does not always outperform Cold-VM rejuvenation in terms of steady-state availability depending on rejuvenation trigger intervals. Migrate-VM rejuvenation is better than the other two as long as live VM migration rate is large enough and the other host server has a capacity to accept the migrated VM.',1,NULL,NULL,NULL),(302,NULL,NULL,'Sato, K. and Sato, H. and Matsuoka, S.','Sato2009','10.1109/CCGRID.2009.24',2009,0,'A Model-Based Algorithm for Optimizing I/O Intensive Applications in Clouds Using VM-Based Migration','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5071906',1,2,'Federated storage resources in geographically distributed environments are becoming viable platforms for data-intensive cloud and grid applications. To improve I/O performance in such environments, we propose a novel model-based I/O performance optimization algorithm for data-intensive applications running on a virtual cluster, which determines virtual machine (VM) migration strategies,i.e., when and where a VM should be migrated, while minimizing the expected value of file access time. We solve this problem as a shortest path problem of a weighted direct acyclic graph (DAG), where the weighted vertex represents a location of a VM and expected file access time from the location, and the weighted edge represents a migration of a VM and time. We construct the DAG from our Markov model which represents the dependency of files. Our simulation-based studies suggest that our proposed algorithm can achieve higher performance than simple techniques, such as ones that never migrate VMs: 38% or always migrate VMs onto the locations that hold target files: 47%.',4,'Antigo citationKey: 5071906','Data access time reduction','Sato et al.'),(303,NULL,NULL,'Tsugawa, M. and Riteau, P. and Matsunaga, A. and Fortes, J.','5700384','10.1109/GLOCOMW.2010.5700384',2010,NULL,'User-level virtual networking mechanisms to support virtual machine migration over multiple clouds','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5700384',NULL,2,'Dynamic allocation of multiple cloud resources adapting to application needs over time can be achieved by taking advantage of wide-area VM live migration technologies. However, migration of VMs across different subnets, potentially in multiple clouds, requires networking support to keep the network state of moving VMs unchanged. Two problems make traditional solutions to machine mobility inefficient in this scenario: (1) administrative overheads due to coordination requirements between moving machines and the network infrastructure; and (2) degraded network performance of machines moved away from their home networks. New solutions are needed to efficiently support the migration of virtual machines over multiple cloud providers. The user-level virtual network architecture presented in this paper implements mechanisms to allow VM migration over clouds without requiring support from the physical network infrastructure, and automatically reconfiguring virtual networks to maximize the network performance of migrated virtual machines.',1,NULL,NULL,NULL),(304,NULL,NULL,'Wei Wang and Ya Zhang and Ben Lin and Xiaoxin Wu and Kai Miao','5485376','10.1109/ICCET.2010.5485376',2010,NULL,'Secured and reliable VM migration in personal cloud','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5485376',NULL,2,'The security and reliability issues that reside in VM live migration is a critical factor for its acceptance by IT industry. We propose to leverage Intel vPro and TPM to improve security in virtual machine live migration. A role-based mechanism with remote attestation is introduced, under which the VM migration is controlled by specific policies that are protected in seal storage. To provide high reliability in client virtualization, we also implement a checkpoint based method that only consumes 5% overall system resource according to the experimental result.',1,NULL,NULL,NULL),(305,NULL,NULL,'Wei Wang and Ya Zhang and Xuezhao Liu and Meilin Zhang and Zhuo Wang','5486236','10.1109/ICCET.2010.5486236',2010,NULL,'Hardware assisted resource sharing platform for personal cloud','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5486236',NULL,2,'More and more novel usage models require the capability of resource sharing among different platforms. To achieve a satisfactory efficiency, we introduce a specific resource sharing technology under which IO peripherals can be shared among different platforms. In particular, in a personal working environment that is built up by a number of devices, IO peripherals at each device can be applied to support application running at another device. This IO sharing is our so-called composable IO because it is equivalent to compose IOs from different devices for an application. We design composable IO logic and achieve pro-migration PCIe devices access, namely a migrated application running at the targeted host can still access the PCIe peripherals at the source host. This is supplementary to traditional VM migration under which application can only use resources from the device where the application runs. Experimental results show that through composable IO logic, applications with remote IO can achieve high efficiency compared with native IO.',1,NULL,NULL,NULL),(306,NULL,NULL,'Unal, E. and Lu, P. and Macdonell, C.','5581344','10.1109/HPCC.2010.109',2010,NULL,'Virtual Application Appliances in Practice: Basic Mechanisms and Overheads','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5581344',NULL,2,'Virtual application appliances (VAA) (i.e., prebuilt virtual machines (VM) for specific scientific applications) are useful mechanisms to deal with the packaging of complex software systems and heterogeneous software environments (e.g., library version conflicts on different clusters and clouds). As an experience paper, we discuss some basic techniques for creating VAAs (e.g., virtual disk repositories (VDR)) and scripting their execution. As an evaluation paper, we quantify some of the key overheads, including the cost of staging data into/out of the VAA and the costs of VM migration. Subsequently, we introduce and evaluate a new Copy Over Shared Memory (CSM) mechanism to reduce the stage in/out overheads of data using secure, shared-memory regions between the host and guest machines. Our empirical evaluation shows that VAAs achieve nearnative, end-to-end performance in widely used bioinformatics applications that we tested (i.e., GROMACS, GAFolder, HMMer). We focus on data movement, VM boot up, shutdown and migration overheads of VAAs and find that they are negligible with respect to total run-times.',1,NULL,NULL,NULL),(307,NULL,NULL,'Maoz, T. and Barak, A. and Amar, L.','4663759','10.1109/CLUSTR.2008.4663759',2008,NULL,'Combining Virtual Machine migration with process migration for HPC on multi-clusters and Grids','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4663759',NULL,2,'The renewed interest in virtualization gives rise to new opportunities for running high performance computing (HPC) applications on clusters and grids. These include the ability to create a uniform (virtual) run-time environment on top of a multitude of hardware and software platforms, and the possibility for dynamic resource allocation towards the improvement of process performance, e.g., by virtual machine (VM) migration as a means for load-balancing. This paper deals with issues related to running HPC applications on multi-clusters and grids using VMware, a virtualization package running on Windows, Linux and OS X. The paper presents the ldquoJobrunrdquo system for transparent, on-demand VM launching upon job submission, and its integration with the MOSIX cluster and grid management system. We present a novel approach to job migration, combining VM migration with process migration using Jobrun, by which it is possible to migrate groups of processes and parallel jobs among different clusters in a multi-cluster or in a grid. We use four real HPC applications to evaluate the overheads of VMware (both on Linux and Windows), the MOSIX cluster extensions and their combination, and present detailed measurements of the performance of Jobrun.',1,NULL,NULL,NULL),(308,NULL,NULL,'Imada, T. and Sato, M. and Kimura, H.','5353054','10.1109/GRID.2009.5353054',2009,NULL,'Power and QoS performance characteristics of virtualized servers','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5353054',NULL,2,'In this paper, we investigate power and QoS (Quality of Service) performance characteristics of virtualized servers with virtual machine technology. Currently, one of the critical problems at data centers with many thousands of servers is the increased power consumption. Virtual machines (VMs) are often used for Internet services for efficient server management and provisioning. While we expect that virtualized servers where multiple VMs run help save power, new issues in virtualized servers arise compared to conventional physical servers: migration of the load between servers and processor core assignment of the server\'s workload from the viewpoints of QoS performance and energy consumption. Our experimental results show that server consolidation using VM migration contributes to power reduction without or with slight QoS performance degradation, and assignment of VMs to multiple processor cores running at a lower frequency can achieve additional power reduction on a server node.',1,NULL,NULL,NULL),(309,NULL,NULL,'Wei Yan and Chuang Lin and Shanchen Pang','5662530','10.1109/GCC.2010.22',2010,NULL,'The Optimized Reinforcement Learning Approach to Run-Time Scheduling in Data Center','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5662530',NULL,2,'Warehouse data center is very large scale and complex, which constains tens of thousands servers and accomodates various applications. What\'s more important, energy consumption has risen to a critical point. Scheduling needs to maintain performance and reduce energy consumption as much as possible. Previous researches have proposed RL (reinforcement learning) as a solution. These approaches have reduced energy consumption in IT equipment to the extent of improved operation while maintain SLA (Service Level Agreement) and throughput. However, with consideration of the special condition of data center, more optimization of RL could be discovered. We find through careful optimization of RL, we could save much more energy in IT equipment (about 12%) while maintain performance. When we consider 110 billion kWh/year energy consumption in 2011, 12% saving of energy in IT equipment would make a dramatic difference in operation cost or even feasibility. Our approach sets up ideal state which would maintain performance and reduce energy consumption. When some machines deviate from that ideal state, the schedulers in the data center compute the gains and costs of VM migrations to determine whether operations such as consolidation or ban lancing would be carried. In this way, we apply long range value estimation, exploration elimination, combining of planning and learning to optimize RL. The simulation of real internet workload trace shows our design saves much more energy in IT equipment and maintains performance with much less VM migration.',1,NULL,NULL,NULL),(310,NULL,NULL,'Mehta, S. and Neogi, A.','4575156','10.1109/NOMS.2008.4575156',2008,NULL,'ReCon: A tool to Recommend dynamic server Consolidation in multi-cluster data centers','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4575156',NULL,2,'Renewed focus on virtualization technologies and increased awareness about management and power costs of running under-utilized servers has spurred interest in consolidating existing applications on fewer number of servers in the data center. The ability to migrate virtual machines dynamically between physical servers in real-time has also added a dynamic aspect to consolidation. However, there is a lack of planning tools that can analyze historical data collected from an existing environment and compute the potential benefits of server consolidation especially in the dynamic setting. In this paper we describe such a consolidation recommendation tool, called ReCon. Recon takes static and dynamic costs of given servers, the costs of VM migration, the historical resource consumption data from the existing environment and provides an optimal dynamic plan of VM to physical server mapping over time. We also present the results of applying the tool on historical data obtained from a large production environment.',1,NULL,NULL,NULL),(311,NULL,NULL,'Hai Jin and Xiaofei Liao and Song Wu and Zhiyuan Shao and Yingwei Luo','4637677','10.1109/HPCC.2008.135',2008,NULL,'ChinaV: Building Virtualized Computing System','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4637677',NULL,2,'Virtualization technology has attracted much attention in recent years. This paper describes the vision and mission of ChinaV, which is the national fundamental research program for virtualization technology in China. Furthermore, related topics about single host virtualization, multiple VM management schemes and desktop virtualization will be introduced. We first describe a remote memory virtualization scheme and a VCPU management scheme for efficient use of physical resource. Then we describe a novel live VM migration approach based on deterministic replay with execution trace. Multiple VM management schemes are also introduced for multi-VM virtualization. In desktop virtualization field, we present the LVD, a system that combines the virtualization technology and inexpensive personal computers to realize a lightweight virtual desktop system. All of those schemes and systems are good practices of virtualization solution and they have become a strong foundation of our future work.',1,NULL,NULL,NULL),(312,NULL,NULL,'Tarighi, M. and Motamedi, S.A. and Arianyan, E.','Tarighi2010a','10.1109/ICCIE.2010.5668380',2010,NULL,'Performance improvement of virtualized cluster computing system using TOPSIS algorithm','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5668380',NULL,2,'This paper shows that performance of the virtualized cluster servers could be improved through intelligent decision over migration time of Virtual Machines (VM) between physical nodes. Virtualization technology has lots of benefits. Via virtualization, Physical server can be consolidated over fewer virtual servers. This approach has many advantages such as less hardware cost, cooling cost, etc. One of the key benefits is better load balancing by using VM migration. To migrate we need to decide which virtual machine needs to be migrated and when this relocation has to be done. Our paper tries to propose a new model of load balancing in virtualized clusters based on Technique for Order Preference by Similarity to the Ideal Solution (TOPSIS) as one of the most efficient Multi Criteria Decision Making methods (MCDM). The cluster is assumed heterogeneous; therefore lots of parameters like CPU clock speed (CPU), CPU usage (%CPU), RAM capacity (RAM), and network bandwidth (NET) are used in decision process.',3,'CitationKey: 5668380','TOPSIS-based Load Balancing Model (TLM)','Tarighi et al.'),(313,NULL,NULL,'Jing Xu and Fortes, J.A.B.','Xu2010','10.1109/GreenCom-CPSCom.2010.137',2010,0,'Multi-Objective Virtual Machine Placement in Virtualized Data Center Environments','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5724828',1,2,'Server consolidation using virtualization technology has become increasingly important for improving data center efficiency. It enables one physical server to host multiple independent virtual machines (VMs), and the transparent movement of workloads from one server to another. Fine-grained virtual machine resource allocation and reallocation are possible in order to meet the performance targets of applications running on virtual machines. On the other hand, these capabilities create demands on system management, especially for large-scale data centers. In this paper, a two-level control system is proposed to manage the mappings of workloads to VMs and VMs to physical resources. The focus is on the VM placement problem which is posed as a multi-objective optimization problem of simultaneously minimizing total resource wastage, power consumption and thermal dissipation costs. An improved genetic algorithm with fuzzy multi-objective evaluation is proposed for efficiently searching the large solution space and conveniently combining possibly conflicting objectives. The simulation-based evaluation using power-consumption and thermal-dissipation models based on profiling of a Blade Center, demonstrates the good performance, scalability and robustness of our proposed approach. Compared with four well-known bin-packing algorithms and two single-objective approaches, the solutions obtained from our approach seek good balance among the conflicting objectives while others cannot.',4,'Antigo citationKey: 5724828','Multi-objective static VM placement','Xu and Fortes'),(314,NULL,NULL,'Machida, F. and Kawato, M. and Maeno, Y.','Machida2010','10.1109/NOMS.2010.5488431',2010,0,'Redundant virtual machine placement for fault-tolerant consolidated server clusters','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5488431',1,2,'Consolidated server systems using server virtualization involves serious risks of host server failures that induce unexpected downs of all hosted virtual machines and applications. To protect applications requiring high-availability from unpredictable host server failures, redundant configuration using virtual machines can be an effective countermeasure. This paper presents a virtual machine placement method for establishing a redundant configuration against host server failures with less host servers. The proposed method estimates the requisite minimum number of virtual machines according to the performance requirements of application services and decides an optimum virtual machine placement so that minimum configurations survive at any k host server failures. The evaluation results clarify that the proposed method achieves requested fault-tolerance level with less number of hosting servers compared to the conventional N+M redundant configuration approach.',4,'Antigo citationKey: 5488431','Redundant VM placement','Machida et al.'),(315,NULL,NULL,'Machida, F. and Dong Seong Kim and Jong Sou Park and Trivedi, K.S.','5355515','10.1109/ISSREW.2008.5355515',2008,NULL,'Toward optimal virtual machine placement and rejuvenation scheduling in a virtualized data center','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5355515',NULL,2,'Virtualization enables data centers to consolidate servers to improve resource utilization and power consumption. This paper presents the issues of performability management in a virtualized data center that hosts multiple services using virtualization. One of main cause of performability degradation is software aging in both virtual machines (VMs) and virtual machine monitor (VMM) in virtualized data center (VDC). Software rejuvenation is a cost effective and a proactive method to counteract software aging. During software rejuvenation operations, there occurs down time, therefore, VM placement and the rejuvenation schedules for VMs and VMM need to be optimized so as to maximize the performability in a VDC. We introduce the state of the art technology on software aging and rejuvenation in virtualized data center and we formulate this problem and show our approach.',1,NULL,NULL,NULL),(316,NULL,NULL,'Xiaoqiao Meng and Vasileios Pappas and Li Zhang','Meng2010','10.1109/INFCOM.2010.5461930',2010,0,'Improving the Scalability of Data Center Networks with Traffic-aware Virtual Machine Placement','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5461930',1,2,'The scalability of modern data centers has become a practical concern and has attracted significant attention in recent years. In contrast to existing solutions that require changes in the network architecture and the routing protocols, this paper proposes using traffic-aware virtual machine (VM) placement to improve the network scalability. By optimizing the placement of VMs on host machines, traffic patterns among VMs can be better aligned with the communication distance between them, e.g. VMs with large mutual bandwidth usage are assigned to host machines in close proximity. We formulate the VM placement as an optimization problem and prove its hardness. We design a two-tier approximate algorithm that efficiently solves the VM placement problem for very large problem sizes. Given the significant difference in the traffic patterns seen in current data centers and the structural differences of the recently proposed data center architectures, we further conduct a comparative analysis on the impact of the traffic patterns and the network architectures on the potential performance gain of traffic-aware VM placement. We use traffic traces collected from production data centers to evaluate our proposed VM placement algorithm, and we show a significant performance improvement compared to existing general methods that do not take advantage of traffic patterns and data center network characteristics.',4,'Antigo citationKey: 5461930','Network-architecture-aware VM placement','Meng et al.'),(317,NULL,NULL,'Emeneker, W. and Apon, A.','5578331','10.1109/CIT.2010.390',2010,NULL,'Cache Effects of Virtual Machine Placement on Multi-Core Processors','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5578331',NULL,2,'Virtual machine prevalence in datacenters introduces a range of potential efficiency issues. It is well known that virtual machines negatively impact application performance when compared to native execution. Multi-core architectures present new opportunities for limiting the performance impact of virtualized execution. The research presented in this paper examines the effects of multi-core cache structure on scientific applications running inside Xen virtual machines. Multiple strategies for assigning virtual machines to physical CPUs are detailed for cases where one or more virtual machines reside on a single node. The results show that placing virtual machines in caches generally improves performance when compared to the default scheduling scheme.',1,NULL,NULL,NULL),(318,NULL,NULL,'Bose, S.K. and Sundarrajan, S.','Bose2009','10.1109/ICPPW.2009.39',2009,0,'Optimizing Migration of Virtual Machines across Data-Centers','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5366093',1,2,'Managing virtual machines (VM) in large scale enterprise grid scenarios, commonly encountered in data centers, is extremely challenging. Currently, live VM migration is based on QoS non-conformance events; migration of a VM is initiated as soon as the aggregate resource (CPU and memory) requirements of the VMs on the physical machine (PM) exceed the capacity available on the PM. However, this paper establishes that, such `event-based\' migration can be extremely myopic. To overcome the limitations posed by the `event-based\' VM migration, this paper, proposes a new migration strategy based on `time-windows\'. The paper outlines a mixed integer linear programming (MILP) model and a heuristic solution for generating near-optimal rearrangements of the VMs on the PMs. The paper evaluates the solutions on various VM scenarios which show encouraging results.',4,'Antigo citationKey: 5366093','QoS-aware VM migration','Bose and Sundarrajan.'),(319,NULL,NULL,'Machida, F. and Kawato, M. and Maeno, Y.','5691344','10.1109/CNSM.2010.5691344',2010,NULL,'Renovating legacy distributed systems using virtual appliance with dependency graph','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5691344',NULL,2,'Legacy distributed systems hosted on old infrastructures can be renovated using virtual appliance that is a package of virtual machines, installed applications and their configurations. By converting a legacy distributed system to a virtual appliance, we can conserve the entire system and restart the application on the specific virtualization platforms. However, in order to execute the application service properly on the new hosting environment, some additional network configurations are required to resolve the dependencies inherited in the original system. In this paper, we propose a framework named DS Renovator that converts a legacy distributed system to a virtual appliance and renovates the system on a new hosting environment with optimum deployment for resolving the dependencies. In the virtual appliance generation process, DS Renovator analyzes server dependencies inherent in the legacy distributed system and generates a dependency graph that consists of nodes and edges representing servers and their dependencies respectively. In the virtual appliance deployment process, DS Renovator applies graph partitioning algorithm to the dependency graph to determine the optimum virtual machine placement which minimizes the dependencies between the hosting servers under the capacity limitations. As a graph partitioning algorithm, we propose an edge-contraction based efficient algorithm. The performance of the proposed algorithm is evaluated with case studies in comparison to other approximation algorithms.',1,NULL,NULL,NULL),(320,NULL,NULL,'Sato, K. and Sato, H. and Matsuoka, S.','Sato2008','10.1109/GRID.2008.4662824',2008,0,'Model-based optimization for data-intensive application on virtual cluster','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4662824',1,2,NULL,1,'Mesmo trabalho do Sato2009 (auto-plágio), só que reduzidíssimo em apenas 2 páginas que não explica nada. \r\nAntigo citationKey: 4662824',NULL,NULL),(321,NULL,NULL,'Hai Jin and Li Deng and Song Wu and Xuanhua Shi and Xiaodong Pan','5289170','10.1109/CLUSTR.2009.5289170',2009,NULL,'Live virtual machine migration with adaptive, memory compression','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5289170',NULL,2,'Live migration of virtual machines has been a powerful tool to facilitate system maintenance, load balancing, fault tolerance, and power-saving, especially in clusters or data centers. Although pre-copy is a predominantly used approach in the state of the art, it is difficult to provide quick migration with low network overhead, due to a great amount of transferred data during migration, leading to large performance degradation of virtual machine services. This paper presents the design and implementation of a novel memory-compression-based VM migration approach (MECOM) that first uses memory compression to provide fast, stable virtual machine migration, while guaranteeing the virtual machine services to be slightly affected. Based on memory page characteristics, we design an adaptive zero-aware compression algorithm for balancing the performance and the cost of virtual machine migration. Pages are quickly compressed in batches on the source and exactly recovered on the target. Experiment demonstrates that compared with Xen, our system can significantly reduce 27.1% of downtime, 32% of total migration time and 68.8% of total transferred data on average.',1,NULL,NULL,NULL),(322,NULL,NULL,'Bianco, A. and Giraudo, L. and Hay, D.','5683164','10.1109/GLOCOM.2010.5683164',2010,NULL,'Optimal Resource Allocation for Disaster Recovery','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5683164',NULL,2,'Two key elements in disaster recovery are backing up data at remote sites and reliability of services by virtualization. To support operational services and to speed up recovery process, resources should be distributed fairly and efficiently. Thus, we discuss resource allocation algorithms to support remote data storage and live virtual machines (VMs) migration. We identify two opposing forces: on the one hand, backup data should be stored as close as possible to the original site to guarantee high access speed and to minimize network load. On the other hand, upon a site failure, VM migration should minimize the impact of resuming VMs on other sites, to protect application performance and to reduce VM restoring time. We present optimal algorithms trading-off these two contrasting goals, and we compare their performance for different network topologies and resource distributions among sites.',1,NULL,NULL,NULL),(323,NULL,NULL,'Bin Chen and Nong Xiao and Zhiping Cai and Fuyong Chu and Zhiying Wang','5197298','10.1109/NAS.2009.14',2009,NULL,'Virtual Disk Image Reclamation for Software Updates in Virtual Machine Environments','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5197298',NULL,2,'Virtual disks (VD) are the main form of storage in today\'s virtual machine (VM) environments for they have many attractive features, such as encapsulation, mobility, isolation, etc. A typical virtual disk image usually has a large size of at least several gigabytes, a disadvantage that may limit its usage in many situations, such as fast software deployment in large-scale virtual machine environments, fast VM migration in local or distributed environments, etc. The COW (Copy-on-Write) virtual block device supported by many virtual machine monitors can split the traditional one-piece large-sized VD image into multiple smaller-sized VD images (COW disks). However, frequent or long-term software updates may result in large numbers of COW disks. Moreover, the accumulated large numbers of old-version COW disks (OCD) installed with old-version software may cause serious disk space wastage. In this paper, we propose two VD image reclamation approaches RPD and RMD to reclaim the OCDs so as to free the disk space taken by the old-version software and to maintain the number of COW disks in a specific range to ease the VD image management. Based on QEMU and Linux system, we have developed tools to implement the two approaches. Experiments show that the two VD image reclamation approaches can effectively reclaim the OCDs so as to be able to support frequent or long-term software updates in virtual machine environments.',1,NULL,NULL,NULL),(324,NULL,NULL,'Watanabe, H. and Masaoka, H. and Ohigashi, T. and Kondo, T. and Nishimura, K. and Aibara, Reiji','5598154','10.1109/SAINT.2010.59',2010,NULL,'Supporting USB Devices for the Global Migration','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5598154',NULL,2,'Virtualization has been also used as desktop utility and supports VM migration, which is a function to migrate a VM (Virtual Machine) from a physical host to another. However, it cannot migrate hardware configuration of peripherals. We focus on USB devices as peripherals and provide the USB device connection method by combination of MAT and USB/IP for the global migration. In our system, MAT is implemented into VM and solves mismatch of VM\'s IP address caused by VM migration. We define VM migration capable of migrating among distinct hosts over different network segments as the global migration. USB device connectivity between host and VM is ensured by USB/IP. Combination of MAT and USB/IP allows a migrated VM to connect a USB device attaching to any hosts after migration. This paper describes the proposed mechanism for the global migration supporting USB devices.',1,NULL,NULL,NULL),(325,NULL,NULL,'Xiang Zhang and Zhigang Huo and Jie Ma and Dan Meng','5600319','10.1109/CLUSTER.2010.17',2010,NULL,'Exploiting Data Deduplication to Accelerate Live Virtual Machine Migration','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5600319',NULL,2,'As one of the key characteristics of virtualization, live virtual machine (VM) migration provides great benefits for load balancing, power management, fault tolerance and other system maintenance issues in modern clusters and data centers. Although Pre-Copy is a widespread used migration algorithm, it does transfer a lot of duplicated memory image data from source to destination, which results in longer migration time and downtime. This paper proposes a novel VM migration approach, named Migration with Data Deduplication (MDD), which introduces data deduplication into migration. MDD utilizes the self-similarity of run-time memory image, uses hash based fingerprints to find identical and similar memory pages, and employs Run Length Encode (RLE) to eliminate redundant memory data during migration. Experiment demonstrates that compared with Xen\'s default Pre-Copy migration algorithm, MDD can reduce 56.60% of total data transferred during migration, 34.93% of total migration time, and 26.16% of downtime on average.',1,NULL,NULL,NULL),(326,NULL,NULL,'Zhaobin Liu and Wenyu Qu and Tao Yan and Haitao Li and Keqiu Li','5617087','10.1109/CyberC.2010.71',2010,NULL,'Hierarchical Copy Algorithm for Xen Live Migration','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5617087',NULL,2,'With the increasing number of technology areas using Virtual Machine (VM) platforms, challenges exist in Virtual Machine migrating from one physical host to another. This paper presents the design and implementation of a novel Hierarchical Copy Algorithm (HCA) for Xen live VM migration. Based on the memory characteristics which will be transferred, the measuring time of working sets is shifted to the earlier migration stage. Furthermore, dirty page is recorded from 0/1 token to precise modification times. Experimental results illustrate that our HCA approach can shorten both the total migration time and downtime obviously under high dirty page rate environment.',1,NULL,NULL,NULL),(327,NULL,NULL,'Zhaobin Liu and Wenyu Qu and Weijiang Liu and Keqiu Li','5704422','10.1109/PDCAT.2010.88',2010,NULL,'Xen Live Migration with Slowdown Scheduling Algorithm','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5704422',NULL,2,'With the increasing number of technology areas using Virtual Machine (VM) platforms, challenges exist in Virtual Machine migrating from one physical host to another. However, the complexity of these virtualized environments presents additional management challenges. Unfortunately, many traditional approaches may be either not effective well for reducing downtime or migration time, or not suitable well for Xen VMs platforms. This paper presents the design and implementation of a novel Slowdown Scheduling Algorithm (SSA) for Xen live VM migration. In our SSA methodology, the CPU resources which have been assigned to migration domain are decrease properly. That is, the dirtying page rate is reduced according to the decrease of CPU activity. Experimental results illustrate that our SSA approach can shorten both the total migration time and downtime obviously under high dirty page rate environment.',1,NULL,NULL,NULL),(328,NULL,NULL,'Goiri, I. and Julia?, F. and Guitart, J.','4912961','10.1109/PDP.2009.15',2009,NULL,'Efficient Data Management Support for Virtualized Service Providers','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4912961',NULL,2,'Virtualization has been lately introduced for supporting and simplifying service providers management with promising results. Nevertheless, using virtualization introduces also new challenges that must be considered. One of them relates with the data management in the provider. This paper proposes an innovative approach for performing efficiently all the data-related processes in a virtualized service provider, namely VM creation, VM migration and data stage-in/out. Our solution provides a global repository where clients can upload the task input files and retrieve the output files. In addition, the provider implements a distributed file system (using NFS) in which each node can access its own local disk and the disk of the other nodes. As demonstrated in the evaluation, this allows efficient VM creation and task execution, but also task migration with minimum overhead, while keeping it accessible during the whole process.',1,NULL,NULL,NULL),(329,NULL,NULL,'Isci, C. and Hanson, J.E. and Whalley, I. and Steinder, M. and Kephart, J.O.','Isci2010','10.1109/NOMS.2010.5488495',2010,0,'Runtime Demand Estimation for effective dynamic resource management','http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5488495',1,2,'Systems management techniques that allocate resources to running entities, such as processes and virtual machines (VMs), often require estimates of the resources required by each of these resource consumers. For example, many proposed virtual machine placement algorithms attempt to allocate VMs to physical hosts in such a way as to minimize the number of physical hosts that are occupied, while ensuring that each VM receives the CPU required to do its task adequately. The common practice is to assume that the CPU requirement is equal to the current CPU utilization, or to use a prediction of it over an appropriate time horizon. In this paper, we demonstrate that, when multiple VMs or processes co-reside on a physical host, the measured CPU utilization may provide a poor estimate of the actual requirement. We derive a simple, much more accurate alternative estimate of CPU demand, implement it, and demonstrate its superiority experimentally. Furthermore, we demonstrate that using our demand estimation framework in conjunction with dynamic resource allocation in a virtualized environment greatly improves the effectiveness of dynamic placement, resulting in one-shot convergence to optimal placement and significant improvements in the overall performance of the individual VMs.',4,'Antigo citationKey: 5488495','Predictive VM resource estimation framework','Isci et al.'),(330,NULL,NULL,'Mann, Zolt\\\'{a}n \\\'{A}d\\\'{a}m','Mann:2015:AVM:2808687.2797211','10.1145/2797211',2015,NULL,'Allocation of Virtual Machines in Cloud Data Centers - A Survey of Problem Models and Optimization Algorithms','http://doi.acm.org/10.1145/2797211',NULL,10,NULL,NULL,NULL,NULL,NULL),(331,NULL,NULL,'Alicherry, Mansoor; Lakshman, T. V.','Alicherry2013','10.1109/INFCOM.2013.6566850',2013,0,'Optimizing data access latencies in cloud systems by intelligent virtual machine placement',NULL,1,2,NULL,4,NULL,'Data-aware VM placement','Alicherry and Lakshman'),(332,NULL,NULL,'Cohen, Rami Lewin-Eytan, Liane Seffi Naor, Joseph Raz, Danny','Cohen2013','10.1109/INFCOM.2013.6566794',2013,0,'Almost Optimal Virtual Machine Placement for Traffic Intense Data Centers Rami',NULL,1,2,NULL,4,NULL,'Bandwidth-constrained VM placement','Cohen et al.'),(334,NULL,NULL,'Zheng, Zeyu Li, Minming Xiao, Xun Wang, Jianping','Zheng2013','10.1109/INFCOM.2013.6566792',2013,0,'Coordinated resource provisioning and maintenance scheduling in cloud data centers',NULL,1,1,NULL,4,NULL,'CRPMS-FM (fixed maintenance) Heuristic','Zheng et al.'),(335,NULL,NULL,'Yanagisawa, Hiroki; Osogami, Takayuki; Raymond, Rudy','Yanagisawa2013','10.1109/INFCOM.2013.6566848',2013,0,'Dependable Virtual Machine Allocation',NULL,1,1,NULL,4,NULL,'Dependable VM Allocation','Yanagisawa et al.'),(336,NULL,NULL,'Li, Kangkang; Wu, Jie; Blaisse, Adam','Li2013','10.1109/CloudNet.2013.6710563',2013,0,'Elasticity-aware virtual machine placement for cloud datacenters',NULL,1,1,NULL,4,NULL,'Hierarchical VM Placement','Kangkang et al.'),(337,NULL,NULL,'Guo, Y; Stolyar, AL; Walid, A','Guo2013','10.1109/INFCOM.2013.6566847',2013,0,'Shadow-routing based dynamic algorithms for virtual machine placement in a network cloud',NULL,1,1,NULL,4,NULL,'Shadow-routing VM placement','Guo et al.'),(338,NULL,NULL,'Majhi, Santosh Kumar','Majhi2014','10.1109/PDGC.2014.7030741',2014,NULL,'{VM Migration Auction : Business Oriented Federation of Cloud Providers for Scaling of Application Services}','',NULL,11,NULL,NULL,NULL,NULL,NULL),(339,NULL,NULL,'Yamada, Hiroshi','Yamada2016','10.11185/imt.11.101',2016,NULL,'{Survey on Mechanisms for Live Virtual Machine Migration and its Improvements}','',NULL,11,NULL,NULL,NULL,NULL,NULL),(340,NULL,NULL,'Aral, Atakan and Ovatman, Tolga','Aral2016','10.1016/j.jss.2016.07.007',2016,NULL,'{Network-aware embedding of virtual machine clusters onto federated cloud infrastructure}','',NULL,11,NULL,NULL,NULL,NULL,NULL),(341,NULL,NULL,'Chang, Victor and Gani, Abdullah and Ha, Siti and Hamid, Ab and Toseef, Muhammad and Shoaib, Umar and Liaqat, Rana','Chang2017','10.1016/j.jnca.2016.10.008',2017,NULL,'{Federated cloud resource management: Review and discussion}','',NULL,11,NULL,NULL,NULL,NULL,NULL),(342,NULL,NULL,'Tordsson, Johan and Montero, Rub{\\\'{e}}n S and Moreno-vozmediano, Rafael and Llorente, Ignacio M','Tordsson2012','10.1016/j.future.2011.07.003',2012,NULL,'{Cloud brokering mechanisms for optimized placement of virtual machines across multiple providers}','',NULL,11,NULL,NULL,NULL,NULL,NULL),(343,NULL,NULL,'Pop, Florin and Potop-butucaru, Maria','Pop2016','10.1016/j.future.2015.07.016',2016,NULL,'{ARMCO : Advanced topics in resource management for ubiquitous cloud computing : An adaptive approach }','',NULL,11,NULL,NULL,NULL,NULL,NULL),(344,NULL,NULL,'Dupont, Corentin and Hermenier, Fabien and Schulze, Thomas and Somov, Andrey','Dupont2012','10.1145/2208828.2208832',2012,NULL,'{An Energy Aware Framework for Virtual Machine Placement in Cloud Federated Data Centres}',NULL,NULL,11,NULL,2,NULL,NULL,NULL),(345,NULL,NULL,'Kirthica, S and Sridhar, Rajeswari','Kirthica2017','10.1016/j.jnca.2016.12.009',2017,NULL,'{CIT : A Cloud Inter-operation Toolkit to enhance elasticity and tolerate shut down of external clouds}','http://dx.doi.org/10.1016/j.jnca.2016.12.009',NULL,11,NULL,NULL,NULL,NULL,NULL),(346,NULL,NULL,'Li, Hongxing and Wu, Chuan and Li, Zongpeng and Lau, Francis C M','Li2016','10.1109/TNET.2015.2435015',2016,NULL,'{Virtual Machine Trading in a Federation of Clouds: Individual Profit and Social Welfare Maximization}','',NULL,11,NULL,NULL,NULL,NULL,NULL);
/*!40000 ALTER TABLE `Paper` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `PaperFieldAnswer`
--

DROP TABLE IF EXISTS `PaperFieldAnswer`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `PaperFieldAnswer` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `subjectiveAnswer` varchar(120) DEFAULT NULL,
  `field_id` int(11) NOT NULL,
  `fieldOption_id` bigint(20) DEFAULT NULL,
  `paper_id` bigint(20) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `ix_PaperFieldAnswerOption` (`paper_id`,`field_id`,`fieldOption_id`),
  UNIQUE KEY `ix_PaperFieldAnswerSubjectAnswer` (`paper_id`,`field_id`,`subjectiveAnswer`),
  KEY `FK_m47ib0wu9msyhqd3v5ncuaduo` (`field_id`),
  KEY `FK_8xlwvrk8tihndn9epybfb77fp` (`fieldOption_id`),
  CONSTRAINT `FK_8xlwvrk8tihndn9epybfb77fp` FOREIGN KEY (`fieldOption_id`) REFERENCES `FieldOption` (`id`),
  CONSTRAINT `FK_m47ib0wu9msyhqd3v5ncuaduo` FOREIGN KEY (`field_id`) REFERENCES `Field` (`id`),
  CONSTRAINT `FK_of08j19jard0x2ly1y5yko0d2` FOREIGN KEY (`paper_id`) REFERENCES `Paper` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=866 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `PaperFieldAnswer`
--

LOCK TABLES `PaperFieldAnswer` WRITE;
/*!40000 ALTER TABLE `PaperFieldAnswer` DISABLE KEYS */;
INSERT INTO `PaperFieldAnswer` VALUES (67,NULL,12,36,106),(74,NULL,22,170,106),(75,NULL,23,173,106),(76,NULL,24,174,106),(78,NULL,13,46,107),(79,NULL,13,50,107),(80,NULL,13,40,107),(81,NULL,10,19,108),(82,NULL,10,17,108),(83,NULL,11,23,108),(85,NULL,13,46,108),(86,NULL,13,51,108),(87,NULL,13,52,108),(88,NULL,13,50,108),(89,NULL,14,72,108),(90,NULL,21,162,108),(91,NULL,19,179,108),(92,NULL,20,150,108),(93,NULL,16,180,108),(94,NULL,12,36,108),(95,NULL,13,43,108),(96,NULL,13,40,108),(97,NULL,22,170,108),(98,NULL,23,173,108),(99,NULL,24,174,108),(100,NULL,13,182,108),(101,NULL,13,183,108),(102,NULL,13,58,109),(103,NULL,13,51,109),(104,NULL,13,39,109),(105,NULL,19,137,109),(106,NULL,19,136,109),(107,NULL,23,173,109),(108,NULL,15,80,109),(109,NULL,15,78,109),(112,NULL,13,46,109),(113,NULL,13,50,109),(114,NULL,20,150,109),(115,NULL,20,149,109),(116,NULL,10,17,109),(117,NULL,14,70,109),(118,NULL,16,94,109),(119,NULL,21,163,109),(120,NULL,22,168,109),(121,NULL,10,19,111),(122,NULL,10,17,111),(123,NULL,13,54,111),(124,NULL,13,183,111),(125,NULL,13,41,111),(126,NULL,21,162,111),(127,NULL,13,57,111),(128,NULL,14,71,111),(129,NULL,15,78,111),(130,NULL,15,184,111),(131,NULL,13,39,111),(132,NULL,12,36,111),(133,NULL,10,17,112),(134,NULL,13,57,112),(135,NULL,13,54,112),(136,NULL,21,162,112),(137,NULL,12,36,112),(138,NULL,10,17,113),(140,NULL,13,39,113),(141,NULL,10,19,113),(142,NULL,25,194,113),(143,NULL,25,185,113),(144,NULL,13,200,113),(145,NULL,15,80,113),(146,NULL,15,78,113),(147,NULL,24,177,113),(148,NULL,11,25,113),(149,NULL,12,36,113),(150,NULL,13,51,113),(151,NULL,14,70,113),(152,NULL,20,147,113),(153,NULL,21,163,113),(154,NULL,22,170,113),(155,NULL,23,173,113),(156,NULL,10,17,114),(157,NULL,13,46,114),(158,NULL,13,57,114),(159,NULL,13,39,114),(160,NULL,15,85,114),(161,NULL,19,137,114),(162,NULL,25,196,114),(163,NULL,25,190,114),(164,NULL,25,185,114),(165,NULL,12,36,114),(166,NULL,13,54,114),(167,NULL,13,50,114),(168,NULL,22,168,114),(169,NULL,23,173,114),(170,NULL,24,175,114),(171,NULL,25,187,114),(172,NULL,25,198,114),(173,NULL,19,201,114),(174,NULL,20,147,114),(175,NULL,20,202,114),(176,NULL,11,25,114),(177,NULL,14,70,114),(178,NULL,21,163,114),(179,NULL,13,39,115),(180,NULL,20,150,115),(181,NULL,20,149,115),(182,NULL,20,147,115),(183,NULL,25,185,115),(184,NULL,10,17,115),(185,NULL,13,54,115),(186,NULL,12,36,115),(187,NULL,11,27,115),(188,NULL,14,70,115),(189,NULL,19,128,115),(190,NULL,21,162,115),(191,NULL,22,170,115),(192,NULL,25,192,115),(193,NULL,25,198,115),(194,NULL,13,40,116),(195,NULL,13,203,116),(196,NULL,13,60,117),(197,NULL,13,46,117),(198,NULL,13,57,117),(199,NULL,13,58,117),(200,NULL,13,51,117),(201,NULL,13,50,117),(202,NULL,13,39,117),(203,NULL,25,194,117),(204,NULL,13,204,117),(205,NULL,21,162,117),(206,NULL,14,70,117),(207,NULL,15,82,117),(208,NULL,23,173,117),(209,NULL,12,36,117),(210,NULL,20,205,117),(211,NULL,10,19,117),(212,NULL,10,17,117),(213,NULL,22,170,117),(214,NULL,25,195,117),(215,NULL,10,19,119),(216,NULL,10,17,119),(217,NULL,13,39,119),(218,NULL,21,163,119),(219,NULL,13,60,119),(220,NULL,13,46,119),(221,NULL,13,51,119),(222,NULL,13,50,119),(223,NULL,14,70,119),(224,NULL,19,207,119),(225,NULL,10,17,106),(226,NULL,13,60,106),(227,NULL,13,46,106),(228,NULL,13,51,106),(229,NULL,13,50,106),(230,NULL,13,39,106),(231,NULL,15,81,106),(232,NULL,19,126,106),(233,NULL,19,130,106),(234,NULL,20,156,106),(235,NULL,26,210,175),(237,NULL,29,216,175),(239,NULL,29,215,158),(243,NULL,29,217,158),(244,NULL,26,211,158),(245,NULL,12,36,119),(246,NULL,20,205,119),(247,NULL,22,170,119),(248,NULL,23,173,119),(249,NULL,25,194,119),(250,NULL,25,195,119),(251,NULL,10,19,126),(252,NULL,10,17,126),(253,NULL,13,40,126),(254,NULL,25,194,126),(255,NULL,25,218,126),(257,NULL,13,40,133),(258,NULL,13,41,133),(259,NULL,14,70,133),(260,NULL,15,83,133),(261,NULL,21,162,133),(262,NULL,22,170,133),(263,NULL,25,188,133),(264,NULL,25,191,133),(265,NULL,10,20,133),(266,NULL,13,56,133),(267,NULL,13,183,133),(268,NULL,19,133,133),(269,NULL,15,85,123),(270,NULL,21,163,123),(271,NULL,13,39,123),(272,NULL,10,17,123),(273,NULL,19,128,123),(274,NULL,20,152,123),(275,NULL,20,150,123),(276,NULL,20,149,123),(277,NULL,25,198,123),(278,NULL,23,173,123),(279,NULL,12,36,123),(280,NULL,22,170,123),(281,NULL,11,25,123),(282,NULL,19,125,111),(283,NULL,20,147,111),(284,NULL,22,170,111),(285,NULL,25,185,111),(286,NULL,25,198,111),(287,NULL,25,191,111),(288,NULL,19,128,111),(289,NULL,10,17,140),(290,NULL,13,51,140),(291,NULL,25,194,140),(292,NULL,13,60,140),(293,NULL,13,46,140),(294,NULL,13,47,140),(295,NULL,13,48,140),(297,NULL,10,18,140),(298,NULL,13,39,140),(299,NULL,22,170,140),(300,NULL,23,173,140),(301,NULL,10,17,279),(302,NULL,11,25,279),(303,NULL,12,36,279),(304,NULL,13,46,279),(305,NULL,13,57,279),(306,NULL,13,40,279),(307,NULL,14,70,279),(308,NULL,15,81,279),(309,NULL,16,99,279),(310,NULL,18,119,279),(311,NULL,20,150,279),(312,NULL,21,162,279),(313,NULL,22,169,279),(314,NULL,23,173,279),(315,NULL,24,174,279),(316,NULL,25,196,279),(317,NULL,25,190,279),(318,NULL,10,17,258),(319,NULL,12,36,258),(320,NULL,13,60,258),(321,NULL,13,58,258),(322,NULL,13,52,258),(323,NULL,13,183,258),(324,NULL,13,50,258),(325,NULL,13,40,258),(326,NULL,13,39,258),(327,NULL,13,41,258),(328,NULL,14,69,258),(329,NULL,15,78,258),(330,NULL,16,96,258),(331,NULL,18,112,258),(332,NULL,19,130,258),(333,NULL,19,131,258),(334,NULL,20,150,258),(335,NULL,20,151,258),(336,NULL,20,149,258),(337,NULL,21,164,258),(338,NULL,22,166,258),(339,NULL,23,173,258),(340,NULL,24,174,258),(341,NULL,25,187,258),(342,NULL,25,195,258),(343,NULL,13,51,263),(344,NULL,13,50,263),(345,NULL,13,40,263),(346,NULL,13,39,263),(347,NULL,15,81,263),(348,NULL,15,78,263),(349,NULL,13,43,271),(350,NULL,13,49,271),(351,NULL,13,44,271),(352,NULL,13,51,271),(353,NULL,13,54,271),(354,NULL,13,52,271),(355,NULL,13,56,271),(356,NULL,13,183,271),(357,NULL,13,50,271),(358,NULL,13,40,271),(359,NULL,13,39,271),(360,NULL,10,17,268),(361,NULL,11,29,268),(362,NULL,12,36,268),(363,NULL,13,43,268),(364,NULL,13,46,268),(365,NULL,13,57,268),(366,NULL,13,58,268),(367,NULL,13,51,268),(368,NULL,13,52,268),(369,NULL,13,50,268),(370,NULL,13,40,268),(371,NULL,13,39,268),(372,NULL,14,69,268),(373,NULL,15,78,268),(374,NULL,16,89,268),(375,NULL,18,110,268),(376,NULL,19,126,268),(377,NULL,19,128,268),(378,NULL,19,129,268),(379,NULL,20,150,268),(380,NULL,20,149,268),(381,NULL,21,164,268),(382,NULL,22,168,268),(383,NULL,23,173,268),(384,NULL,24,175,268),(385,NULL,25,194,268),(387,NULL,25,195,268),(388,NULL,10,17,254),(390,NULL,13,60,254),(391,NULL,13,46,254),(392,NULL,13,51,254),(393,NULL,13,54,254),(394,NULL,13,50,254),(395,NULL,13,40,254),(396,NULL,14,69,254),(397,NULL,15,81,254),(398,NULL,16,93,254),(399,NULL,18,105,254),(400,NULL,19,136,254),(401,NULL,20,153,254),(402,NULL,20,150,254),(403,NULL,20,145,254),(404,NULL,20,147,254),(405,NULL,21,162,254),(406,NULL,22,166,254),(407,NULL,23,173,254),(408,NULL,25,194,254),(409,NULL,25,185,254),(410,NULL,25,195,254),(411,NULL,25,198,254),(412,NULL,10,18,274),(413,NULL,11,30,274),(414,NULL,12,36,274),(415,NULL,13,61,274),(416,NULL,13,46,274),(417,NULL,13,51,274),(418,NULL,13,52,274),(419,NULL,13,50,274),(420,NULL,13,43,274),(421,NULL,13,62,274),(422,NULL,14,72,274),(423,NULL,15,81,274),(424,NULL,15,78,274),(425,NULL,16,93,274),(426,NULL,18,113,274),(427,NULL,19,132,274),(428,NULL,19,133,274),(429,NULL,20,156,274),(430,NULL,21,164,274),(431,NULL,22,170,274),(432,NULL,24,176,274),(433,NULL,25,195,274),(434,NULL,25,194,274),(435,NULL,10,17,283),(436,NULL,11,23,283),(437,NULL,11,31,283),(438,NULL,12,36,283),(439,NULL,13,46,283),(440,NULL,13,51,283),(441,NULL,13,54,283),(442,NULL,13,40,283),(443,NULL,14,69,283),(444,NULL,16,92,283),(445,NULL,18,115,283),(446,NULL,20,152,283),(447,NULL,21,162,283),(448,NULL,22,167,283),(449,NULL,25,194,283),(450,NULL,25,198,283),(451,NULL,10,17,291),(452,NULL,12,36,291),(453,NULL,13,46,291),(454,NULL,13,51,291),(455,NULL,13,52,291),(456,NULL,13,63,291),(457,NULL,13,183,291),(458,NULL,13,50,291),(459,NULL,13,40,291),(460,NULL,14,69,291),(461,NULL,15,81,291),(462,NULL,16,97,291),(463,NULL,18,114,291),(464,NULL,20,156,291),(465,NULL,20,153,291),(466,NULL,20,145,291),(467,NULL,21,162,291),(468,NULL,22,170,291),(469,NULL,23,173,291),(470,NULL,24,175,291),(471,NULL,25,187,291),(472,NULL,25,218,291),(473,NULL,25,195,291),(474,NULL,10,18,255),(475,NULL,12,33,255),(476,NULL,13,51,255),(477,NULL,13,52,255),(478,NULL,13,50,255),(479,NULL,13,40,255),(480,NULL,13,39,255),(481,NULL,14,71,255),(482,NULL,16,93,255),(483,NULL,18,105,255),(484,NULL,20,150,255),(485,NULL,20,149,255),(486,NULL,20,147,255),(487,NULL,21,162,255),(488,NULL,22,168,255),(489,NULL,25,194,255),(490,NULL,10,17,276),(491,NULL,11,23,276),(492,NULL,12,36,276),(493,NULL,13,46,276),(494,NULL,13,51,276),(495,NULL,13,50,276),(496,NULL,13,40,276),(497,NULL,14,70,276),(498,NULL,18,118,276),(499,NULL,20,152,276),(500,NULL,20,150,276),(501,NULL,20,149,276),(502,NULL,21,162,276),(503,NULL,22,170,276),(504,NULL,25,194,276),(505,NULL,25,195,276),(506,NULL,10,17,266),(507,NULL,11,26,266),(508,NULL,12,36,266),(509,NULL,13,43,266),(510,NULL,13,57,266),(511,NULL,13,44,266),(512,NULL,13,51,266),(513,NULL,13,54,266),(514,NULL,13,39,266),(515,NULL,15,81,266),(516,NULL,15,78,266),(517,NULL,16,95,266),(518,NULL,18,108,266),(519,NULL,19,126,266),(520,NULL,19,127,266),(521,NULL,20,146,266),(522,NULL,20,145,266),(523,NULL,20,148,266),(524,NULL,25,188,266),(525,NULL,25,195,266),(526,NULL,25,197,266),(527,NULL,10,19,250),(528,NULL,10,17,250),(529,NULL,10,18,250),(530,NULL,11,30,250),(532,NULL,13,46,250),(533,NULL,13,44,250),(534,NULL,13,43,250),(535,NULL,13,40,250),(536,NULL,14,71,250),(537,NULL,25,189,250),(538,NULL,10,17,249),(539,NULL,10,18,249),(540,NULL,11,27,249),(541,NULL,11,30,249),(542,NULL,12,36,249),(543,NULL,13,46,249),(544,NULL,13,51,249),(545,NULL,13,54,249),(546,NULL,13,50,249),(547,NULL,13,40,249),(548,NULL,14,70,249),(549,NULL,15,81,249),(550,NULL,16,98,249),(551,NULL,18,117,249),(552,NULL,20,147,249),(553,NULL,21,162,249),(554,NULL,22,169,249),(555,NULL,23,173,249),(556,NULL,25,196,249),(557,NULL,25,185,249),(558,NULL,10,19,285),(559,NULL,10,17,285),(560,NULL,11,23,285),(562,NULL,13,46,285),(563,NULL,13,51,285),(564,NULL,13,50,285),(565,NULL,13,38,285),(566,NULL,14,71,285),(567,NULL,15,84,285),(568,NULL,16,93,285),(569,NULL,18,103,285),(570,NULL,19,135,285),(571,NULL,20,150,285),(572,NULL,21,162,285),(573,NULL,22,168,285),(574,NULL,25,196,285),(575,NULL,25,194,285),(576,NULL,13,45,265),(577,NULL,13,49,265),(578,NULL,13,51,265),(579,NULL,13,50,265),(580,NULL,13,40,265),(581,NULL,10,17,253),(582,NULL,11,23,253),(583,NULL,11,22,253),(585,NULL,13,45,253),(586,NULL,13,43,253),(587,NULL,13,46,253),(588,NULL,13,47,253),(589,NULL,13,49,253),(590,NULL,13,44,253),(591,NULL,13,51,253),(592,NULL,13,52,253),(593,NULL,13,48,253),(594,NULL,13,50,253),(595,NULL,13,37,253),(596,NULL,13,40,253),(597,NULL,13,39,253),(598,NULL,14,69,253),(599,NULL,15,78,253),(600,NULL,16,91,253),(601,NULL,18,104,253),(602,NULL,21,162,253),(603,NULL,25,188,253),(604,NULL,10,17,251),(605,NULL,11,28,251),(606,NULL,12,36,251),(607,NULL,13,46,251),(608,NULL,13,51,251),(609,NULL,13,50,251),(610,NULL,13,40,251),(611,NULL,13,39,251),(612,NULL,14,72,251),(613,NULL,15,78,251),(614,NULL,16,90,251),(615,NULL,18,109,251),(616,NULL,20,154,251),(617,NULL,21,164,251),(618,NULL,22,169,251),(619,NULL,23,173,251),(620,NULL,25,196,251),(621,NULL,25,195,251),(622,NULL,10,17,289),(623,NULL,11,25,289),(625,NULL,13,51,289),(626,NULL,13,54,289),(627,NULL,13,52,289),(628,NULL,13,183,289),(629,NULL,13,40,289),(630,NULL,13,39,289),(631,NULL,13,41,289),(632,NULL,14,69,289),(633,NULL,15,80,289),(634,NULL,15,78,289),(635,NULL,15,79,289),(636,NULL,16,92,289),(637,NULL,18,103,289),(638,NULL,19,125,289),(639,NULL,20,152,289),(640,NULL,21,162,289),(641,NULL,22,166,289),(642,NULL,25,191,289),(643,NULL,10,17,302),(644,NULL,10,18,302),(646,NULL,13,40,302),(647,NULL,13,39,302),(648,NULL,14,69,302),(649,NULL,15,88,302),(650,NULL,16,89,302),(651,NULL,19,143,302),(652,NULL,20,152,302),(653,NULL,20,151,302),(654,NULL,21,161,302),(655,NULL,22,170,302),(656,NULL,25,185,302),(657,NULL,10,17,296),(658,NULL,11,23,296),(660,NULL,13,46,296),(661,NULL,13,66,296),(662,NULL,13,64,296),(663,NULL,13,40,296),(664,NULL,13,39,296),(665,NULL,14,70,296),(666,NULL,16,89,296),(667,NULL,18,121,296),(668,NULL,19,144,296),(669,NULL,20,152,296),(670,NULL,20,146,296),(671,NULL,20,148,296),(672,NULL,20,147,296),(673,NULL,21,162,296),(674,NULL,22,170,296),(675,NULL,25,185,296),(676,NULL,10,19,316),(678,NULL,13,46,316),(679,NULL,13,54,316),(680,NULL,13,39,316),(681,NULL,14,70,316),(682,NULL,15,78,316),(683,NULL,15,85,316),(684,NULL,16,90,316),(685,NULL,18,117,316),(686,NULL,19,123,316),(687,NULL,19,124,316),(688,NULL,20,147,316),(689,NULL,21,162,316),(690,NULL,22,170,316),(691,NULL,23,173,316),(692,NULL,25,198,316),(698,NULL,10,19,313),(699,NULL,10,17,313),(701,NULL,13,46,313),(702,NULL,13,58,313),(703,NULL,13,65,313),(704,NULL,13,51,313),(705,NULL,13,39,313),(706,NULL,14,69,313),(707,NULL,15,78,313),(708,NULL,18,105,313),(709,NULL,19,135,313),(710,NULL,19,130,313),(711,NULL,20,150,313),(712,NULL,20,145,313),(713,NULL,20,149,313),(714,NULL,20,157,313),(715,NULL,21,163,313),(716,NULL,22,170,313),(717,NULL,23,173,313),(718,NULL,24,178,313),(719,NULL,25,194,313),(720,NULL,10,20,294),(721,NULL,10,19,294),(722,NULL,10,17,294),(724,NULL,13,60,294),(725,NULL,13,39,294),(726,NULL,14,70,294),(727,NULL,15,78,294),(728,NULL,16,89,294),(729,NULL,18,102,294),(730,NULL,19,142,294),(731,NULL,20,152,294),(732,NULL,20,150,294),(733,NULL,20,145,294),(734,NULL,20,149,294),(735,NULL,21,162,294),(736,NULL,25,187,294),(737,NULL,10,17,318),(739,NULL,13,46,318),(740,NULL,13,52,318),(741,NULL,13,50,318),(742,NULL,13,40,318),(743,NULL,14,70,318),(744,NULL,16,89,318),(745,NULL,18,102,318),(746,NULL,19,129,318),(747,NULL,20,150,318),(748,NULL,20,149,318),(749,NULL,21,162,318),(750,NULL,22,170,318),(751,NULL,23,173,318),(752,NULL,25,188,318),(753,NULL,25,191,318),(754,NULL,10,17,295),(755,NULL,10,18,295),(757,NULL,13,43,295),(758,NULL,13,51,295),(759,NULL,13,50,295),(760,NULL,13,39,295),(761,NULL,13,42,295),(762,NULL,14,70,295),(763,NULL,15,87,295),(764,NULL,16,89,295),(765,NULL,18,102,295),(766,NULL,19,141,295),(767,NULL,21,162,295),(768,NULL,22,167,295),(769,NULL,23,173,295),(770,NULL,25,194,295),(771,NULL,25,188,295),(772,NULL,25,195,295),(773,NULL,10,17,314),(774,NULL,10,18,314),(776,NULL,13,60,314),(777,NULL,13,67,314),(778,NULL,13,46,314),(779,NULL,13,57,314),(780,NULL,13,50,314),(781,NULL,13,39,314),(782,NULL,14,69,314),(783,NULL,15,81,314),(784,NULL,15,80,314),(785,NULL,15,78,314),(786,NULL,16,89,314),(787,NULL,18,122,314),(788,NULL,19,133,314),(789,NULL,20,158,314),(790,NULL,21,163,314),(791,NULL,22,170,314),(792,NULL,25,192,314),(793,NULL,25,188,314),(794,NULL,10,19,329),(795,NULL,10,17,329),(797,NULL,13,46,329),(798,NULL,13,50,329),(799,NULL,13,40,329),(800,NULL,13,39,329),(801,NULL,14,71,329),(802,NULL,16,89,329),(803,NULL,20,150,329),(804,NULL,21,162,329),(805,NULL,22,168,329),(806,NULL,25,196,329),(807,NULL,25,194,329),(808,NULL,10,21,299),(810,NULL,13,46,299),(811,NULL,13,57,299),(812,NULL,13,54,299),(813,NULL,13,50,299),(814,NULL,13,40,299),(815,NULL,13,39,299),(816,NULL,14,70,299),(817,NULL,15,86,299),(818,NULL,19,124,299),(819,NULL,20,156,299),(820,NULL,20,153,299),(821,NULL,21,162,299),(822,NULL,22,169,299),(823,NULL,23,173,299),(824,NULL,25,196,299),(825,NULL,25,198,299),(826,NULL,10,21,300),(828,NULL,13,60,300),(829,NULL,13,65,300),(830,NULL,13,51,300),(831,NULL,13,52,300),(832,NULL,13,62,300),(833,NULL,14,70,300),(834,NULL,16,89,300),(835,NULL,18,103,300),(836,NULL,20,150,300),(837,NULL,20,145,300),(838,NULL,20,157,300),(839,NULL,20,158,300),(840,NULL,21,162,300),(841,NULL,22,171,300),(842,NULL,25,196,300),(843,NULL,25,194,300),(844,NULL,25,188,300),(845,NULL,19,220,295),(846,NULL,10,18,312),(847,NULL,11,30,312),(848,NULL,13,182,312),(849,NULL,13,46,312),(850,NULL,13,57,312),(851,NULL,13,50,312),(852,NULL,13,40,312),(853,NULL,14,70,312),(854,NULL,16,93,312),(855,NULL,18,119,312),(856,NULL,19,140,312),(857,NULL,19,139,312),(858,NULL,20,152,312),(859,NULL,20,150,312),(860,NULL,20,149,312),(861,NULL,20,157,312),(862,NULL,20,147,312),(863,NULL,21,162,312),(864,NULL,22,167,312),(865,NULL,25,190,312);
/*!40000 ALTER TABLE `PaperFieldAnswer` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `PaperStatus`
--

DROP TABLE IF EXISTS `PaperStatus`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `PaperStatus` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `accepted` bit(1) NOT NULL,
  `reviewPhase_id` bigint(20) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `ix_PaperStatus` (`reviewPhase_id`,`accepted`),
  CONSTRAINT `FK_4o8y0i1ee48csrwax7n2rdgmf` FOREIGN KEY (`reviewPhase_id`) REFERENCES `ReviewPhase` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `PaperStatus`
--

LOCK TABLES `PaperStatus` WRITE;
/*!40000 ALTER TABLE `PaperStatus` DISABLE KEYS */;
INSERT INTO `PaperStatus` VALUES (1,'\0',1),(2,'',1),(3,'\0',2),(4,'',2);
/*!40000 ALTER TABLE `PaperStatus` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `PaperType`
--

DROP TABLE IF EXISTS `PaperType`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `PaperType` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `abbreviation` varchar(2) DEFAULT NULL,
  `description` varchar(50) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `ix_PaperTypeDesc` (`description`),
  UNIQUE KEY `ix_PaperTypeAbbrev` (`abbreviation`)
) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `PaperType`
--

LOCK TABLES `PaperType` WRITE;
/*!40000 ALTER TABLE `PaperType` DISABLE KEYS */;
INSERT INTO `PaperType` VALUES (1,'C','Conference'),(2,'J','Journal'),(3,'M','Master Thesis'),(4,'P','PhD Thesis'),(5,'T','Technical Report');
/*!40000 ALTER TABLE `PaperType` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `Project`
--

DROP TABLE IF EXISTS `Project`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `Project` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `description` varchar(100) NOT NULL,
  `endUser_id` bigint(20) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `ix_Project` (`description`,`endUser_id`),
  KEY `FK_k9bsjyft5u30r2qd88aqdvepy` (`endUser_id`),
  CONSTRAINT `FK_k9bsjyft5u30r2qd88aqdvepy` FOREIGN KEY (`endUser_id`) REFERENCES `Enduser` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=9 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `Project`
--

LOCK TABLES `Project` WRITE;
/*!40000 ALTER TABLE `Project` DISABLE KEYS */;
INSERT INTO `Project` VALUES (1,'VM Placement Survey',1),(8,'ZTeste2',1);
/*!40000 ALTER TABLE `Project` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `Repository`
--

DROP TABLE IF EXISTS `Repository`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `Repository` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `description` varchar(50) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `ix_Repository` (`description`)
) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `Repository`
--

LOCK TABLES `Repository` WRITE;
/*!40000 ALTER TABLE `Repository` DISABLE KEYS */;
INSERT INTO `Repository` VALUES (2,'ACM'),(1,'IEEE'),(5,'Multiple'),(3,'Science Direct'),(4,'Springer');
/*!40000 ALTER TABLE `Repository` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `ReviewPhase`
--

DROP TABLE IF EXISTS `ReviewPhase`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `ReviewPhase` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `description` varchar(60) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `ix_ReviewPhaseDesc` (`description`)
) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `ReviewPhase`
--

LOCK TABLES `ReviewPhase` WRITE;
/*!40000 ALTER TABLE `ReviewPhase` DISABLE KEYS */;
INSERT INTO `ReviewPhase` VALUES (2,'Extraction Phase'),(1,'Selection Phase');
/*!40000 ALTER TABLE `ReviewPhase` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `SearchSession`
--

DROP TABLE IF EXISTS `SearchSession`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `SearchSession` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `searchDate` date NOT NULL,
  `searchString` varchar(200) NOT NULL,
  `project_id` bigint(20) NOT NULL,
  `repository_id` bigint(20) NOT NULL,
  PRIMARY KEY (`id`),
  KEY `FK_cqlhnwhxhftlnnueuac7hb122` (`project_id`),
  KEY `FK_bi55ltuuk0infwje3ksdxvf2u` (`repository_id`),
  CONSTRAINT `FK_bi55ltuuk0infwje3ksdxvf2u` FOREIGN KEY (`repository_id`) REFERENCES `Repository` (`id`),
  CONSTRAINT `FK_cqlhnwhxhftlnnueuac7hb122` FOREIGN KEY (`project_id`) REFERENCES `Project` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=12 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `SearchSession`
--

LOCK TABLES `SearchSession` WRITE;
/*!40000 ALTER TABLE `SearchSession` DISABLE KEYS */;
INSERT INTO `SearchSession` VALUES (1,'2014-09-30','(Cloud Computing) AND ( ((VM Placement) OR (VM Migration)) OR (Server Consolidation) OR (Load Balancing)) and Year >= 2011',1,1),(2,'2015-04-20','(Cloud Computing) AND ( (\"VM Placement\") OR (\"Virtual Machine Placement\") OR (\"VM Migration\")) and Year between [2008 .. 2010]',1,1),(3,'2015-10-21','pub-date > 2007 and TITLE-ABSTR-KEY({VM Migration} or {Virtual Machine Migration}) or TITLE-ABSTR-KEY({VM Placement} or {Virtual Machine Placement}) 	#Only on journals (that include conference papers)',1,3),(8,'2168-06-01','22',8,2),(10,'2016-01-13','survey',1,2),(11,'2174-01-07','(\"vm placement\" or \"vm migration\") and \"cloud federation\"',1,5);
/*!40000 ALTER TABLE `SearchSession` ENABLE KEYS */;
UNLOCK TABLES;
/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;

-- Dump completed on 2017-07-06 13:29:41
